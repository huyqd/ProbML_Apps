{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n6oPAr_dprK"
      },
      "source": [
        "# Bayesian Linear Regression: Updating Beliefs about Model Parameters (Plotly Interactive)\n",
        "\n",
        "In this notebook, we will explore Bayesian Linear Regression. This is a fundamental probabilistic model where we place a probability distribution over the possible values of our model parameters and update this distribution as we observe data. We'll see how a Gaussian prior distribution, combined with a linear model and Gaussian noise, leads to a Gaussian posterior distribution that can be computed analytically.\n",
        "\n",
        "We will use JAX for numerical computation, `ipywidgets` for interactive controls, and **Plotly** for dynamic and interactive plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "joSolJlJdprL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded: 20 points\n",
            "Input shape (X): (20, 1)\n",
            "Output shape (Y): (20,)\n",
            "Noise standard deviation (sigma): 1.5\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jrandom\n",
        "import numpy as np  # Useful for converting JAX arrays for plotting\n",
        "\n",
        "# No matplotlib.pyplot needed in the main update function\n",
        "import scipy.io  # To load the .mat data file\n",
        "import ipywidgets as widgets  # For interactive controls\n",
        "from IPython.display import display  # To display widgets and output\n",
        "from jax.scipy.linalg import cholesky  # For plotting Gaussian contours\n",
        "from gaussians import Gaussian\n",
        "\n",
        "# Import Plotly\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px  # Can be useful for simpler plots\n",
        "\n",
        "# Optional: No direct color import from tueplots needed for Plotly, define colors here\n",
        "PLOTLY_COLORS = {\n",
        "    \"dark\": \"rgba(0,0,0,1.0)\",  # Black\n",
        "    \"gray\": \"rgba(128,128,128,1.0)\",  # Gray\n",
        "    \"blue\": \"rgba(0,0,255,1.0)\",  # Blue\n",
        "    \"red\": \"rgba(255,0,0,1.0)\",  # Red\n",
        "    \"dark_alpha\": \"rgba(0,0,0,0.2)\",  # Black with transparency for bands/samples\n",
        "    \"blue_alpha\": \"rgba(0,0,255,0.2)\",  # Blue with transparency\n",
        "    \"red_alpha\": \"rgba(255,0,0,0.5)\",  # Red with transparency\n",
        "}\n",
        "\n",
        "\n",
        "# Optional: Configure JAX for 64-bit precision for potential numerical stability\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "initial_key = jrandom.PRNGKey(0)\n",
        "\n",
        "## Loading Data\n",
        "\n",
        "# We'll use the data provided in the `lindata.mat` file.\n",
        "# This file contains input features $X$, corresponding output values $Y$, and the known standard deviation of the observation noise $\\\\sigma$.\n",
        "\n",
        "data = scipy.io.loadmat(\"lindata.mat\")\n",
        "X_all = data[\"X\"]  # inputs (N, 1)\n",
        "Y_all = data[\"Y\"][:, 0]  # outputs (N,)\n",
        "sigma_noise = data[\"sigma\"][0].flatten()[0]  # Noise standard deviation (scalar)\n",
        "N_total = X_all.shape[0]  # Total number of data points\n",
        "\n",
        "print(f\"Data loaded: {N_total} points\")\n",
        "print(f\"Input shape (X): {X_all.shape}\")\n",
        "print(f\"Output shape (Y): {Y_all.shape}\")\n",
        "print(f\"Noise standard deviation (sigma): {sigma_noise}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY2SUMDudprM"
      },
      "source": [
        "## The Linear Model\n",
        "\n",
        "We are modeling the relationship between the input $x$ and output $y$ using a simple linear model:\n",
        "$$ y = w_0 + w_1 x + \\epsilon $$\n",
        "where $w\\_0$ is the intercept, $w\\_1$ is the slope, and $\\\\epsilon$ is observation noise. We combine the parameters into a vector $w = \\\\begin{bmatrix} w\\_0 \\\\ w\\_1 \\\\end{bmatrix}$.\n",
        "\n",
        "To write this in a more general linear regression form, we use a **feature function** $\\\\phi(x)$ that transforms the input $x$ into a feature vector. For this simple linear model, $\\\\phi(x) = \\\\begin{bmatrix} 1 \\\\ x \\\\end{bmatrix}$. Then the model becomes:\n",
        "$$ y = \\phi(x)^T w + \\epsilon $$\n",
        "\n",
        "In our probabilistic setting, we assume the noise $\\\\epsilon$ is Gaussian, $\\\\epsilon \\\\sim \\\\mathcal{N}(0, \\\\sigma^2)$, where $\\\\sigma^2$ is the noise variance (square of `sigma_noise`). This means the likelihood of observing $y$ given $x$ and the parameters $w$ is a Gaussian centered at $\\\\phi(x)^T w$ with variance $\\\\sigma^2$:\n",
        "$$ p(y | x, w) = \\mathcal{N}(y; \\phi(x)^T w, \\sigma^2) $$\n",
        "\n",
        "In the case of multiple data points $(X, Y)$, assuming they are independent given $w$, the joint likelihood $p(Y\\_{select} | X\\_{select}, w)$ is a multivariate Gaussian:\n",
        "$$ p(Y_{select} | X_{select}, w) = \\mathcal{N}(Y_{select}; \\Phi_{select} w, \\sigma^2 I) $$\n",
        "where $\\\\Phi\\_{select}$ is the matrix where each row is $\\\\phi(x\\_i)^T$ for the selected inputs $x\\_i$, and $I$ is the identity matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZS1NDUvDdprM"
      },
      "outputs": [],
      "source": [
        "# --- Define the feature function ---\n",
        "def phi(x):\n",
        "  \"\"\"\n",
        "  Feature function for simple linear regression: [1, x]\n",
        "  Accepts a single scalar x or a JAX array of shape (N, 1).\n",
        "  Returns a JAX array of shape (N, num_features) or (num_features,).\n",
        "  \"\"\"\n",
        "  if jnp.ndim(x) == 0: # Handle scalar input\n",
        "      return jnp.array([1.0, x])\n",
        "  else: # Handle array input (N, 1)\n",
        "      return jnp.hstack([jnp.ones_like(x), x])\n",
        "\n",
        "# Example usage:\n",
        "x_example = 2.0\n",
        "phi_example = phi(x_example)\n",
        "print(f\"phi({x_example}) = {phi_example}\")\n",
        "\n",
        "X_subset_example = X_all[:3] if X_all is not None else jnp.array([[0.],[1.],[2.]])\n",
        "Phi_subset_example = phi(X_subset_example)\n",
        "print(f\"phi(subset of X):\\n{Phi_subset_example}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM2bxCX1dprM"
      },
      "source": [
        "## The Gaussian Prior\n",
        "\n",
        "In the Bayesian approach, we start with a **prior distribution** over the parameters $w$. This prior represents our beliefs about the parameters before observing any data. For mathematical convenience, and because it is often a reasonable choice when we have some idea about the range and relationship of parameters, we choose a **Gaussian prior** for $w$:\n",
        "$$ p(w) = \\mathcal{N}(w; \\mu_{prior}, \\Sigma_{prior}) $$\n",
        "where $\\\\mu\\_{prior} \\\\in \\\\mathbb{R}^2$ is the prior mean vector and $\\\\Sigma\\_{prior} \\\\in \\\\mathbb{R}^{2 \\\\times 2}$ is the prior covariance matrix.\n",
        "\n",
        "  * $\\\\mu\\_{prior}$: Our initial best guess for the values of $w\\_0$ and $w\\_1$.\n",
        "  * $\\\\Sigma\\_{prior}$: Our initial uncertainty about $w$. The diagonal elements represent the variance of our belief about $w\\_0$ and $w\\_1$ independently. The off-diagonal elements represent how our belief about $w\\_0$ is correlated with our belief about $w\\_1$. A large diagonal value means high uncertainty.\n",
        "\n",
        "In the interactive plot, you can adjust the parameters of this prior distribution. We define the 2x2 prior covariance matrix using the variance of $w\\_0$ ($\\\\Sigma\\_{11}$), the variance of $w\\_1$ ($\\\\Sigma\\_{22}$), and the correlation coefficient ($\\\\rho$) between them:\n",
        "$$ \\Sigma_{prior} = \\begin{bmatrix} \\Sigma_{11} & \\rho \\sqrt{\\Sigma_{11}\\Sigma_{22}} \\\\ \\rho \\sqrt{\\Sigma_{11}\\Sigma_{22}} & \\Sigma_{22} \\end{bmatrix} $$\n",
        "\n",
        "## Bayesian Inference: Computing the Posterior\n",
        "\n",
        "The goal of Bayesian inference is to update our prior beliefs about $w$ using the observed data $(X\\_{select}, Y\\_{select})$. The updated belief is represented by the **posterior distribution**:\n",
        "$$ p(w | X_{select}, Y_{select}) = \\frac{p(Y_{select} | X_{select}, w) p(w)}{p(Y_{select} | X_{select})} $$\n",
        "where $p(Y\\_{select} | X\\_{select})$ is the marginal likelihood, a normalization constant.\n",
        "\n",
        "Since we chose a Gaussian prior $p(w)$ and the likelihood $p(Y\\_{select} | X\\_{select}, w)$ is a Gaussian (as explained above), and because the Gaussian distribution is its own conjugate prior for a Gaussian likelihood, the resulting posterior distribution $p(w | X\\_{select}, Y\\_{select})$ is also **Gaussian**:\n",
        "$$ p(w | X_{select}, Y_{select}) = \\mathcal{N}(w; \\mu_{posterior}, \\Sigma_{posterior}) $$\n",
        "\n",
        "The parameters of the posterior distribution, $\\\\mu\\_{posterior}$ and $\\\\Sigma\\_{posterior}$, are updated from the prior parameters ($\\\\mu\\_{prior}, \\\\Sigma\\_{prior}$) and the selected data $(X\\_{select}, Y\\_{select})$ using specific analytical formulas derived from Gaussian conditioning. These are the same formulas we discussed in Lecture 06 for conditioning a Gaussian variable on another linearly related Gaussian variable. If $w \\\\sim \\\\mathcal{N}(\\\\mu\\_{prior}, \\\\Sigma\\_{prior})$ and $Y\\_{select} \\\\sim \\\\mathcal{N}(\\\\Phi\\_{select} w, \\\\sigma^2 I)$, then $w | Y\\_{select} \\\\sim \\\\mathcal{N}(\\\\mu\\_{posterior}, \\\\Sigma\\_{posterior})$.\n",
        "\n",
        "The original script uses a `Gaussian` class with a `.condition()` method. This method encapsulates these analytical formulas for computing the posterior mean and covariance given the prior Gaussian, the feature matrix $\\\\Phi\\_{select}$, the observed data $Y\\_{select}$, and the noise covariance $\\\\sigma^2 I$.\n",
        "\n",
        "## The Role of Selected Data\n",
        "\n",
        "The power of interactive exploration here comes from selecting *which* data points we condition our posterior on. Initially, with no data selected, the posterior is the same as the prior. As you select data points, the posterior distribution (and the corresponding function space) will update to reflect the information gained from those specific observations.\n",
        "\n",
        "## Interactive Bayesian Linear Regression with Plotly\n",
        "\n",
        "We will now set up the interactive controls using `ipywidgets` and link them to a function that performs the Bayesian update and generates the Plotly plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acT4AQcGdprM"
      },
      "outputs": [],
      "source": [
        "# --- Define interactive widgets ---\n",
        "\n",
        "# Sliders for the prior mean (2D)\n",
        "mu0_prior_slider = widgets.FloatSlider(min=-5., max=5., value=0., step=.1, description='Prior Mu_0:')\n",
        "mu1_prior_slider = widgets.FloatSlider(min=-5., max=5., value=0., step=.1, description='Prior Mu_1:')\n",
        "\n",
        "# Sliders for the prior covariance matrix parameters\n",
        "# Sigma_prior = [[S11, rho*sqrt(S11*S22)], [rho*sqrt(S11*S22), S22]]\n",
        "s11_prior_slider = widgets.FloatSlider(min=0.1, max=5., value=1., step=.1, description='Prior Sigma_11:')\n",
        "s22_prior_slider = widgets.FloatSlider(min=0.1, max=5., value=1., step=.1, description='Prior Sigma_22:')\n",
        "rho_prior_slider = widgets.FloatSlider(min=-0.99, max=0.99, value=0., step=.01, description='Prior rho:')\n",
        "\n",
        "# Widget to select data points\n",
        "# Display point indices to select\n",
        "data_selector = widgets.SelectMultiple(\n",
        "    options=[(f'Point {i+1} (x={X_all[i,0]:.2f}, y={Y_all[i]:.2f})', i) for i in range(N_total)] if X_all is not None else [],\n",
        "    description='Select Data Points:',\n",
        "    disabled=(X_all is None),\n",
        "    layout={'width': '400px'}\n",
        ")\n",
        "\n",
        "\n",
        "# --- Function to update plots based on widget values ---\n",
        "\n",
        "def update_regression_plot_plotly(mu0_prior, mu1_prior, s11_prior, s22_prior, rho_prior, selected_indices):\n",
        "    if X_all is None or Y_all is None or sigma_noise is None:\n",
        "        print(\"Data not loaded. Cannot update plot.\")\n",
        "        # Display an empty plot or message in the output area\n",
        "        fig = go.Figure()\n",
        "        fig.update_layout(title=\"Data not loaded.\")\n",
        "        fig.show()\n",
        "        return\n",
        "\n",
        "    # Construct the prior mean and covariance matrix\n",
        "    mu_prior = jnp.asarray([mu0_prior, mu1_prior])\n",
        "    # Ensure s11 and s22 are positive for sqrt\n",
        "    s11_safe = jnp.maximum(s11_prior, 1e-6)\n",
        "    s22_safe = jnp.maximum(s22_prior, 1e-6)\n",
        "    # Ensure rho is within valid range [-1, 1]\n",
        "    rho_safe = jnp.clip(rho_prior, -0.999, 0.999)\n",
        "\n",
        "    S12_prior = rho_safe * jnp.sqrt(s11_safe * s22_safe)\n",
        "    Sigma_prior = jnp.asarray([[s11_safe, S12_prior], [S12_prior, s22_safe]])\n",
        "\n",
        "    # Create the prior Gaussian distribution object\n",
        "    prior_dist = Gaussian(mu=mu_prior, Sigma=Sigma_prior)\n",
        "\n",
        "    # Select the data points based on indices\n",
        "    X_select = X_all[list(selected_indices)]\n",
        "    Y_select = Y_all[list(selected_indices)]\n",
        "    # Noise covariance matrix for selected data (assuming iid noise)\n",
        "    Lambda_select_sq = sigma_noise**2 * jnp.eye(len(selected_indices)) if len(selected_indices) > 0 else None\n",
        "\n",
        "\n",
        "    # Compute the posterior distribution\n",
        "    if len(selected_indices) > 0:\n",
        "        # The .condition() method of the Gaussian class computes the posterior\n",
        "        posterior_dist = prior_dist.condition(phi(X_select), Y_select, Lambda_select_sq)\n",
        "    else:\n",
        "        # If no data is selected, the posterior is the same as the prior\n",
        "        posterior_dist = prior_dist\n",
        "\n",
        "    # Regenerate key for samples each update\n",
        "    global initial_key\n",
        "    initial_key, subkey = jrandom.split(initial_key)\n",
        "\n",
        "    # --- Prepare data for Plotly plotting ---\n",
        "\n",
        "    # Parameter space plot data (Contours and Samples)\n",
        "    n_contour_levels = 3 # Plot contours at 1, 2, 3 standard deviations\n",
        "    theta = jnp.linspace(0, 2 * jnp.pi, 100)\n",
        "    circle_pts = jnp.stack([jnp.cos(theta), jnp.sin(theta)], axis=1) # Unit circle points (100, 2)\n",
        "\n",
        "    # Prior contour points\n",
        "    prior_contour_pts = []\n",
        "    if prior_dist.L is not None:\n",
        "        for i in range(1, n_contour_levels + 1):\n",
        "             pts = prior_dist.mu + i * jnp.dot(circle_pts, prior_dist.L.T)\n",
        "             prior_contour_pts.append(pts)\n",
        "\n",
        "    # Posterior contour points\n",
        "    posterior_contour_pts = []\n",
        "    if posterior_dist.L is not None:\n",
        "         for i in range(1, n_contour_levels + 1):\n",
        "             pts = posterior_dist.mu + i * jnp.dot(circle_pts, posterior_dist.L.T)\n",
        "             posterior_contour_pts.append(pts)\n",
        "\n",
        "    # Sample from prior and posterior parameter space\n",
        "    num_samples_param_space = 10 # Plot a few samples\n",
        "    prior_samples_param_space = prior_dist.sample(subkey, num_samples_param_space) if prior_dist.L is not None else None\n",
        "    key, subkey = jrandom.split(subkey) # Use new key for posterior samples\n",
        "    posterior_samples_param_space = posterior_dist.sample(subkey, num_samples_param_space) if posterior_dist.L is not None else None\n",
        "\n",
        "\n",
        "    # Function space plot data (Data, Mean Functions, Uncertainty Bands, Sample Functions)\n",
        "    x_plot = jnp.linspace(-5, 5, 100)[:, None] # X values for plotting functions\n",
        "    phi_plot = phi(x_plot) # Feature matrix for plotting\n",
        "\n",
        "    # Prior mean function and uncertainty band (+/- 2 std dev)\n",
        "    prior_mean_f = jnp.dot(phi_plot, prior_dist.mu)\n",
        "    prior_var_f = jnp.sum(phi_plot * jnp.dot(phi_plot, prior_dist.Sigma), axis=1)\n",
        "    prior_std_f = jnp.sqrt(prior_var_f)\n",
        "    prior_upper_f = prior_mean_f + 2 * prior_std_f\n",
        "    prior_lower_f = prior_mean_f - 2 * prior_std_f\n",
        "\n",
        "    # Posterior mean function and uncertainty band (+/- 2 std dev)\n",
        "    posterior_mean_f = jnp.dot(phi_plot, posterior_dist.mu)\n",
        "    posterior_var_f = jnp.sum(phi_plot * jnp.dot(phi_plot, posterior_dist.Sigma), axis=1)\n",
        "    posterior_std_f = jnp.sqrt(posterior_var_f)\n",
        "    posterior_upper_f = posterior_mean_f + 2 * posterior_std_f\n",
        "    posterior_lower_f = posterior_mean_f - 2 * posterior_std_f\n",
        "\n",
        "    # Sample functions from prior and posterior\n",
        "    num_samples_func_space = 5 # Plot a few sample functions\n",
        "    key, subkey = jrandom.split(subkey) # Use new key for function samples\n",
        "    prior_samples_param_space_func = prior_dist.sample(subkey, num_samples_func_space) if prior_dist.L is not None else None\n",
        "    prior_func_samples = jnp.dot(phi_plot, prior_samples_param_space_func.T) if prior_samples_param_space_func is not None else None\n",
        "\n",
        "    key, subkey = jrandom.split(subkey) # Use new key for posterior function samples\n",
        "    posterior_samples_param_space_func = posterior_dist.sample(subkey, num_samples_func_space) if posterior_dist.L is not None else None\n",
        "    posterior_func_samples = jnp.dot(phi_plot, posterior_samples_param_space_func.T) if posterior_samples_param_space_func is not None else None\n",
        "\n",
        "\n",
        "    # --- Create Plotly Figures ---\n",
        "\n",
        "    # Create figure with two subplots\n",
        "    fig = go.Figure(\n",
        "        layout=go.Layout(\n",
        "            title=\"Bayesian Linear Regression\",\n",
        "            grid=dict(\n",
        "                rows=1,\n",
        "                columns=2,\n",
        "                pattern=\"independent\",\n",
        "            ),\n",
        "            showlegend=True,\n",
        "            width=1000, # Adjust figure width as needed\n",
        "            height=500, # Adjust figure height as needed\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # --- Plot 1: Parameter Space ---\n",
        "\n",
        "    # Plot prior contours\n",
        "    for i, pts in enumerate(prior_contour_pts):\n",
        "        fig.add_trace(go.Scattergl(x=np.array(pts[:, 0]), y=np.array(pts[:, 1]), mode='lines',\n",
        "                                  line=dict(color=PLOTLY_COLORS[\"gray\"], dash='dash', width=2.5/(i+1)),\n",
        "                                  name=f'Prior {i+1}σ Contour', showlegend=(i==0), # Show legend only once\n",
        "                                  xaxis='x1', yaxis='y1'))\n",
        "    # Plot prior mean\n",
        "    fig.add_trace(go.Scattergl(x=[prior_dist.mu[0]], y=[prior_dist.mu[1]], mode='markers',\n",
        "                               marker=dict(color=PLOTLY_COLORS[\"gray\"], size=8),\n",
        "                               name='Prior Mean', showlegend=True,\n",
        "                               xaxis='x1', yaxis='y1'))\n",
        "    # Plot prior samples\n",
        "    if prior_samples_param_space is not None:\n",
        "         fig.add_trace(go.Scattergl(x=np.array(prior_samples_param_space[:, 0]), y=np.array(prior_samples_param_space[:, 1]),\n",
        "                                   mode='markers', name='Prior Samples',\n",
        "                                   marker=dict(size=4, opacity=0.8, color=PLOTLY_COLORS[\"dark\"]),\n",
        "                                   showlegend=True, xaxis='x1', yaxis='y1'))\n",
        "\n",
        "\n",
        "    # Plot Likelihood (MLE point) if data is selected\n",
        "    w_mle = None\n",
        "    if len(selected_indices) > 0:\n",
        "         try:\n",
        "             Phi_select = phi(X_select)\n",
        "             # Check if Phi_select.T @ Phi_select is invertible\n",
        "             if jnp.linalg.det(jnp.dot(Phi_select.T, Phi_select)) > 1e-6:\n",
        "                 w_mle = jnp.linalg.solve(jnp.dot(Phi_select.T, Phi_select), jnp.dot(Phi_select.T, Y_select))\n",
        "                 fig.add_trace(go.Scattergl(x=[w_mle[0]], y=[w_mle[1]], mode='markers',\n",
        "                                          marker=dict(color=PLOTLY_COLORS[\"blue\"], size=8),\n",
        "                                          name='Likelihood (MLE)', showlegend=True,\n",
        "                                          xaxis='x1', yaxis='y1'))\n",
        "             # Else: not enough distinct points, MLE is not unique, don't plot point\n",
        "         except Exception as e:\n",
        "             print(f\"Error calculating MLE point: {e}\")\n",
        "\n",
        "\n",
        "    # Plot posterior contours\n",
        "    for i, pts in enumerate(posterior_contour_pts):\n",
        "        fig.add_trace(go.Scattergl(x=np.array(pts[:, 0]), y=np.array(pts[:, 1]), mode='lines',\n",
        "                                  line=dict(color=PLOTLY_COLORS[\"red\"], dash='dash', width=2.5/(i+1)),\n",
        "                                  name=f'Posterior {i+1}σ Contour', showlegend=(i==0), # Show legend only once\n",
        "                                  xaxis='x1', yaxis='y1'))\n",
        "    # Plot posterior mean\n",
        "    fig.add_trace(go.Scattergl(x=[posterior_dist.mu[0]], y=[posterior_dist.mu[1]], mode='markers',\n",
        "                               marker=dict(color=PLOTLY_COLORS[\"red\"], size=8),\n",
        "                               name='Posterior Mean', showlegend=True,\n",
        "                               xaxis='x1', yaxis='y1'))\n",
        "    # Plot posterior samples\n",
        "    if posterior_samples_param_space is not None:\n",
        "        fig.add_trace(go.Scattergl(x=np.array(posterior_samples_param_space[:, 0]), y=np.array(posterior_samples_param_space[:, 1]),\n",
        "                                   mode='markers', name='Posterior Samples',\n",
        "                                   marker=dict(size=4, opacity=0.8, color=PLOTLY_COLORS[\"red\"]),\n",
        "                                   showlegend=True, xaxis='x1', yaxis='y1'))\n",
        "\n",
        "\n",
        "    # Update layout for parameter space plot\n",
        "    fig.update_layout(\n",
        "        xaxis1=dict(\n",
        "            title='$w_0$ (Intercept)',\n",
        "            range=[-3, 3], # Fixed limits\n",
        "            scaleanchor=\"y1\", scaleratio=1, # Equal aspect ratio\n",
        "            domain=[0, 0.48] # Position in the subplot grid\n",
        "        ),\n",
        "        yaxis1=dict(\n",
        "            title='$w_1$ (Slope)',\n",
        "            range=[-3, 3], # Fixed limits\n",
        "            domain=[0, 1] # Position in the subplot grid\n",
        "        ),\n",
        "        title_x=0.5, # Center the main title\n",
        "        title_y=0.95 # Position the main title slightly lower\n",
        "    )\n",
        "\n",
        "\n",
        "    # --- Plot 2: Function Space ---\n",
        "\n",
        "    # Plot all data points\n",
        "    fig.add_trace(go.Scattergl(x=np.array(X_all[:, 0]), y=np.array(Y_all),\n",
        "                               mode='markers', name='All Data',\n",
        "                               marker=dict(color=PLOTLY_COLORS[\"dark\"], size=5),\n",
        "                               error_y=dict(type='data', array=np.array(sigma_noise * jnp.ones_like(Y_all))),\n",
        "                               showlegend=True, xaxis='x2', yaxis='y2'))\n",
        "    # Highlight selected data points\n",
        "    if len(selected_indices) > 0:\n",
        "         fig.add_trace(go.Scattergl(x=np.array(X_select[:, 0]), y=np.array(Y_select),\n",
        "                                    mode='markers', name='Selected Data',\n",
        "                                    marker=dict(color=PLOTLY_COLORS[\"red\"], size=7, line=dict(width=1, color='DarkRed')),\n",
        "                                    error_y=dict(type='data', array=np.array(sigma_noise * jnp.ones_like(Y_select))),\n",
        "                                    showlegend=True, xaxis='x2', yaxis='y2'))\n",
        "\n",
        "\n",
        "    # Plot prior mean function\n",
        "    fig.add_trace(go.Scattergl(x=np.array(x_plot[:, 0]), y=np.array(prior_mean_f),\n",
        "                               mode='lines', name='Prior Mean',\n",
        "                               line=dict(color=PLOTLY_COLORS[\"dark\"], width=2),\n",
        "                               showlegend=True, xaxis='x2', yaxis='y2'))\n",
        "    # Plot prior uncertainty band (+/- 2 std dev)\n",
        "    fig.add_trace(go.Scattergl(x=np.array(x_plot[:, 0]), y=np.array(prior_upper_f), mode='lines',\n",
        "                               line=dict(width=0), name='Prior +2σ', showlegend=False, xaxis='x2', yaxis='y2'))\n",
        "    fig.add_trace(go.Scattergl(x=np.array(x_plot[:, 0]), y=np.array(prior_lower_f), mode='lines',\n",
        "                               line=dict(width=0), name='Prior -2σ', showlegend=False, xaxis='x2', yaxis='y2'),\n",
        "                               fill='tonexty', fillcolor=PLOTLY_COLORS[\"dark_alpha\"]) # Fill between upper and lower\n",
        "\n",
        "\n",
        "    # Plot prior function samples\n",
        "    if prior_func_samples is not None:\n",
        "         for i in range(prior_func_samples.shape[1]):\n",
        "              fig.add_trace(go.Scattergl(x=np.array(x_plot[:, 0]), y=np.array(prior_func_samples[:, i]), mode='lines',\n",
        "                                       line=dict(color=PLOTLY_COLORS[\"dark\"], width=1, opacity=0.3),\n",
        "                                       name='Prior Sample Functions', showlegend=False, # Don't show legend for each sample\n",
        "                                       xaxis='x2', yaxis='y2'))\n",
        "\n",
        "\n",
        "    # Plot posterior mean function\n",
        "    fig.add_trace(go.Scattergl(x=np.array(x_plot[:, 0]), y=np.array(posterior_mean_f),\n",
        "                               mode='lines', name='Posterior Mean',\n",
        "                               line=dict(color=PLOTLY_COLORS[\"red\"], width=2),\n",
        "                               showlegend=True, xaxis='x2', yaxis='y2'))\n",
        "    # Plot posterior uncertainty band (+/- 2 std dev)\n",
        "    fig.add_trace(go.Scattergl(x=np.array(x_plot[:, 0]), y=np.array(posterior_upper_f), mode='lines',\n",
        "                               line=dict(width=0), name='Posterior +2σ', showlegend=False, xaxis='x2', yaxis='y2'))\n",
        "    fig.add_trace(go.Scattergl(x=np.array(x_plot[:, 0]), y=np.array(posterior_lower_f), mode='lines',\n",
        "                               line=dict(width=0), name='Posterior -2σ', showlegend=False, xaxis='x2', yaxis='y2'),\n",
        "                               fill='tonexty', fillcolor=PLOTLY_COLORS[\"red_alpha\"]) # Fill between upper and lower\n",
        "\n",
        "\n",
        "    # Plot posterior function samples\n",
        "    if posterior_func_samples is not None:\n",
        "         for i in range(posterior_func_samples.shape[1]):\n",
        "             fig.add_trace(go.Scattergl(x=np.array(x_plot[:, 0]), y=np.array(posterior_func_samples[:, i]), mode='lines',\n",
        "                                       line=dict(color=PLOTLY_COLORS[\"red\"], width=1, opacity=0.5),\n",
        "                                       name='Posterior Sample Functions', showlegend=False, # Don't show legend for each sample\n",
        "                                       xaxis='x2', yaxis='y2'))\n",
        "\n",
        "    # Plot Likelihood (MLE Function) if calculated\n",
        "    if w_mle is not None: # Check if MLE was calculated and stored\n",
        "         mle_func = jnp.dot(phi_plot, w_mle)\n",
        "         fig.add_trace(go.Scattergl(x=np.array(x_plot[:, 0]), y=np.array(mle_func), mode='lines',\n",
        "                                    line=dict(color=PLOTLY_COLORS[\"blue\"], dash='dash', width=2),\n",
        "                                    name='Likelihood (MLE Function)', showlegend=True,\n",
        "                                    xaxis='x2', yaxis='y2'))\n",
        "\n",
        "\n",
        "    # Update layout for function space plot\n",
        "    fig.update_layout(\n",
        "        xaxis2=dict(\n",
        "            title='$x$',\n",
        "             range=[-5, 5], # Fixed limits\n",
        "            domain=[0.52, 1] # Position in the subplot grid\n",
        "        ),\n",
        "        yaxis2=dict(\n",
        "            title='$y$',\n",
        "            range=[-10, 10], # Fixed limits\n",
        "            domain=[0, 1] # Position in the subplot grid\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Final layout adjustments\n",
        "    fig.update_layout(\n",
        "         hovermode='closest',\n",
        "         legend=dict(x=0.01, y=0.99), # Position the legend (adjust as needed)\n",
        "         margin=dict(l=20, r=20, t=40, b=20), # Adjust margins\n",
        "         title='Bayesian Linear Regression: Parameter and Function Space'\n",
        "    )\n",
        "\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "# --- Display widgets and link to update function ---\n",
        "\n",
        "if X_all is not None: # Only display widgets if data was loaded\n",
        "    # Arrange widgets\n",
        "    prior_controls = widgets.VBox([\n",
        "        widgets.Label(\"Prior Parameters:\"),\n",
        "        mu0_prior_slider,\n",
        "        mu1_prior_slider,\n",
        "        s11_prior_slider,\n",
        "        s22_prior_slider,\n",
        "        rho_prior_slider,\n",
        "    ])\n",
        "\n",
        "    data_selection_control = widgets.VBox([\n",
        "        widgets.Label(\"Data Selection:\"),\n",
        "        data_selector,\n",
        "    ])\n",
        "\n",
        "    # Using HBox to place controls side-by-side\n",
        "    controls = widgets.HBox([prior_controls, data_selection_control])\n",
        "\n",
        "\n",
        "    # Link widgets to the update function\n",
        "    # Ensure widget names match function argument names\n",
        "    interactive_plot = widgets.interactive_output(\n",
        "        update_regression_plot_plotly,\n",
        "        {\n",
        "            'mu0_prior': mu0_prior_slider,\n",
        "            'mu1_prior': mu1_prior_slider,\n",
        "            's11_prior': s11_prior_slider,\n",
        "            's22_prior': s22_prior_slider,\n",
        "            'rho_prior': rho_prior_slider,\n",
        "            'selected_indices': data_selector,\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "    # Display controls and the plot output\n",
        "    display(controls, interactive_plot)\n",
        "\n",
        "else:\n",
        "    print(\"Cannot display interactive widgets because data was not loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJzIDXKCdprN"
      },
      "source": [
        "### Explanation of Bayesian Linear Regression\n",
        "\n",
        "At its core, Bayesian linear regression is about finding a probability distribution over the possible straight lines (or hyperplanes in higher dimensions) that could have generated the data. Instead of finding a single \"best\" line, we maintain a belief about what the parameters $w\\_0$ (intercept) and $w\\_1$ (slope) could be, represented by a joint probability distribution $p(w\\_0, w\\_1)$.\n",
        "\n",
        "1.  **The Model:** We assume the data is generated by a linear function corrupted by Gaussian noise: $y = w\\_0 + w\\_1 x + \\\\epsilon$, where $\\\\epsilon \\\\sim \\\\mathcal{N}(0, \\\\sigma^2)$.\n",
        "2.  **The Prior:** We start with a prior belief about the parameters $w$. A common and mathematically convenient choice is a Gaussian prior $p(w) = \\\\mathcal{N}(w; \\\\mu\\_{prior}, \\\\Sigma\\_{prior})$. This distribution reflects our initial uncertainty about the intercept and slope before seeing any data.\n",
        "      - In the **parameter space plot** (left), the contours of the prior Gaussian show regions of higher probability for the $[w\\_0, w\\_1]$ pair. The center of the ellipse is the prior mean $\\\\mu\\_{prior}$. The shape and orientation of the ellipse are determined by the prior covariance $\\\\Sigma\\_{prior}$.\n",
        "      - In the **function space plot** (right), the prior mean function $f(x) = \\\\phi(x)^T \\\\mu\\_{prior}$ is shown as a line. The uncertainty band around it shows the range of function values (e.g., $\\\\pm 2$ standard deviations) predicted by the prior distribution over parameters. Each sample from the prior in parameter space corresponds to a specific line in function space, illustrating the variety of functions considered plausible under the prior.\n",
        "3.  **The Likelihood:** The likelihood $p(y | x, w)$ tells us the probability of observing a data point $(x, y)$ given specific parameter values $w$. Due to the Gaussian noise assumption, this likelihood is Gaussian: the observed $y$ is likely to be close to $\\\\phi(x)^T w$. For multiple independent data points, the joint likelihood $p(Y\\_{select} | X\\_{select}, w)$ is also Gaussian.\n",
        "      - In the **parameter space plot**, the \"Likelihood (MLE)\" point represents the parameter values that maximize the likelihood for the selected data – effectively, the line that best fits *only* the selected data according to the least squares criterion. (Note: The likelihood itself is a function of $w$ for fixed data, not a distribution over $w$ that you can sample from directly).\n",
        "      - In the **function space plot**, the \"Likelihood (MLE Function)\" shows the line corresponding to the MLE parameters. The selected data points are also highlighted.\n",
        "4.  **The Posterior:** When we observe data, we update our prior belief to get the posterior distribution $p(w | X\\_{select}, Y\\_{select})$. Thanks to the conjugate property of Gaussian priors with Gaussian likelihoods, the posterior is also Gaussian, but with updated mean $\\\\mu\\_{posterior}$ and covariance $\\\\Sigma\\_{posterior}$.\n",
        "      - The **posterior mean** $\\\\mu\\_{posterior}$ is a weighted average of the prior mean and the information from the data (specifically, the MLE). As you add more data, especially informative data, the posterior mean will move towards the MLE.\n",
        "      - The **posterior covariance** $\\\\Sigma\\_{posterior}$ is smaller than the prior covariance, reflecting a reduction in uncertainty about the parameters after observing data. As you add more data, the posterior ellipse in parameter space will shrink.\n",
        "      - In the **parameter space plot** (left subplot), the posterior contours and samples show the updated belief about $w$.\n",
        "      - In the **function space plot** (right subplot), the posterior mean function shows the line that best fits the selected data, considering the prior. The posterior uncertainty band is typically narrower than the prior band, reflecting increased certainty about the function after seeing data. Samples from the posterior in function space show lines that are plausible given the data and the prior.\n",
        "\n",
        "By selecting data points using the interactive widget, you can observe how the posterior distribution shifts and shrinks, and how this translates into a more certain belief about the linear relationship between $x$ and $y$ in the function space. The posterior mean function becomes a better fit to the selected data, and the uncertainty band narrows, particularly in regions where data has been observed.\n",
        "\n",
        "This interactive notebook allows you to visualize this core process of Bayesian learning: starting with a belief (prior), observing evidence (data), and updating that belief (posterior).\n",
        "\n",
        "```\n",
        "\n",
        "This raw Markdown content includes the text and code blocks for the Bayesian Linear Regression notebook with `ipywidgets` and Plotly, formatted with `$` and `$$` for KaTeX compatibility. You can copy and paste this directly into a Markdown cell in your Jupyter notebook.\n",
        "\n",
        "**Key changes for Plotly:**\n",
        "\n",
        "* Replaced `matplotlib.pyplot` imports and calls with `plotly.graph_objects` (`go`).\n",
        "* Created a single `go.Figure` with a grid layout for the two subplots.\n",
        "* Used `fig.add_trace` to add all plot elements (scatter points, lines, filled areas) to the figure, specifying which subplot axis (`xaxis='x1', yaxis='y1'` for parameter space, `xaxis='x2', yaxis='y2'` for function space).\n",
        "* Replicated contour plotting by drawing lines based on points sampled around ellipses.\n",
        "* Replicated `fill_between` for uncertainty bands by plotting the upper bound line and filling down to the lower bound line using `fill='tonexty'`.\n",
        "* Set axis titles, ranges, and aspect ratios using `fig.update_layout` and nested axis dictionaries (`xaxis1`, `yaxis1`, `xaxis2`, `yaxis2`).\n",
        "* Used `go.Scattergl` for scatter plots, which is often better for performance with many points.\n",
        "* Added basic color definitions (`PLOTLY_COLORS`) to replace `tueplots.constants.color`.\n",
        "\n",
        "Remember, you will need `ipywidgets`, `plotly`, `jax`, `jaxlib`, and `scipy` installed, and the `lindata.mat` file accessible, for this notebook to run. The simplified `Gaussian` class is included directly for convenience.\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
