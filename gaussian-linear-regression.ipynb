{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n6oPAr_dprK"
      },
      "source": [
        "## Bayesian Linear Regression: Updating Beliefs about Model Parameters (Plotly Interactive)\n",
        "\n",
        "In this notebook, we will explore Bayesian Linear Regression. This is a fundamental probabilistic model where we place a probability distribution over the possible values of our model parameters and update this distribution as we observe data. We'll see how a Gaussian prior distribution, combined with a linear model and Gaussian noise, leads to a Gaussian posterior distribution that can be computed analytically.\n",
        "\n",
        "We will use JAX for numerical computation, `ipywidgets` for interactive controls, and **Plotly** for dynamic and interactive plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "joSolJlJdprL"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jrandom\n",
        "import numpy as np  # Useful for converting JAX arrays for plotting\n",
        "\n",
        "# No matplotlib.pyplot needed in the main update function\n",
        "import scipy.io  # To load the .mat data file\n",
        "import ipywidgets as widgets  # For interactive controls\n",
        "from IPython.display import display  # To display widgets and output\n",
        "from jax.scipy.linalg import cholesky  # For plotting Gaussian contours\n",
        "from gaussians import Gaussian\n",
        "from gaussians_utils import (\n",
        "    phi,\n",
        "    _select_data,\n",
        "    create_prior_distribution,\n",
        "    compute_posterior,\n",
        "    generate_parameter_space_data,\n",
        "    generate_function_space_data,\n",
        ")\n",
        "\n",
        "# Import Plotly\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px  # Can be useful for simpler plots\n",
        "import plotly.io as pio  # For saving figures\n",
        "\n",
        "pio.templates.default = \"plotly_white\"  # Set default template for Plotly\n",
        "\n",
        "# Optional: No direct color import from tueplots needed for Plotly, define colors here\n",
        "PLOTLY_COLORS = {\n",
        "    \"dark\": \"rgba(0,0,0,1.0)\",  # Black\n",
        "    \"gray\": \"rgba(128,128,128,1.0)\",  # Gray\n",
        "    \"blue\": \"rgba(0,0,255,1.0)\",  # Blue\n",
        "    \"red\": \"rgba(255,0,0,1.0)\",  # Red\n",
        "    \"dark_alpha\": \"rgba(0,0,0,0.2)\",  # Black with transparency for bands/samples\n",
        "    \"blue_alpha\": \"rgba(0,0,255,0.2)\",  # Blue with transparency\n",
        "    \"red_alpha\": \"rgba(255,0,0,0.5)\",  # Red with transparency\n",
        "}\n",
        "\n",
        "\n",
        "# Optional: Configure JAX for 64-bit precision for potential numerical stability\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "initial_key = jrandom.PRNGKey(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded: 20 points\n",
            "Input shape (X): (20, 1)\n",
            "Output shape (Y): (20,)\n",
            "Noise standard deviation (sigma): 1.5\n"
          ]
        }
      ],
      "source": [
        "## Loading Data\n",
        "\n",
        "# We'll use the data provided in the `lindata.mat` file.\n",
        "# This file contains input features $X$, corresponding output values $Y$, and the known standard deviation of the observation noise $\\\\sigma$.\n",
        "\n",
        "data = scipy.io.loadmat(\"lindata.mat\")\n",
        "X_all = data[\"X\"]  # inputs (N, 1)\n",
        "Y_all = data[\"Y\"][:, 0]  # outputs (N,)\n",
        "sigma_noise = data[\"sigma\"][0].flatten()[0]  # Noise standard deviation (scalar)\n",
        "N_total = X_all.shape[0]  # Total number of data points\n",
        "\n",
        "print(f\"Data loaded: {N_total} points\")\n",
        "print(f\"Input shape (X): {X_all.shape}\")\n",
        "print(f\"Output shape (Y): {Y_all.shape}\")\n",
        "print(f\"Noise standard deviation (sigma): {sigma_noise}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Linear Model\n",
        "\n",
        "We are modeling the relationship between the input $x$ and output $y$ using a simple linear model:\n",
        "$$ y = w_0 + w_1 x + \\epsilon $$\n",
        "where $w_0$ is the intercept, $w_1$ is the slope, and $\\epsilon$ is observation noise. We combine the parameters into a vector $w = \\begin{bmatrix} w_0 \\ w_1 \\end{bmatrix}$.\n",
        "\n",
        "To write this in a more general linear regression form, we use a feature function $\\phi(x)$ that transforms the input $x$ into a feature vector. For this simple linear model, $\\phi(x) = \\begin{bmatrix} 1 \\ x \\end{bmatrix}$. Then the model becomes:\n",
        "$$ y = \\phi(x)^T w + \\epsilon $$\n",
        "\n",
        "In our probabilistic setting, we assume the noise $\\epsilon$ is Gaussian, $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, where $\\sigma^2$ is the noise variance (square of sigma_noise). This means the likelihood of observing $y$ given $x$ and the parameters $w$ is a Gaussian centered at $\\phi(x)^T w$ with variance $\\sigma^2$:\n",
        "$$ p(y | x, w) = \\mathcal{N}(y; \\phi(x)^T w, \\sigma^2) $$\n",
        "\n",
        "In the case of multiple data points $(X, Y)$, assuming they are independent given $w$, the joint likelihood $p(Y_{select} | X_{select}, w)$ is a multivariate Gaussian:\n",
        "$$ p(Y_{select} | X_{select}, w) = \\mathcal{N}(Y_{select}; \\Phi_{select} w, \\sigma^2 I) $$\n",
        "where $\\Phi_{select}$ is the matrix where each row is $\\phi(x_i)^T$ for the selected inputs $x_i$, and $I$ is the identity matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "phi(2.0) = [1. 2.]\n",
            "phi(subset of X):\n",
            "[[ 1.         -5.        ]\n",
            " [ 1.         -4.47368421]\n",
            " [ 1.         -3.94736842]]\n"
          ]
        }
      ],
      "source": [
        "# --- Define the feature function ---\n",
        "def phi(x):\n",
        "    \"\"\"\n",
        "    Feature function for simple linear regression: [1, x]\n",
        "    Accepts a single scalar x or a JAX array of shape (N, 1).\n",
        "    Returns a JAX array of shape (N, num_features) or (num_features,).\n",
        "    \"\"\"\n",
        "    if jnp.ndim(x) == 0:  # Handle scalar input\n",
        "        return jnp.array([1.0, x])\n",
        "    else:  # Handle array input (N, 1)\n",
        "        return jnp.hstack([jnp.ones_like(x), x])\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "x_example = 2.0\n",
        "phi_example = phi(x_example)\n",
        "print(f\"phi({x_example}) = {phi_example}\")\n",
        "\n",
        "X_subset_example = X_all[:3] if X_all is not None else jnp.array([[0.0], [1.0], [2.0]])\n",
        "Phi_subset_example = phi(X_subset_example)\n",
        "print(f\"phi(subset of X):\\n{Phi_subset_example}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Gaussian Prior\n",
        "\n",
        "In the Bayesian approach, we start with a prior distribution over the parameters $w$. This prior represents our beliefs about the parameters before observing any data. For mathematical convenience, and because it is often a reasonable choice when we have some idea about the range and relationship of parameters, we choose a Gaussian prior for $w$:\n",
        "$$ p(w) = \\mathcal{N}(w; \\mu_{prior}, \\Sigma_{prior}) $$\n",
        "where $\\mu_{prior} \\in \\mathbb{R}^2$ is the prior mean vector and $\\Sigma_{prior} \\in \\mathbb{R}^{2 \\times 2}$ is the prior covariance matrix.\n",
        "\n",
        "*   $\\mu_{prior}$: Our initial best guess for the values of $w_0$ and $w_1$.\n",
        "*   $\\Sigma_{prior}$: Our initial uncertainty about $w$. The diagonal elements represent the variance of our belief about $w_0$ and $w_1$ independently. The off-diagonal elements represent how our belief about $w_0$ is correlated with our belief about $w_1$. A large diagonal value means high uncertainty.\n",
        "\n",
        "In the interactive plot, you can adjust the parameters of this prior distribution. We define the $2 \\times 2$ prior covariance matrix using the variance of $w_0$ ($\\Sigma_{11}$), the variance of $w_1$ ($\\Sigma_{22}$), and the correlation coefficient ($\\rho$) between them:\n",
        "$$\n",
        "\\Sigma_{prior} =\n",
        "\\begin{bmatrix}\n",
        "\\Sigma_{11} & \\rho \\sqrt{\\Sigma_{11}\\Sigma_{22}} \\\\\n",
        "\\rho \\sqrt{\\Sigma_{11}\\Sigma_{22}} & \\Sigma_{22}\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bayesian Inference: Computing the Posterior\n",
        "\n",
        "The goal of Bayesian inference is to update our prior beliefs about $w$ using the observed data $(X_{select}, Y_{select})$. The updated belief is represented by the posterior distribution:\n",
        "$$ p(w | X_{select}, Y_{select}) = \\frac{p(Y_{select} | X_{select}, w) p(w)}{p(Y_{select} | X_{select})} $$\n",
        "where $p(Y_{select} | X_{select})$ is the marginal likelihood, a normalization constant.\n",
        "\n",
        "Since we chose a Gaussian prior $p(w)$ and the likelihood $p(Y_{select} | X_{select}, w)$ is a Gaussian (as explained above), and because the Gaussian distribution is its own conjugate prior for a Gaussian likelihood, the resulting posterior distribution $p(w | X_{select}, Y_{select})$ is also Gaussian:\n",
        "$$ p(w | X_{select}, Y_{select}) = \\mathcal{N}(w; \\mu_{posterior}, \\Sigma_{posterior}) $$\n",
        "\n",
        "The parameters of the posterior distribution, $\\mu_{posterior}$ and $\\Sigma_{posterior}$, are updated from the prior parameters ($\\mu_{prior}, \\Sigma_{prior}$) and the selected data $(X_{select}, Y_{select})$ using specific analytical formulas derived from Gaussian conditioning. These are the same formulas we discussed in Lecture 06 for conditioning a Gaussian variable on another linearly related Gaussian variable. If $w \\sim \\mathcal{N}(\\mu_{prior}, \\Sigma_{prior})$ and $Y_{select} \\sim \\mathcal{N}(\\Phi_{select} w, \\sigma^2 I)$, then $w | Y_{select} \\sim \\mathcal{N}(\\mu_{posterior}, \\Sigma_{posterior})$.\n",
        "\n",
        "The original script uses a Gaussian class with a .condition() method. This method encapsulates these analytical formulas for computing the posterior mean and covariance given the prior Gaussian, the feature matrix $\\Phi_{select}$, the observed data $Y_{select}$, and the noise covariance $\\sigma^2 I$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Role of Selected Data\n",
        "\n",
        "The power of interactive exploration here comes from selecting which data points we condition our posterior on. Initially, with no data selected, the posterior is the same as the prior. As you select data points, the posterior distribution (and the corresponding function space) will update to reflect the information gained from those specific observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Bayesian Linear Regression with Plotly\n",
        "\n",
        "We will now set up the interactive controls using ipywidgets and link them to a function that performs the Bayesian update and generates the Plotly plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plotting utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Function to create interactive widgets ---\n",
        "def create_regression_widgets(X_all, Y_all, N_total):\n",
        "    \"\"\"Creates and returns ipywidgets for Bayesian Linear Regression parameters and data selection.\"\"\"\n",
        "\n",
        "    # Sliders for the prior mean (2D)\n",
        "    mu0_prior_slider = widgets.FloatSlider(\n",
        "        min=-5.0,\n",
        "        max=5.0,\n",
        "        value=0.0,\n",
        "        step=0.1,\n",
        "        description=\"Prior Mu_0:\",\n",
        "        style={\"description_width\": \"initial\"},\n",
        "    )\n",
        "    mu1_prior_slider = widgets.FloatSlider(\n",
        "        min=-5.0,\n",
        "        max=5.0,\n",
        "        value=0.0,\n",
        "        step=0.1,\n",
        "        description=\"Prior Mu_1:\",\n",
        "        style={\"description_width\": \"initial\"},\n",
        "    )\n",
        "\n",
        "    # Sliders for the prior covariance matrix parameters\n",
        "    s11_prior_slider = widgets.FloatSlider(\n",
        "        min=0.1,\n",
        "        max=5.0,\n",
        "        value=1.0,\n",
        "        step=0.1,\n",
        "        description=\"Prior Sigma_11:\",\n",
        "        style={\"description_width\": \"initial\"},\n",
        "    )\n",
        "    s22_prior_slider = widgets.FloatSlider(\n",
        "        min=0.1,\n",
        "        max=5.0,\n",
        "        value=1.0,\n",
        "        step=0.1,\n",
        "        description=\"Prior Sigma_22:\",\n",
        "        style={\"description_width\": \"initial\"},\n",
        "    )\n",
        "    rho_prior_slider = widgets.FloatSlider(\n",
        "        min=-0.99,\n",
        "        max=0.99,\n",
        "        value=0.0,\n",
        "        step=0.01,\n",
        "        description=\"Prior rho:\",\n",
        "        style={\"description_width\": \"initial\"},\n",
        "    )\n",
        "\n",
        "    # Widget to select data points\n",
        "    data_selector_options = (\n",
        "        [\n",
        "            (f\"Point {i + 1} (x={X_all[i, 0]:.2f}, y={Y_all[i]:.2f})\", i)\n",
        "            for i in range(N_total)\n",
        "        ]\n",
        "        if X_all is not None\n",
        "        else []\n",
        "    )\n",
        "    data_selector = widgets.SelectMultiple(\n",
        "        options=data_selector_options,\n",
        "        description=\"Select Data Points:\",\n",
        "        disabled=(X_all is None),\n",
        "        layout={\"width\": \"500px\"},\n",
        "        style={\"description_width\": \"initial\"},\n",
        "    )\n",
        "\n",
        "    # Return a dictionary of widgets\n",
        "    return {\n",
        "        \"mu0_prior\": mu0_prior_slider,\n",
        "        \"mu1_prior\": mu1_prior_slider,\n",
        "        \"s11_prior\": s11_prior_slider,\n",
        "        \"s22_prior\": s22_prior_slider,\n",
        "        \"rho_prior\": rho_prior_slider,\n",
        "        \"selected_indices\": data_selector,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _initialize_regression_figure():\n",
        "    \"\"\"Initializes a Plotly figure with two subplots for parameter and function space.\"\"\"\n",
        "    # Create two separate figures: one for parameter space, one for function space\n",
        "    fig_param = go.Figure(\n",
        "        layout=go.Layout(\n",
        "            title=\"Bayesian Linear Regression: Parameter Space\",\n",
        "            xaxis=dict(\n",
        "                title='<span class=\"math-inline\">w0</span> (Intercept)',\n",
        "                range=[-3, 3],\n",
        "                scaleanchor=\"y\",\n",
        "                scaleratio=1,\n",
        "            ),\n",
        "            yaxis=dict(\n",
        "                title='<span class=\"math-inline\">w1</span> (Slope)',\n",
        "                range=[-3, 3],\n",
        "            ),\n",
        "            showlegend=True,\n",
        "            width=600,\n",
        "            height=500,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig_func = go.Figure(\n",
        "        layout=go.Layout(\n",
        "            title=\"Bayesian Linear Regression: Function Space\",\n",
        "            xaxis=dict(\n",
        "                title='<span class=\"math-inline\">x</span>',\n",
        "                range=[-5, 5],\n",
        "            ),\n",
        "            yaxis=dict(\n",
        "                title='<span class=\"math-inline\">y</span>',\n",
        "                range=[-10, 10],\n",
        "            ),\n",
        "            showlegend=True,\n",
        "            width=600,\n",
        "            height=500,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return fig_param, fig_func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _add_parameter_space_traces(fig, data, PLOTLY_COLORS):\n",
        "    \"\"\"Adds traces for the parameter space plot to the figure.\"\"\"\n",
        "    # Plot prior contours\n",
        "    for i, pts in enumerate(data[\"prior_contour_pts\"]):\n",
        "        fig.add_trace(\n",
        "            go.Scattergl(\n",
        "                x=np.array(pts[:, 0]),\n",
        "                y=np.array(pts[:, 1]),\n",
        "                mode=\"lines\",\n",
        "                line=dict(\n",
        "                    color=PLOTLY_COLORS[\"gray\"], dash=\"dash\", width=2.5 / (i + 1)\n",
        "                ),\n",
        "                name=f\"Prior {i + 1}σ Contour\",\n",
        "                showlegend=True,  # Show legend only once\n",
        "                xaxis=\"x1\",\n",
        "                yaxis=\"y1\",\n",
        "            )\n",
        "        )\n",
        "    # Plot prior mean\n",
        "    fig.add_trace(\n",
        "        go.Scattergl(\n",
        "            x=[data[\"prior_dist\"].mu[0]],\n",
        "            y=[data[\"prior_dist\"].mu[1]],\n",
        "            mode=\"markers\",\n",
        "            marker=dict(color=PLOTLY_COLORS[\"gray\"], size=8),\n",
        "            name=\"Prior Mean\",\n",
        "            showlegend=True,\n",
        "            xaxis=\"x1\",\n",
        "            yaxis=\"y1\",\n",
        "        )\n",
        "    )\n",
        "    # Plot prior samples\n",
        "    if data[\"prior_samples\"] is not None:\n",
        "        fig.add_trace(\n",
        "            go.Scattergl(\n",
        "                x=np.array(data[\"prior_samples\"][:, 0]),\n",
        "                y=np.array(data[\"prior_samples\"][:, 1]),\n",
        "                mode=\"markers\",\n",
        "                name=\"Prior Samples\",\n",
        "                marker=dict(size=4, opacity=0.8, color=PLOTLY_COLORS[\"dark\"]),\n",
        "                showlegend=True,\n",
        "                xaxis=\"x1\",\n",
        "                yaxis=\"y1\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Plot Likelihood (MLE point) if data is selected\n",
        "    if data[\"w_mle\"] is not None:\n",
        "        fig.add_trace(\n",
        "            go.Scattergl(\n",
        "                x=[data[\"w_mle\"][0]],\n",
        "                y=[data[\"w_mle\"][1]],\n",
        "                mode=\"markers\",\n",
        "                marker=dict(color=PLOTLY_COLORS[\"blue\"], size=8),\n",
        "                name=\"Likelihood (MLE)\",\n",
        "                showlegend=True,\n",
        "                xaxis=\"x1\",\n",
        "                yaxis=\"y1\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Plot posterior contours\n",
        "    for i, pts in enumerate(data[\"posterior_contour_pts\"]):\n",
        "        fig.add_trace(\n",
        "            go.Scattergl(\n",
        "                x=np.array(pts[:, 0]),\n",
        "                y=np.array(pts[:, 1]),\n",
        "                mode=\"lines\",\n",
        "                line=dict(color=PLOTLY_COLORS[\"red\"], dash=\"dash\", width=2.5 / (i + 1)),\n",
        "                name=f\"Posterior {i + 1}σ Contour\",\n",
        "                showlegend=True,  # Show legend only once\n",
        "                xaxis=\"x1\",\n",
        "                yaxis=\"y1\",\n",
        "            )\n",
        "        )\n",
        "    # Plot posterior mean\n",
        "    fig.add_trace(\n",
        "        go.Scattergl(\n",
        "            x=[data[\"posterior_dist\"].mu[0]],\n",
        "            y=[data[\"posterior_dist\"].mu[1]],\n",
        "            mode=\"markers\",\n",
        "            marker=dict(color=PLOTLY_COLORS[\"red\"], size=8),\n",
        "            name=\"Posterior Mean\",\n",
        "            showlegend=True,\n",
        "            xaxis=\"x1\",\n",
        "            yaxis=\"y1\",\n",
        "        )\n",
        "    )\n",
        "    # Plot posterior samples\n",
        "    if data[\"posterior_samples\"] is not None:\n",
        "        fig.add_trace(\n",
        "            go.Scattergl(\n",
        "                x=np.array(data[\"posterior_samples\"][:, 0]),\n",
        "                y=np.array(data[\"posterior_samples\"][:, 1]),\n",
        "                mode=\"markers\",\n",
        "                name=\"Posterior Samples\",\n",
        "                marker=dict(size=4, opacity=0.8, color=PLOTLY_COLORS[\"red\"]),\n",
        "                showlegend=True,\n",
        "                xaxis=\"x1\",\n",
        "                yaxis=\"y1\",\n",
        "            )\n",
        "        )\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _add_function_space_traces(fig, data, PLOTLY_COLORS):\n",
        "    \"\"\"Adds traces for the function space plot to the figure.\"\"\"\n",
        "    # Plot all data points\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=np.array(data[\"all_data_x\"]),\n",
        "            y=np.array(data[\"all_data_y\"]),\n",
        "            mode=\"markers\",\n",
        "            name=\"All Data\",\n",
        "            marker=dict(color=PLOTLY_COLORS[\"dark\"], size=5),\n",
        "            error_y=dict(\n",
        "                type=\"data\",\n",
        "                array=np.array(data[\"sigma_noise\"] * jnp.ones_like(data[\"all_data_y\"])),\n",
        "            ),\n",
        "            showlegend=True,\n",
        "            xaxis=\"x2\",\n",
        "            yaxis=\"y2\",\n",
        "        )\n",
        "    )\n",
        "    # Highlight selected data points\n",
        "    if data[\"selected_data_x\"].shape[0] > 0:  # Check if there are selected points\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=np.array(data[\"selected_data_x\"]),\n",
        "                y=np.array(data[\"selected_data_y\"]),\n",
        "                mode=\"markers\",\n",
        "                name=\"Selected Data\",\n",
        "                marker=dict(\n",
        "                    color=PLOTLY_COLORS[\"red\"],\n",
        "                    size=7,\n",
        "                    line=dict(width=1, color=\"DarkRed\"),\n",
        "                ),\n",
        "                error_y=dict(\n",
        "                    type=\"data\",\n",
        "                    array=np.array(\n",
        "                        data[\"sigma_noise\"] * jnp.ones_like(data[\"selected_data_y\"])\n",
        "                    ),\n",
        "                ),\n",
        "                showlegend=True,\n",
        "                xaxis=\"x2\",\n",
        "                yaxis=\"y2\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Plot prior mean function\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=np.array(data[\"x_plot\"][:, 0]),\n",
        "            y=np.array(data[\"prior_mean_f\"]),\n",
        "            mode=\"lines\",\n",
        "            name=\"Prior Mean\",\n",
        "            line=dict(color=PLOTLY_COLORS[\"dark\"], width=2),\n",
        "            showlegend=True,\n",
        "            xaxis=\"x2\",\n",
        "            yaxis=\"y2\",\n",
        "        )\n",
        "    )\n",
        "    # Plot prior uncertainty band (+/- 2 std dev)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=np.array(data[\"x_plot\"][:, 0]),\n",
        "            y=np.array(data[\"prior_upper_f\"]),\n",
        "            mode=\"lines\",\n",
        "            line=dict(width=0),\n",
        "            name=\"Prior +2σ\",\n",
        "            showlegend=False,\n",
        "            xaxis=\"x2\",\n",
        "            yaxis=\"y2\",\n",
        "        )\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=np.array(data[\"x_plot\"][:, 0]),\n",
        "            y=np.array(data[\"prior_lower_f\"]),\n",
        "            mode=\"lines\",\n",
        "            line=dict(width=0),\n",
        "            name=\"Prior 2σ Uncertainty Band\",\n",
        "            showlegend=True,\n",
        "            xaxis=\"x2\",\n",
        "            yaxis=\"y2\",\n",
        "            fill=\"tonexty\",\n",
        "            fillcolor=PLOTLY_COLORS[\"dark_alpha\"],\n",
        "        )\n",
        "    )  # Fill between upper and lower\n",
        "\n",
        "    # Plot prior function samples\n",
        "    if data[\"prior_func_samples\"] is not None:\n",
        "        for i in range(data[\"prior_func_samples\"].shape[1]):\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=np.array(data[\"x_plot\"][:, 0]),\n",
        "                    y=np.array(data[\"prior_func_samples\"][:, i]),\n",
        "                    mode=\"lines\",\n",
        "                    line=dict(color=PLOTLY_COLORS[\"dark\"], width=1),\n",
        "                    opacity=0.3,\n",
        "                    name=\"Prior Sample Functions\",\n",
        "                    showlegend=False,  # Don't show legend for each sample\n",
        "                    xaxis=\"x2\",\n",
        "                    yaxis=\"y2\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # Plot posterior mean function\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=np.array(data[\"x_plot\"][:, 0]),\n",
        "            y=np.array(data[\"posterior_mean_f\"]),\n",
        "            mode=\"lines\",\n",
        "            name=\"Posterior Mean\",\n",
        "            line=dict(color=PLOTLY_COLORS[\"red\"], width=2),\n",
        "            showlegend=True,\n",
        "            xaxis=\"x2\",\n",
        "            yaxis=\"y2\",\n",
        "        )\n",
        "    )\n",
        "    # Plot posterior uncertainty band (+/- 2 std dev)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=np.array(data[\"x_plot\"][:, 0]),\n",
        "            y=np.array(data[\"posterior_upper_f\"]),\n",
        "            mode=\"lines\",\n",
        "            line=dict(width=0),\n",
        "            name=\"Posterior +2σ\",\n",
        "            showlegend=False,\n",
        "            xaxis=\"x2\",\n",
        "            yaxis=\"y2\",\n",
        "        )\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=np.array(data[\"x_plot\"][:, 0]),\n",
        "            y=np.array(data[\"posterior_lower_f\"]),\n",
        "            mode=\"lines\",\n",
        "            line=dict(width=0),\n",
        "            name=\"Posterior 2σ Uncertainty Band\",\n",
        "            showlegend=True,\n",
        "            xaxis=\"x2\",\n",
        "            yaxis=\"y2\",\n",
        "            fill=\"tonexty\",\n",
        "            fillcolor=PLOTLY_COLORS[\"red_alpha\"],\n",
        "        )\n",
        "    )  # Fill between upper and lower\n",
        "\n",
        "    # Plot posterior function samples\n",
        "    if data[\"posterior_func_samples\"] is not None:\n",
        "        for i in range(data[\"posterior_func_samples\"].shape[1]):\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=np.array(data[\"x_plot\"][:, 0]),\n",
        "                    y=np.array(data[\"posterior_func_samples\"][:, i]),\n",
        "                    mode=\"lines\",\n",
        "                    line=dict(color=PLOTLY_COLORS[\"red\"], width=1),\n",
        "                    opacity=0.5,\n",
        "                    name=\"Posterior Sample Functions\",\n",
        "                    showlegend=False,  # Don't show legend for each sample\n",
        "                    xaxis=\"x2\",\n",
        "                    yaxis=\"y2\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # Plot Likelihood (MLE Function) if calculated\n",
        "    if data[\"mle_func\"] is not None:\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=np.array(data[\"x_plot\"][:, 0]),\n",
        "                y=np.array(data[\"mle_func\"]),\n",
        "                mode=\"lines\",\n",
        "                line=dict(color=PLOTLY_COLORS[\"blue\"], dash=\"dash\", width=2),\n",
        "                name=\"Likelihood (MLE Function)\",\n",
        "                showlegend=True,\n",
        "                xaxis=\"x2\",\n",
        "                yaxis=\"y2\",\n",
        "            )\n",
        "        )\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _finalize_and_show_figure(fig_param, fig_func):\n",
        "    \"\"\"Applies final layout adjustments and displays the figure.\"\"\"\n",
        "    fig_param.update_layout(\n",
        "        legend=dict(orientation=\"h\", x=0.5, y=-0.15, xanchor=\"center\", yanchor=\"top\"),\n",
        "        title_x=0.5,  # Center the main title\n",
        "        title_y=0.95,  # Position the main title slightly lower\n",
        "        title=\"Bayesian Linear Regression: Parameter Space\",  # Set the main title here\n",
        "        width=1200,\n",
        "        height=600,\n",
        "    )\n",
        "\n",
        "    fig_param.show()\n",
        "\n",
        "    fig_func.update_layout(\n",
        "        legend=dict(orientation=\"h\", x=0.5, y=-0.15, xanchor=\"center\", yanchor=\"top\"),\n",
        "        title_x=0.5,  # Center the main title\n",
        "        title_y=0.95,  # Position the main title slightly lower\n",
        "        title=\"Bayesian Linear Regression: Function Space\",  # Set the main title here\n",
        "        width=1200,\n",
        "        height=600,\n",
        "    )\n",
        "\n",
        "    fig_func.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Function to set up and display the interactive interface ---\n",
        "def setup_interactive_regression(X_all, Y_all, sigma_noise, N_total, update_plot_fn):\n",
        "    \"\"\"\n",
        "    Creates widgets, links them to the update_plot_fn, and displays the interactive interface.\n",
        "    \"\"\"\n",
        "    if X_all is None or Y_all is None or sigma_noise is None:\n",
        "        print(\"Data not loaded. Cannot setup interactive interface.\")\n",
        "        return\n",
        "\n",
        "    # Create the widgets\n",
        "    regression_widgets = create_regression_widgets(X_all, Y_all, N_total)\n",
        "\n",
        "    # Link widgets to the update function\n",
        "    interactive_plot_output = widgets.interactive_output(\n",
        "        update_plot_fn,\n",
        "        regression_widgets,  # Pass the dictionary of widgets\n",
        "    )\n",
        "\n",
        "    # Arrange widgets (Assuming a horizontal box layout for controls)\n",
        "    prior_controls_box = widgets.VBox(\n",
        "        [\n",
        "            widgets.Label(\"Prior Parameters:\"),\n",
        "            regression_widgets[\"mu0_prior\"],\n",
        "            regression_widgets[\"mu1_prior\"],\n",
        "            regression_widgets[\"s11_prior\"],\n",
        "            regression_widgets[\"s22_prior\"],\n",
        "            regression_widgets[\"rho_prior\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    data_selection_control_box = widgets.VBox(\n",
        "        [\n",
        "            widgets.Label(\"Data Selection:\"),\n",
        "            regression_widgets[\"selected_indices\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    controls_box = widgets.HBox([prior_controls_box, data_selection_control_box])\n",
        "\n",
        "    # Display the controls and the plot output\n",
        "    print(\n",
        "        \"Adjust the sliders and select data points to explore Bayesian Linear Regression:\"\n",
        "    )\n",
        "    display(controls_box, interactive_plot_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Main Update Function (Orchestrator) ---\n",
        "def update_regression_plot_plotly(\n",
        "    mu0_prior, mu1_prior, s11_prior, s22_prior, rho_prior, selected_indices\n",
        "):\n",
        "    \"\"\"\n",
        "    Updates the Plotly plots for Bayesian Linear Regression based on the selected parameters and data.\n",
        "    This function orchestrates calls to smaller helper functions.\n",
        "    \"\"\"\n",
        "    # Assume X_all, Y_all, sigma_noise are available globally or passed in higher scope\n",
        "    if X_all is None or Y_all is None or sigma_noise is None:\n",
        "        # Display an empty plot or message in the output area if data is not loaded\n",
        "        fig = go.Figure()\n",
        "        fig.update_layout(title=\"Data not loaded.\")\n",
        "        fig.show()\n",
        "        return\n",
        "\n",
        "    # 1. Data Handling & Prior Setup\n",
        "    prior_dist = create_prior_distribution(\n",
        "        mu0_prior, mu1_prior, s11_prior, s22_prior, rho_prior\n",
        "    )\n",
        "    X_select, Y_select, Lambda_select_sq = _select_data(\n",
        "        X_all, Y_all, sigma_noise, selected_indices\n",
        "    )\n",
        "\n",
        "    # 2. Posterior Calculation\n",
        "    posterior_dist = compute_posterior(prior_dist, X_select, Y_select, Lambda_select_sq)\n",
        "\n",
        "    # 3. Plotting Data Preparation\n",
        "    # Regenerate key for samples each update\n",
        "    global initial_key  # Assuming initial_key is a global JAX PRNG key\n",
        "    initial_key, subkey = jrandom.split(initial_key)\n",
        "\n",
        "    param_space_data = generate_parameter_space_data(\n",
        "        prior_dist, posterior_dist, X_select, Y_select, subkey\n",
        "    )\n",
        "    # Update key based on usage in param_space_data generation\n",
        "    subkey = param_space_data[\"key\"]\n",
        "\n",
        "    func_space_data = generate_function_space_data(\n",
        "        prior_dist,\n",
        "        posterior_dist,\n",
        "        X_all,\n",
        "        Y_all,\n",
        "        sigma_noise,\n",
        "        X_select,\n",
        "        Y_select,\n",
        "        param_space_data[\"w_mle\"],\n",
        "        subkey,\n",
        "    )\n",
        "    # Update key based on usage in func_space_data generation\n",
        "    initial_key = func_space_data[\"key\"]  # Update the global key for the next call\n",
        "\n",
        "    # Add prior and posterior distributions to the data dictionaries for easier access in plotting\n",
        "    param_space_data[\"prior_dist\"] = prior_dist\n",
        "    param_space_data[\"posterior_dist\"] = posterior_dist\n",
        "\n",
        "    # 4. Plotly Figure Creation & Population\n",
        "    fig_param, fig_function = _initialize_regression_figure()\n",
        "    fig_param = _add_parameter_space_traces(fig_param, param_space_data, PLOTLY_COLORS)\n",
        "    fig_function = _add_function_space_traces(\n",
        "        fig_function, func_space_data, PLOTLY_COLORS\n",
        "    )\n",
        "\n",
        "    # 5. Finalize and Show\n",
        "    _finalize_and_show_figure(fig_param, fig_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interactive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjust the sliders and select data points to explore Bayesian Linear Regression:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a3c172def934b6aab459b4b9552ca16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(VBox(children=(Label(value='Prior Parameters:'), FloatSlider(value=0.0, description='Prior Mu_0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "593af7e52d4d4f4b9756e1ec5c748094",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Run the setup function ---\n",
        "if X_all is not None:  # Check if data loaded before setting up\n",
        "    setup_interactive_regression(\n",
        "        X_all, Y_all, sigma_noise, N_total, update_regression_plot_plotly\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation of Bayesian Linear Regression\n",
        "\n",
        "At its core, Bayesian linear regression is about finding a probability distribution over the possible straight lines (or hyperplanes in higher dimensions) that could have generated the data. Instead of finding a single \"best\" line, we maintain a belief about what the parameters $w_0$ (intercept) and $w_1$ (slope) could be, represented by a joint probability distribution $p(w_0, w_1)$.\n",
        "\n",
        "1. The Model: We assume the data is generated by a linear function corrupted by Gaussian noise: $y = w_0 + w_1 x + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n",
        "2. The Prior: We start with a prior belief about the parameters $w$. A common and mathematically convenient choice is a Gaussian prior $p(w) = \\mathcal{N}(w; \\mu_{prior}, \\Sigma_{prior})$. This distribution reflects our initial uncertainty about the intercept and slope before seeing any data.\n",
        "    * In the parameter space plot (left), the contours of the prior Gaussian show regions of higher probability for the $[w_0, w_1]$ pair. The center of the ellipse is the prior mean $\\mu_{prior}$. The shape and orientation of the ellipse are determined by the prior covariance $\\Sigma_{prior}$.\n",
        "    * In the function space plot (right), the prior mean function $f(x) = \\phi(x)^T \\mu_{prior}$ is shown as a line. The uncertainty band around it shows the range of function values (e.g., $\\pm 2$ standard deviations) predicted by the prior distribution over parameters. Each sample from the prior in parameter space corresponds to a specific line in function space, illustrating the variety of functions considered plausible under the prior.\n",
        "3. The Likelihood: The likelihood $p(y | x, w)$ tells us the probability of observing a data point $(x, y)$ given specific parameter values $w$. Due to the Gaussian noise assumption, this likelihood is Gaussian: the observed $y$ is likely to be close to $\\phi(x)^T w$. For multiple independent data points, the joint likelihood $p(Y_{select} | X_{select}, w)$ is also Gaussian.\n",
        "    * In the parameter space plot, the \"Likelihood (MLE)\" point represents the parameter values that maximize the likelihood for the selected data – effectively, the line that best fits only the selected data according to the least squares criterion. (Note: The likelihood itself is a function of $w$ for fixed data, not a distribution over $w$ that you can sample from directly).\n",
        "    * In the function space plot, the \"Likelihood (MLE Function)\" shows the line corresponding to the MLE parameters. The selected data points are also highlighted.\n",
        "4. The Posterior: When we observe data, we update our prior belief to get the posterior distribution $p(w | X_{select}, Y_{select})$. Thanks to the conjugate property of Gaussian priors with Gaussian likelihoods, the posterior is also Gaussian, but with updated mean $\\mu_{posterior}$ and covariance $\\Sigma_{posterior}$.\n",
        "    * The posterior mean $\\mu_{posterior}$ is a weighted average of the prior mean and the information from the data (specifically, the MLE). As you add more data, especially informative data, the posterior mean will move towards the MLE.\n",
        "    * The posterior covariance $\\Sigma_{posterior}$ is smaller than the prior covariance, reflecting a reduction in uncertainty about the parameters after observing data. As you add more data, the posterior ellipse in parameter space will shrink.\n",
        "    * In the parameter space plot (left subplot), the posterior contours and samples show the updated belief about $w$.\n",
        "    * In the function space plot (right subplot), the posterior mean function shows the line that best fits the selected data, considering the prior. The posterior uncertainty band is typically narrower than the prior band, reflecting increased certainty about the function after seeing data. Samples from the posterior in function space show lines that are plausible given the data and the prior.\n",
        "\n",
        "By selecting data points using the interactive widget, you can observe how the posterior distribution shifts and shrinks, and how this translates into a more certain belief about the linear relationship between $x$ and $y$ in the function space. The posterior mean function becomes a better fit to the selected data, and the uncertainty band narrows, particularly in regions where data has been observed.\n",
        "\n",
        "This interactive notebook allows you to visualize this core process of Bayesian learning: starting with a belief (prior), observing evidence (data), and updating that belief (posterior)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
