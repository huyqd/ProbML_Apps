{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a4c70a",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning: Lecture 20 - Gauss-Markov Models\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Welcome to Lecture 20 of Probabilistic Machine Learning! This lecture introduces **Gauss-Markov Models**, a fundamental class of probabilistic models used for time series analysis. We will explore how conditional independence assumptions lead to computationally efficient inference algorithms like the Kalman Filter and the Rauch-Tung-Striebel Smoother. These algorithms are crucial for processing data that arrives as a stream, enabling real-time predictions and retrospective analysis.\n",
    "\n",
    "This notebook will guide you through the theoretical concepts and provide practical implementations using **JAX** for numerical computations and **Plotly** for interactive visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4e8e9",
   "metadata": {},
   "source": [
    "#### 1. Goal for this Week: Time Series as a Problem Class\n",
    "\n",
    "The overarching goal for this week is to understand **Time Series as a problem class** (Slide 2). We'll break this down into several layers:\n",
    "\n",
    "* **Conceptual Layer**: Conditional Independence affects computational complexity of inference.\n",
    "* **Application Layer**: Data arriving as a stream.\n",
    "* **Model Structure Layer**: Markov Chains / Hidden Markov Models.\n",
    "* **Concrete Model Layer**: Gauss-Markov Models.\n",
    "* **Algorithm Layer**: Kalman Filter & RTS Smoother.\n",
    "\n",
    "We'll also briefly touch upon generalizations to non-Gaussian models (with approximations) and the theoretical layer of Stochastic Differential Equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30a53f",
   "metadata": {},
   "source": [
    "#### 2. Conditional Independence and Computational Complexity of Inference\n",
    "\n",
    "As we've seen throughout this course (and recapped in Lecture 2, Slides 3-4), conditional independence plays a crucial role in determining the computational complexity of inference. For complex probabilistic models, exploiting independence structures can drastically reduce the computational burden.\n",
    "\n",
    "* **Parametric Models (Graphical View)**: In parametric models, data points are conditionally independent given the model weights. This leads to inference complexity often scaling as $\\mathcal{O}(NF^2 + F^3)$ for $F$ parameters and $N$ data points (Slide 5).\n",
    "* **Nonparametric Models (Graphical View)**: In contrast, nonparametric models like Gaussian Processes typically have no finite sufficient statistic, meaning all data points directly interact. This results in inference complexity scaling as $\\mathcal{O}(N^3)$ (Slide 6), which becomes prohibitive for large $N$.\n",
    "\n",
    "This distinction highlights the challenge when dealing with time series, where $N$ can grow indefinitely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c7fba",
   "metadata": {},
   "source": [
    "#### 3. Time Series: $\\mathcal{O}(N)$ Inference is Indispensable\n",
    "\n",
    "When data arrives as a stream, we require inference that scales linearly with the number of observations, i.e., $\\mathcal{O}(N)$ (Slide 7). This is the defining characteristic of a **time series** (Slide 8).\n",
    "\n",
    "**Definition**: A time series is a sequence $[y(t_i)]_{i \\in \\mathbb{N}}$ of observations $y_i := x(t_i) \\in \\mathbb{Y}$, indexed by a scalar variable $t \\in \\mathbb{R}$. In many applications, the time points $t_j$ are equally spaced: $t_i = t_0 + i \\cdot \\delta_t$.\n",
    "\n",
    "Examples of time series are ubiquitous:\n",
    "* Climate & weather observations\n",
    "* Sensor readings in cars\n",
    "* EEG, ECG, patch clamp signals\n",
    "* Stock prices, supply & demand data\n",
    "\n",
    "Inference in time series often needs to happen in real-time and scale to an unbounded set of data, typically on small-scale or embedded systems. This necessitates (low) constant time and memory complexity per time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb3348",
   "metadata": {},
   "source": [
    "#### 4. Markov Chains: Processes with a \"Local Memory\"\n",
    "\n",
    "To achieve $\\mathcal{O}(N)$ inference, we need a model with \"local memory\" that is \"passed forward\" through time. This concept is formalized by **Markov Chains**, and associated models are called **State-Space Models** (Slide 9).\n",
    "\n",
    "**Definition**: A joint distribution $p(X)$ over a sequence of random variables $X := [x_0, \\dots, x_N]$ is said to have the **Markov property** if:\n",
    "$$p(x_i | x_0, x_1, \\dots, x_{i-1}) = p(x_i | x_{i-1})$$\n",
    "\n",
    "The sequence is then called a Markov chain. This means the future state $x_i$ only depends on the immediate past state $x_{i-1}$, not on the entire history. This conditional independence structure is key to efficient inference.\n",
    "\n",
    "In state-space models, we typically assume:\n",
    "* **State transition**: $p(x_t | X_{0:t-1}) = p(x_t | x_{t-1})$ (the Markov property for the latent states).\n",
    "* **Observation likelihood**: $p(y_t | X) = p(y_t | x_t)$ (observations depend only on the current latent state).\n",
    "\n",
    "This structure allows for inference to be separated into three operations (Slides 15-22):\n",
    "\n",
    "1.  **Predict (Chapman-Kolmogorov Equation)**: Propagating the belief about the state forward in time.\n",
    "    $$p(x_t | Y_{0:t-1}) = \\int p(x_t | x_{t-1}) p(x_{t-1} | Y_{0:t-1}) dx_{t-1}$$\n",
    "\n",
    "2.  **Update (Bayes' Theorem)**: Incorporating a new observation $y_t$ to refine the belief about the current state.\n",
    "    $$p(x_t | Y_{0:t}) = \\frac{p(y_t | x_t) p(x_t | Y_{0:t-1})}{p(y_t)}$$\n",
    "\n",
    "3.  **Smooth (Backward Pass)**: Refining past state estimates by incorporating future observations.\n",
    "    $$p(x_t | Y) = p(x_t | Y_{0:t}) \\int \\frac{p(x_{t+1} | x_t) p(x_{t+1} | Y)}{p(x_{t+1} | Y_{0:t})} dx_{t+1}$$\n",
    "\n",
    "Both filtering (predict + update) and smoothing can be performed in $\\mathcal{O}(T)$ time, where $T$ is the number of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c866c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set JAX to use 64-bit floats for numerical stability\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def generate_linear_gaussian_data(\n",
    "    T=100,\n",
    "    state_dim=1,\n",
    "    obs_dim=1,\n",
    "    A=None,\n",
    "    Q_std=0.1,\n",
    "    H=None,\n",
    "    R_std=0.1,\n",
    "    m0=None,\n",
    "    P0_std=1.0,\n",
    "    key=None,\n",
    "):\n",
    "    \"\"\"Generates synthetic data from a linear Gaussian state-space model.\"\"\"\n",
    "    if key is None:\n",
    "        key = jax.random.PRNGKey(0)\n",
    "\n",
    "    if A is None:\n",
    "        A = jnp.eye(state_dim) * 0.9  # State transition matrix\n",
    "    if H is None:\n",
    "        H = jnp.eye(obs_dim, state_dim)  # Observation matrix\n",
    "\n",
    "    Q = jnp.eye(state_dim) * Q_std**2  # State noise covariance\n",
    "    R = jnp.eye(obs_dim) * R_std**2  # Observation noise covariance\n",
    "\n",
    "    if m0 is None:\n",
    "        m0 = jnp.zeros(state_dim)  # Initial state mean\n",
    "    P0 = jnp.eye(state_dim) * P0_std**2  # Initial state covariance\n",
    "\n",
    "    states = [m0]\n",
    "    observations = []\n",
    "\n",
    "    for t in range(T):\n",
    "        key, subkey_state, subkey_obs = jax.random.split(key, 3)\n",
    "        # State transition: x_t = A @ x_{t-1} + w_t, w_t ~ N(0, Q)\n",
    "        state_noise = jax.random.multivariate_normal(\n",
    "            subkey_state, jnp.zeros(state_dim), Q\n",
    "        )\n",
    "        next_state = A @ states[-1] + state_noise\n",
    "        states.append(next_state)\n",
    "\n",
    "        # Observation: y_t = H @ x_t + v_t, v_t ~ N(0, R)\n",
    "        obs_noise = jax.random.multivariate_normal(subkey_obs, jnp.zeros(obs_dim), R)\n",
    "        observation = H @ next_state + obs_noise\n",
    "        observations.append(observation)\n",
    "\n",
    "    return jnp.array(states[1:]), jnp.array(observations), A, Q, H, R, m0, P0\n",
    "\n",
    "\n",
    "def plot_kalman_results(\n",
    "    true_states,\n",
    "    observations,\n",
    "    filtered_means,\n",
    "    filtered_stds,\n",
    "    smoothed_means=None,\n",
    "    smoothed_stds=None,\n",
    "    title=\"\",\n",
    "    state_idx=0,\n",
    "):\n",
    "    \"\"\"Plots true states, observations, and Kalman filter/smoother results for a single dimension.\"\"\"\n",
    "    fig = go.Figure()\n",
    "    time_steps = jnp.arange(len(true_states))\n",
    "\n",
    "    # True States\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=time_steps,\n",
    "            y=true_states[:, state_idx],\n",
    "            mode=\"lines\",\n",
    "            name=\"True State\",\n",
    "            line=dict(color=\"blue\", width=2),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Observations\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=time_steps,\n",
    "            y=observations[:, state_idx],\n",
    "            mode=\"markers\",\n",
    "            name=\"Observations\",\n",
    "            marker=dict(color=\"red\", size=4, opacity=0.6),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Filtered Mean\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=time_steps,\n",
    "            y=filtered_means[:, state_idx],\n",
    "            mode=\"lines\",\n",
    "            name=\"Filtered Mean\",\n",
    "            line=dict(color=\"green\", width=1, dash=\"dash\"),\n",
    "        )\n",
    "    )\n",
    "    # Filtered Uncertainty\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=jnp.concatenate([time_steps, time_steps[::-1]]),\n",
    "            y=jnp.concatenate(\n",
    "                [\n",
    "                    filtered_means[:, state_idx] + 2 * filtered_stds[:, state_idx],\n",
    "                    (filtered_means[:, state_idx] - 2 * filtered_stds[:, state_idx])[\n",
    "                        ::-1\n",
    "                    ],\n",
    "                ]\n",
    "            ),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(0,255,0,0.1)\",\n",
    "            line_color=\"rgba(255,255,255,0)\",\n",
    "            name=\"Filtered 2 Std Dev\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Smoothed Mean (if provided)\n",
    "    if smoothed_means is not None:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=time_steps,\n",
    "                y=smoothed_means[:, state_idx],\n",
    "                mode=\"lines\",\n",
    "                name=\"Smoothed Mean\",\n",
    "                line=dict(color=\"purple\", width=1),\n",
    "            )\n",
    "        )\n",
    "        # Smoothed Uncertainty\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=jnp.concatenate([time_steps, time_steps[::-1]]),\n",
    "                y=jnp.concatenate(\n",
    "                    [\n",
    "                        smoothed_means[:, state_idx] + 2 * smoothed_stds[:, state_idx],\n",
    "                        (\n",
    "                            smoothed_means[:, state_idx]\n",
    "                            - 2 * smoothed_stds[:, state_idx]\n",
    "                        )[::-1],\n",
    "                    ]\n",
    "                ),\n",
    "                fill=\"toself\",\n",
    "                fillcolor=\"rgba(128,0,128,0.1)\",\n",
    "                line_color=\"rgba(255,255,255,0)\",\n",
    "                name=\"Smoothed 2 Std Dev\",\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"Time Step\",\n",
    "        yaxis_title=\"State Value\",\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81a794",
   "metadata": {},
   "source": [
    "#### 5. Gauss-Markov Models: Linear Gaussian Case\n",
    "\n",
    "If all relationships are **linear and Gaussian**, then inference (filtering and smoothing) becomes analytic and involves only linear algebra. This is the realm of **Gauss-Markov Models** (Slide 23).\n",
    "\n",
    "The model is defined by:\n",
    "* **State transition**: $p(x_t | x_{t-1}) = \\mathcal{N}(x_t; A x_{t-1}, Q)$\n",
    "* **Observation likelihood**: $p(y_t | x_t) = \\mathcal{N}(y_t; H x_t, R)$\n",
    "* **Initial state**: $p(x_0) = \\mathcal{N}(x_0; m_0, P_0)$\n",
    "\n",
    "Here:\n",
    "* $A$: State transition matrix.\n",
    "* $Q$: State noise covariance matrix.\n",
    "* $H$: Observation matrix.\n",
    "* $R$: Observation noise covariance matrix.\n",
    "* $m_0, P_0$: Mean and covariance of the initial state.\n",
    "\n",
    "The corresponding algorithms are:\n",
    "\n",
    "### The Kalman Filter (Filtering: $\\mathcal{O}(T)$)\n",
    "\n",
    "The Kalman Filter performs the predict and update steps iteratively for each new observation. It maintains a Gaussian belief over the current state.\n",
    "\n",
    "**Prediction Step (Time Update)** (Slide 24):\n",
    "* Predictive mean: $m_t^- = A m_{t-1}$\n",
    "* Predictive covariance: $P_t^- = A P_{t-1} A^T + Q$\n",
    "\n",
    "**Update Step (Measurement Update)** (Slide 25):\n",
    "* Innovation residual: $z = y_t - H m_t^-$\n",
    "* Innovation covariance: $S = H P_t^- H^T + R$\n",
    "* Kalman gain: $K = P_t^- H^T S^{-1}$\n",
    "* Updated mean: $m_t = m_t^- + K z$\n",
    "* Updated covariance: $P_t = (I - K H) P_t^-$\n",
    "\n",
    "The overall complexity of the filtering pass through $T$ time steps is $\\mathcal{O}(T \\cdot (|X|^3 + |Y|^3))$, where $|X|$ is state dimension and $|Y|$ is observation dimension (Slide 27)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5b9a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Kalman Filter Implementation ---\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def kalman_filter(observations, A, Q, H, R, m0, P0):\n",
    "    \"\"\"\n",
    "    Implements the Kalman Filter for a linear Gaussian state-space model.\n",
    "\n",
    "    Args:\n",
    "        observations (jnp.ndarray): Sequence of observations (T, obs_dim).\n",
    "        A (jnp.ndarray): State transition matrix (state_dim, state_dim).\n",
    "        Q (jnp.ndarray): State noise covariance (state_dim, state_dim).\n",
    "        H (jnp.ndarray): Observation matrix (obs_dim, state_dim).\n",
    "        R (jnp.ndarray): Observation noise covariance (obs_dim, obs_dim).\n",
    "        m0 (jnp.ndarray): Initial state mean (state_dim,).\n",
    "        P0 (jnp.ndarray): Initial state covariance (state_dim, state_dim).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (filtered_means, filtered_covs, predicted_means, predicted_covs)\n",
    "    \"\"\"\n",
    "    T = observations.shape[0]\n",
    "    state_dim = m0.shape[0]\n",
    "\n",
    "    filtered_means = jnp.zeros((T, state_dim))\n",
    "    filtered_covs = jnp.zeros((T, state_dim, state_dim))\n",
    "    predicted_means = jnp.zeros((T, state_dim))\n",
    "    predicted_covs = jnp.zeros((T, state_dim, state_dim))\n",
    "\n",
    "    m_prev = m0\n",
    "    P_prev = P0\n",
    "\n",
    "    for t in range(T):\n",
    "        # Prediction Step (Time Update)\n",
    "        m_minus = A @ m_prev  # Predictive mean\n",
    "        P_minus = A @ P_prev @ A.T + Q  # Predictive covariance\n",
    "\n",
    "        # Store predicted values\n",
    "        predicted_means = predicted_means.at[t].set(m_minus)\n",
    "        predicted_covs = predicted_covs.at[t].set(P_minus)\n",
    "\n",
    "        # Update Step (Measurement Update)\n",
    "        z = observations[t] - H @ m_minus  # Innovation residual\n",
    "        S = H @ P_minus @ H.T + R  # Innovation covariance\n",
    "        K = P_minus @ H.T @ jnp.linalg.inv(S)  # Kalman gain\n",
    "\n",
    "        m_t = m_minus + K @ z  # Updated mean\n",
    "        P_t = (jnp.eye(state_dim) - K @ H) @ P_minus  # Updated covariance\n",
    "\n",
    "        # Store filtered values\n",
    "        filtered_means = filtered_means.at[t].set(m_t)\n",
    "        filtered_covs = filtered_covs.at[t].set(P_t)\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        m_prev = m_t\n",
    "        P_prev = P_t\n",
    "\n",
    "    return filtered_means, filtered_covs, predicted_means, predicted_covs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99bc8a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Understanding the Kalman Filter and Its Connection to State Space Models\n",
    "\n",
    "### What is a State Space Model?\n",
    "\n",
    "A **state space model** is a mathematical framework for modeling time series data where we assume there is an underlying (possibly unobserved) process, called the **state**, that evolves over time and generates the observed data. The model is defined by two equations:\n",
    "\n",
    "1. **State Transition (Dynamics):**\n",
    "    $$\n",
    "    x_t = A x_{t-1} + w_t, \\quad w_t \\sim \\mathcal{N}(0, Q)\n",
    "    $$\n",
    "    - $x_t$: The hidden (latent) state at time $t$.\n",
    "    - $A$: State transition matrix (how the state evolves).\n",
    "    - $w_t$: Process noise (randomness in the evolution), Gaussian with covariance $Q$.\n",
    "\n",
    "2. **Observation (Measurement):**\n",
    "    $$\n",
    "    y_t = H x_t + v_t, \\quad v_t \\sim \\mathcal{N}(0, R)\n",
    "    $$\n",
    "    - $y_t$: The observed data at time $t$.\n",
    "    - $H$: Observation matrix (how the state is mapped to observations).\n",
    "    - $v_t$: Observation noise, Gaussian with covariance $R$.\n",
    "\n",
    "This structure is called a **Linear Gaussian State Space Model** (or Gauss-Markov Model).\n",
    "\n",
    "---\n",
    "\n",
    "### What is the Kalman Filter?\n",
    "\n",
    "The **Kalman Filter** is an algorithm for **sequentially estimating** the hidden state $x_t$ of a linear Gaussian state space model, given a sequence of noisy observations $y_{1:t}$. It is optimal (in the mean squared error sense) when the model is linear and all noise is Gaussian.\n",
    "\n",
    "#### Key Ideas:\n",
    "- **Recursive:** The filter updates its estimate as each new observation arrives, without needing to store all past data.\n",
    "- **Probabilistic:** It maintains a Gaussian belief (mean and covariance) about the current state.\n",
    "- **Efficient:** Each update is $\\mathcal{O}(1)$ in time and memory per step (for fixed state dimension).\n",
    "\n",
    "---\n",
    "\n",
    "### How Does the Kalman Filter Work?\n",
    "\n",
    "At each time step, the Kalman filter performs two main operations:\n",
    "\n",
    "1. **Prediction (Time Update):**\n",
    "    - Use the previous state estimate to predict the current state **before** seeing the new observation.\n",
    "    - Equations:\n",
    "      $$\n",
    "      m_t^- = A m_{t-1}\n",
    "      $$\n",
    "      $$\n",
    "      P_t^- = A P_{t-1} A^T + Q\n",
    "      $$\n",
    "      Where $m_{t-1}$ and $P_{t-1}$ are the mean and covariance of the previous state estimate.\n",
    "\n",
    "2. **Update (Measurement Update):**\n",
    "    - Incorporate the new observation $y_t$ to refine the state estimate.\n",
    "    - Equations:\n",
    "      $$\n",
    "      z_t = y_t - H m_t^- \\quad \\text{(innovation)}\n",
    "      $$\n",
    "      $$\n",
    "      S_t = H P_t^- H^T + R \\quad \\text{(innovation covariance)}\n",
    "      $$\n",
    "      $$\n",
    "      K_t = P_t^- H^T S_t^{-1} \\quad \\text{(Kalman gain)}\n",
    "      $$\n",
    "      $$\n",
    "      m_t = m_t^- + K_t z_t \\quad \\text{(updated mean)}\n",
    "      $$\n",
    "      $$\n",
    "      P_t = (I - K_t H) P_t^- \\quad \\text{(updated covariance)}\n",
    "      $$\n",
    "\n",
    "This process repeats for each new observation.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is the Kalman Filter Important for Time Series Forecasting?\n",
    "\n",
    "- **Real-Time Inference:** The Kalman filter is ideal for streaming data, as it updates estimates on-the-fly.\n",
    "- **Forecasting:** The prediction step provides a forecast of the next state (and thus the next observation) before the new data arrives.\n",
    "- **Uncertainty Quantification:** The filter not only gives a point estimate but also quantifies uncertainty via the covariance.\n",
    "- **Optimality:** For linear-Gaussian models, no other filter can do better in terms of mean squared error.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuitive Example\n",
    "\n",
    "Suppose you are tracking the position of a car using noisy GPS measurements. The true position is the hidden state $x_t$, and the GPS reading is $y_t$. The Kalman filter combines your knowledge of how the car moves (the dynamics) and the noisy measurements to give the best possible estimate of the car's position at each time.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Step         | What it does                        | Formula (Mean)         | Formula (Covariance)        |\n",
    "|--------------|-------------------------------------|------------------------|-----------------------------|\n",
    "| **Predict**  | Forecast next state                 | $A m_{t-1}$            | $A P_{t-1} A^T + Q$         |\n",
    "| **Update**   | Correct with new observation        | $m_t^- + K_t z_t$      | $(I - K_t H) P_t^-$         |\n",
    "\n",
    "---\n",
    "\n",
    "### Relation to State Space Models\n",
    "\n",
    "- The Kalman filter is the **inference algorithm** for linear Gaussian state space models.\n",
    "- It exploits the **Markov property** (future depends only on present) and **Gaussianity** (all distributions remain Gaussian).\n",
    "- For **nonlinear** or **non-Gaussian** models, extensions like the Extended Kalman Filter (EKF) or Particle Filter are used.\n",
    "\n",
    "---\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Wikipedia: Kalman Filter](https://en.wikipedia.org/wiki/Kalman_filter)\n",
    "- [Probabilistic Machine Learning: An Introduction (Kevin Murphy), Chapter 18](https://probml.github.io/pml-book/book2.html#kalman-filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc11b2a",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "## Relation of the Kalman Filter (KMF) to Other State Space Models (e.g., ETS)\n",
    "\n",
    "### State Space Models: A General Framework\n",
    "\n",
    "The **Kalman Filter** is an inference algorithm for **linear Gaussian state space models** (also called Gauss-Markov models). Here, \"inference algorithm\" means a method for **estimating the hidden (latent) states** of the model, given the observed data. This is different from \"statistical inference\" in the classical sense (such as hypothesis testing or parameter estimation).\n",
    "\n",
    "- In the context of state space models, **inference** refers to the process of computing the probability distribution (or point estimates, such as the mean and covariance) of the hidden state variables at each time step, given all the observations up to that point (filtering), or all observations in the sequence (smoothing).\n",
    "- The Kalman filter provides an efficient, recursive way to perform this state estimation for linear-Gaussian models.\n",
    "- In contrast, **statistical inference** usually refers to learning the model parameters (like $A$, $Q$, $H$, $R$) from data, or testing hypotheses about them.\n",
    "\n",
    "So, in summary:  \n",
    "- **Kalman filter as an inference algorithm**: Computes the best estimate of the hidden states over time, given the model parameters and observed data.\n",
    "- **Statistical inference**: Typically refers to learning or testing about the model parameters themselves.\n",
    "\n",
    "In state space modeling, both types of inference are important, but the Kalman filter specifically addresses the problem of **state estimation** (sometimes called \"latent variable inference\" or \"filtering/smoothing\"), not parameter learning.\n",
    "\n",
    "- **ARMA/ARIMA models**\n",
    "- **Exponential Smoothing (ETS) models**\n",
    "- **Structural Time Series models** (level, trend, seasonality, regression, etc.)\n",
    "- **Dynamic Linear Models (DLMs)**\n",
    "- **Nonlinear and non-Gaussian models** (with extensions like the Extended Kalman Filter or Particle Filters)\n",
    "\n",
    "All these models can be written in the general state space form:\n",
    "- **State equation:** $x_t = f(x_{t-1}) + w_t$\n",
    "- **Observation equation:** $y_t = h(x_t) + v_t$\n",
    "where $w_t$ and $v_t$ are noise terms.\n",
    "\n",
    "---\n",
    "\n",
    "### ETS Models as State Space Models\n",
    "\n",
    "**ETS** stands for **Error, Trend, Seasonality**. ETS models (such as Holt-Winters exponential smoothing) are widely used for time series forecasting and can be written as state space models.\n",
    "\n",
    "#### Example: Local Level + Trend + Seasonality\n",
    "\n",
    "Suppose we want to model a time series with:\n",
    "- **Level** ($\\ell_t$): the baseline value\n",
    "- **Trend** ($b_t$): the slope or growth rate\n",
    "- **Seasonality** ($s_t$): repeating patterns (e.g., yearly, weekly)\n",
    "\n",
    "A typical **additive ETS state space model** is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{State vector:} \\quad & x_t = \\begin{bmatrix} \\ell_t \\\\ b_t \\\\ s_t \\end{bmatrix} \\\\\n",
    "\\text{State transition:} \\quad & x_t = A x_{t-1} + w_t \\\\\n",
    "\\text{Observation:} \\quad & y_t = H x_t + v_t\n",
    "\\end{align*}\n",
    "$$\n",
    "where $A$ and $H$ are designed to encode how level, trend, and seasonality evolve and contribute to the observation.\n",
    "\n",
    "#### Example: Local Linear Trend + Seasonality\n",
    "\n",
    "For a time series with level, trend, and $S$-period seasonality:\n",
    "- **State vector:** $x_t = [\\ell_t, b_t, s_{t,1}, ..., s_{t,S-1}]^T$\n",
    "- **State transition matrix $A$:** updates level, trend, and rotates seasonal states\n",
    "- **Observation matrix $H$:** picks out the relevant components\n",
    "\n",
    "This can be written in the same form as the Kalman filter, and the Kalman filter can be used for inference if the noise is Gaussian.\n",
    "\n",
    "---\n",
    "\n",
    "### How to Model Level, Trend, and Seasonality in State Space\n",
    "\n",
    "- **Level:** Add a state variable $\\ell_t$ that evolves over time (e.g., random walk: $\\ell_t = \\ell_{t-1} + w_t$).\n",
    "- **Trend:** Add a state variable $b_t$ for the slope, with its own evolution (e.g., $b_t = b_{t-1} + w_t^{(b)}$).\n",
    "- **Seasonality:** Add $S-1$ state variables for seasonal effects, updated cyclically.\n",
    "\n",
    "**State vector example for additive model:**\n",
    "$$\n",
    "x_t = \\begin{bmatrix}\n",
    "\\ell_t \\\\\n",
    "b_t \\\\\n",
    "s_{t,1} \\\\\n",
    "\\vdots \\\\\n",
    "s_{t,S-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**State transition matrix $A$** and **observation matrix $H$** are constructed to reflect the desired dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Model Type         | State Space Form? | Kalman Filter Applicable? | Notes |\n",
    "|--------------------|------------------|--------------------------|-------|\n",
    "| ARMA/ARIMA         | Yes              | Yes (linear, Gaussian)   | Special case of state space |\n",
    "| ETS (Exponential Smoothing) | Yes      | Yes (linear, Gaussian)   | Level, trend, seasonality as states |\n",
    "| Structural Time Series | Yes           | Yes                      | Flexible, interpretable components |\n",
    "| Nonlinear/Non-Gaussian | Yes          | No (use EKF, Particle Filter) | Need approximate inference |\n",
    "\n",
    "---\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Durbin & Koopman, \"Time Series Analysis by State Space Methods\"](https://www.worldcat.org/title/1029823142)\n",
    "- [Hyndman et al., \"Forecasting: Principles and Practice\" (ETS models)](https://otexts.com/fpp3/ets.html)\n",
    "- [Wikipedia: State Space Representation](https://en.wikipedia.org/wiki/State-space_representation_(controls))\n",
    "\n",
    "---\n",
    "\n",
    "### Example: State Space Formulation for ETS(A,A,A) (Additive Error, Trend, Seasonality)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell_t &= \\ell_{t-1} + b_{t-1} + \\alpha e_t \\\\\n",
    "b_t &= b_{t-1} + \\beta e_t \\\\\n",
    "s_t &= s_{t-m} + \\gamma e_t \\\\\n",
    "y_t &= \\ell_{t-1} + b_{t-1} + s_{t-m} + e_t\n",
    "\\end{align*}\n",
    "$$\n",
    "where $e_t$ is the error term, $m$ is the seasonal period, and $\\alpha, \\beta, \\gamma$ are smoothing parameters.\n",
    "\n",
    "This can be written in the general state space form and solved with the Kalman filter if $e_t$ is Gaussian.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "The Kalman filter is a special case of the general state space approach, and many popular time series models (including ETS) can be written as state space models with appropriate state vectors and transition/observation matrices. This allows you to model not just the level, but also trend and seasonality, and to use the Kalman filter for efficient inference and forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf23983",
   "metadata": {},
   "source": [
    "### The Rauch-Tung-Striebel (RTS) Smoother (Smoothing: $\\mathcal{O}(T)$)\n",
    "\n",
    "The RTS Smoother performs a backward pass through the filtered estimates to improve the state estimates by incorporating information from future observations. It provides a more accurate estimate of the state at each time step than the filter alone.\n",
    "\n",
    "**Smoothing Step** (Slide 26):\n",
    "* Smoother gain: $G_t = P_t A^T (P_{t+1}^-)^{-1}$\n",
    "* Smoothed mean: $m_t^s = m_t + G_t (m_{t+1}^s - m_{t+1}^-)$\n",
    "* Smoothed covariance: $P_t^s = P_t + G_t (P_{t+1}^s - P_{t+1}^-) G_t^T$\n",
    "\n",
    "The overall complexity of the smoothing pass is $\\mathcal{O}(T \\cdot |X|^3)$ (Slide 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349324e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Rauch-Tung-Striebel (RTS) Smoother Implementation ---\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def rts_smoother(filtered_means, filtered_covs, predicted_means, predicted_covs, A, Q):\n",
    "    \"\"\"\n",
    "    Implements the Rauch-Tung-Striebel (RTS) Smoother.\n",
    "\n",
    "    Args:\n",
    "        filtered_means (jnp.ndarray): Means from the Kalman filter (T, state_dim).\n",
    "        filtered_covs (jnp.ndarray): Covariances from the Kalman filter (T, state_dim, state_dim).\n",
    "        predicted_means (jnp.ndarray): Predicted means from the Kalman filter (T, state_dim).\n",
    "        predicted_covs (jnp.ndarray): Predicted covariances from the Kalman filter (T, state_dim, state_dim).\n",
    "        A (jnp.ndarray): State transition matrix (state_dim, state_dim).\n",
    "        Q (jnp.ndarray): State noise covariance (state_dim, state_dim).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (smoothed_means, smoothed_covs)\n",
    "    \"\"\"\n",
    "    T = filtered_means.shape[0]\n",
    "    state_dim = filtered_means.shape[1]\n",
    "\n",
    "    smoothed_means = jnp.copy(filtered_means)\n",
    "    smoothed_covs = jnp.copy(filtered_covs)\n",
    "\n",
    "    for t in reversed(range(T - 1)):\n",
    "        # Smoother Gain\n",
    "        # P_t_plus_1_minus_inv = jnp.linalg.inv(predicted_covs[t+1]) # Direct inverse, can be unstable\n",
    "        # G_t = filtered_covs[t] @ A.T @ P_t_plus_1_minus_inv\n",
    "\n",
    "        # More stable way to compute G_t using solve\n",
    "        G_t = jnp.linalg.solve(\n",
    "            predicted_covs[t + 1].T, (filtered_covs[t] @ A.T).T\n",
    "        ).T  # (P_t_plus_1_minus)^-1 * (P_t * A.T)\n",
    "\n",
    "        # Smoothed Mean\n",
    "        smoothed_means = smoothed_means.at[t].set(\n",
    "            filtered_means[t] + G_t @ (smoothed_means[t + 1] - predicted_means[t + 1])\n",
    "        )\n",
    "\n",
    "        # Smoothed Covariance\n",
    "        smoothed_covs = smoothed_covs.at[t].set(\n",
    "            filtered_covs[t]\n",
    "            + G_t @ (smoothed_covs[t + 1] - predicted_covs[t + 1]) @ G_t.T\n",
    "        )\n",
    "\n",
    "    return smoothed_means, smoothed_covs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6243d651",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Understanding the Rauch-Tung-Striebel (RTS) Smoother\n",
    "\n",
    "### What is Smoothing in State Space Models?\n",
    "\n",
    "In time series models, **filtering** and **smoothing** are two different inference tasks:\n",
    "\n",
    "- **Filtering**: At each time step $t$, estimate the hidden state $x_t$ using all observations up to and including time $t$ (i.e., $y_{1:t}$). This is what the **Kalman filter** does.\n",
    "    - **Filtered estimate:** $p(x_t \\mid y_{1:t})$\n",
    "    - **Use case:** Real-time estimation, forecasting, control.\n",
    "\n",
    "- **Smoothing**: After collecting the entire sequence of observations $y_{1:T}$, estimate the hidden state $x_t$ at each time step using **all** observations, both past and future.\n",
    "    - **Smoothed estimate:** $p(x_t \\mid y_{1:T})$\n",
    "    - **Use case:** Retrospective analysis, denoising, signal reconstruction.\n",
    "\n",
    "**Key Difference:**  \n",
    "- The filter only uses information up to the current time step, while the smoother uses the entire dataset, including future observations, to refine the estimate at each time.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is Smoothing More Accurate?\n",
    "\n",
    "- **Filtering** is causal: it cannot \"see the future.\" Its estimate at time $t$ is optimal given only $y_{1:t}$.\n",
    "- **Smoothing** is acausal: it can use all data, including $y_{t+1}, y_{t+2}, ..., y_T$. This extra information allows it to correct or refine earlier state estimates, often reducing uncertainty and error.\n",
    "\n",
    "---\n",
    "\n",
    "### The Rauch-Tung-Striebel (RTS) Smoother\n",
    "\n",
    "The **RTS smoother** is an efficient algorithm for computing the smoothed state estimates in linear Gaussian state space models (i.e., Gauss-Markov models). It works in two stages:\n",
    "\n",
    "1. **Forward Pass (Filtering):** Run the Kalman filter to compute $p(x_t \\mid y_{1:t})$ for all $t$.\n",
    "2. **Backward Pass (Smoothing):** Starting from the last time step, recursively refine the state estimates using future information.\n",
    "\n",
    "#### RTS Smoother Equations\n",
    "\n",
    "Let:\n",
    "- $m_t$ and $P_t$ be the filtered mean and covariance at time $t$ (from the Kalman filter).\n",
    "- $m_{t+1}^-$ and $P_{t+1}^-$ be the predicted mean and covariance for $x_{t+1}$ given $y_{1:t}$.\n",
    "- $m_{t+1}^s$ and $P_{t+1}^s$ be the smoothed mean and covariance at time $t+1$.\n",
    "\n",
    "The RTS smoother computes, for $t = T-1, ..., 0$:\n",
    "- **Smoother gain:**  \n",
    "  $$\n",
    "  G_t = P_t A^\\top (P_{t+1}^-)^{-1}\n",
    "  $$\n",
    "- **Smoothed mean:**  \n",
    "  $$\n",
    "  m_t^s = m_t + G_t (m_{t+1}^s - m_{t+1}^-)\n",
    "  $$\n",
    "- **Smoothed covariance:**  \n",
    "  $$\n",
    "  P_t^s = P_t + G_t (P_{t+1}^s - P_{t+1}^-) G_t^\\top\n",
    "  $$\n",
    "\n",
    "**Intuition:**  \n",
    "- The smoother gain $G_t$ determines how much to adjust the filtered estimate at time $t$ based on the difference between the smoothed and predicted state at $t+1$.\n",
    "- If the future data suggests that the prediction at $t+1$ was off, the smoother \"corrects\" the earlier state accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### Kalman Filter vs. RTS Smoother: Summary Table\n",
    "\n",
    "| Task      | Estimate                | Uses Future Data? | Typical Use Case         |\n",
    "|-----------|-------------------------|-------------------|-------------------------|\n",
    "| Filter    | $p(x_t \\mid y_{1:t})$   | No                | Real-time, forecasting  |\n",
    "| Smoother  | $p(x_t \\mid y_{1:T})$   | Yes               | Retrospective analysis  |\n",
    "\n",
    "- **Kalman filter**: Fast, online, but cannot use information from after time $t$.\n",
    "- **RTS smoother**: Two-pass (forward + backward), uses all data, gives more accurate and less uncertain state estimates.\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "- In the plot above, the **green dashed line** (filtered mean) is the Kalman filter's estimate at each time, using only past and present data.\n",
    "- The **purple line** (smoothed mean) is the RTS smoother's estimate, which is typically closer to the true state and has smaller uncertainty, because it uses the entire sequence of observations.\n",
    "\n",
    "---\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Wikipedia: Kalman Smoother](https://en.wikipedia.org/wiki/Kalman_filter#Fixed-interval_smoothers)\n",
    "- [Murphy, \"Probabilistic Machine Learning: An Introduction\", Section 18.4](https://probml.github.io/pml-book/book2.html#kalman-smoother)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4478d",
   "metadata": {},
   "source": [
    "### Example: Applying Kalman Filter and RTS Smoother\n",
    "\n",
    "Let's generate some synthetic 1D time series data and apply the Kalman Filter and RTS Smoother to estimate the underlying true state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8ac48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Main Execution: Kalman Filter and RTS Smoother Example ---\n",
    "\n",
    "# Generate data\n",
    "T = 100  # Number of time steps\n",
    "state_dim = 1  # 1D state\n",
    "obs_dim = 1  # 1D observation\n",
    "\n",
    "A = jnp.array([[0.98]])  # Slightly decaying state\n",
    "Q_std = 0.1  # Standard deviation of state noise\n",
    "H = jnp.array([[1.0]])  # Direct observation of state\n",
    "R_std = 0.5  # Standard deviation of observation noise\n",
    "m0 = jnp.array([0.0])  # Initial state mean\n",
    "P0_std = 1.0  # Initial state standard deviation\n",
    "\n",
    "key = jax.random.PRNGKey(50)\n",
    "true_states, observations, A, Q, H, R, m0, P0 = generate_linear_gaussian_data(\n",
    "    T=T,\n",
    "    state_dim=state_dim,\n",
    "    obs_dim=obs_dim,\n",
    "    A=A,\n",
    "    Q_std=Q_std,\n",
    "    H=H,\n",
    "    R_std=R_std,\n",
    "    m0=m0,\n",
    "    P0_std=P0_std,\n",
    "    key=key,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Running Kalman Filter ---\")\n",
    "filtered_means, filtered_covs, predicted_means, predicted_covs = kalman_filter(\n",
    "    observations, A, Q, H, R, m0, P0\n",
    ")\n",
    "filtered_stds = jnp.sqrt(jnp.diagonal(filtered_covs, axis1=1, axis2=2))\n",
    "\n",
    "print(\"\\n--- Running RTS Smoother ---\")\n",
    "smoothed_means, smoothed_covs = rts_smoother(\n",
    "    filtered_means, filtered_covs, predicted_means, predicted_covs, A, Q\n",
    ")\n",
    "smoothed_stds = jnp.sqrt(jnp.diagonal(smoothed_covs, axis1=1, axis2=2))\n",
    "\n",
    "# Plotting results for the first (and only) state dimension\n",
    "plot_kalman_results(\n",
    "    true_states,\n",
    "    observations,\n",
    "    filtered_means,\n",
    "    filtered_stds,\n",
    "    smoothed_means,\n",
    "    smoothed_stds,\n",
    "    title=\"Kalman Filter and RTS Smoother Results (1D State)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2078eb6d",
   "metadata": {},
   "source": [
    "In the plot above, you can observe:\n",
    "* **True State (Blue)**: The actual underlying process.\n",
    "* **Observations (Red Markers)**: Noisy measurements of the true state.\n",
    "* **Filtered Mean (Green Dashed Line)**: The Kalman filter's estimate of the state at each time step, using all observations up to that point. The green shaded area represents its uncertainty (2 standard deviations).\n",
    "* **Smoothed Mean (Purple Line)**: The RTS smoother's estimate, which is generally closer to the true state and has smaller uncertainty (purple shaded area) because it uses *all* available observations (past and future) for each time step's estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123aeab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from functools import partial\n",
    "\n",
    "# Example: State Space Model with Level, Trend, and Seasonality (1D observation)\n",
    "# We'll use a local linear trend + seasonal model, and apply Kalman Filter and RTS Smoother.\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Parameters\n",
    "T = 120  # Number of time steps\n",
    "season_period = 12  # e.g., monthly seasonality\n",
    "state_dim = 2 + (season_period - 1)  # [level, trend, season_1, ..., season_{S-1}]\n",
    "obs_dim = 1\n",
    "\n",
    "# State transition matrix A\n",
    "A = jnp.zeros((state_dim, state_dim))\n",
    "# Level update: level_t = level_{t-1} + trend_{t-1}\n",
    "A = A.at[0, 0].set(1.0)\n",
    "A = A.at[0, 1].set(1.0)\n",
    "# Trend update: trend_t = trend_{t-1}\n",
    "A = A.at[1, 1].set(1.0)\n",
    "# Seasonality update: rotate seasonal components\n",
    "for i in range(2, state_dim - 1):\n",
    "    A = A.at[i, i + 1].set(1.0)\n",
    "A = A.at[state_dim - 1, 2].set(1.0)  # wrap-around\n",
    "\n",
    "# Observation matrix H: observe level + current season\n",
    "H = jnp.zeros((obs_dim, state_dim))\n",
    "H = H.at[0, 0].set(1.0)  # level\n",
    "H = H.at[0, 2].set(1.0)  # current season\n",
    "\n",
    "# Noise covariances\n",
    "Q_std_level = 0.05\n",
    "Q_std_trend = 0.01\n",
    "Q_std_season = 0.01\n",
    "Q = jnp.diag(\n",
    "    jnp.array(\n",
    "        [Q_std_level**2, Q_std_trend**2] + [Q_std_season**2] * (season_period - 1)\n",
    "    )\n",
    ")\n",
    "R_std = 0.3\n",
    "R = jnp.eye(obs_dim) * R_std**2\n",
    "\n",
    "# Initial state\n",
    "m0 = jnp.zeros(state_dim)\n",
    "m0 = m0.at[0].set(2.0)  # initial level\n",
    "m0 = m0.at[1].set(0.1)  # initial trend\n",
    "# Initial seasonality: sum to zero constraint (last component is -sum(others))\n",
    "season_init = jnp.array([0.5, -0.3, 0.2, -0.4, 0.1, 0.3, -0.2, 0.0, 0.1, -0.1, -0.2])\n",
    "m0 = m0.at[2:].set(season_init)\n",
    "P0 = jnp.eye(state_dim) * 1.0\n",
    "\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_trend_season_data(T, A, Q, H, R, m0, key):\n",
    "    state_dim = A.shape[0]\n",
    "    obs_dim = H.shape[0]\n",
    "    states = [m0]\n",
    "    observations = []\n",
    "    key = key\n",
    "    for t in range(T):\n",
    "        key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "        state_noise = jax.random.multivariate_normal(subkey1, jnp.zeros(state_dim), Q)\n",
    "        next_state = A @ states[-1] + state_noise\n",
    "        # Enforce sum-to-zero constraint for seasonality\n",
    "        season_sum = jnp.sum(next_state[2:])\n",
    "        next_state = next_state.at[2:].add(-season_sum / (season_period - 1))\n",
    "        obs_noise = jax.random.multivariate_normal(subkey2, jnp.zeros(obs_dim), R)\n",
    "        observation = H @ next_state + obs_noise\n",
    "        states.append(next_state)\n",
    "        observations.append(observation)\n",
    "        # Rotate H to observe the correct seasonal component\n",
    "        H = jnp.roll(H, shift=1, axis=1)\n",
    "    return jnp.array(states[1:]), jnp.array(observations)\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(123)\n",
    "true_states, observations = generate_trend_season_data(T, A, Q, H, R, m0, key)\n",
    "\n",
    "\n",
    "# Because H rotates, we need to reconstruct the correct H for each time step for filtering\n",
    "def get_H_sequence(obs_dim, state_dim, season_period, T):\n",
    "    H_seq = []\n",
    "    H = jnp.zeros((obs_dim, state_dim))\n",
    "    H = H.at[0, 0].set(1.0)\n",
    "    H = H.at[0, 2].set(1.0)\n",
    "    for t in range(T):\n",
    "        H_seq.append(H)\n",
    "        H = jnp.roll(H, shift=1, axis=1)\n",
    "    return jnp.stack(H_seq)\n",
    "\n",
    "\n",
    "H_seq = get_H_sequence(obs_dim, state_dim, season_period, T)\n",
    "\n",
    "\n",
    "# Kalman filter for time-varying H\n",
    "def kalman_filter_timevarying_H(observations, A, Q, H_seq, R, m0, P0):\n",
    "    T = observations.shape[0]\n",
    "    state_dim = m0.shape[0]\n",
    "    filtered_means = jnp.zeros((T, state_dim))\n",
    "    filtered_covs = jnp.zeros((T, state_dim, state_dim))\n",
    "    predicted_means = jnp.zeros((T, state_dim))\n",
    "    predicted_covs = jnp.zeros((T, state_dim, state_dim))\n",
    "    m_prev = m0\n",
    "    P_prev = P0\n",
    "    for t in range(T):\n",
    "        H = H_seq[t]\n",
    "        # Prediction\n",
    "        m_minus = A @ m_prev\n",
    "        P_minus = A @ P_prev @ A.T + Q\n",
    "        predicted_means = predicted_means.at[t].set(m_minus)\n",
    "        predicted_covs = predicted_covs.at[t].set(P_minus)\n",
    "        # Update\n",
    "        z = observations[t] - H @ m_minus\n",
    "        S = H @ P_minus @ H.T + R\n",
    "        K = P_minus @ H.T @ jnp.linalg.inv(S)\n",
    "        m_t = m_minus + K @ z\n",
    "        P_t = (jnp.eye(state_dim) - K @ H) @ P_minus\n",
    "        filtered_means = filtered_means.at[t].set(m_t)\n",
    "        filtered_covs = filtered_covs.at[t].set(P_t)\n",
    "        m_prev = m_t\n",
    "        P_prev = P_t\n",
    "    return filtered_means, filtered_covs, predicted_means, predicted_covs\n",
    "\n",
    "\n",
    "filtered_means, filtered_covs, predicted_means, predicted_covs = (\n",
    "    kalman_filter_timevarying_H(observations, A, Q, H_seq, R, m0, P0)\n",
    ")\n",
    "filtered_stds = jnp.sqrt(jnp.diagonal(filtered_covs, axis1=1, axis2=2))\n",
    "\n",
    "# RTS smoother (A and Q are constant)\n",
    "\n",
    "\n",
    "def rts_smoother_timevarying_A(\n",
    "    filtered_means, filtered_covs, predicted_means, predicted_covs, A, Q\n",
    "):\n",
    "    T = filtered_means.shape[0]\n",
    "    state_dim = filtered_means.shape[1]\n",
    "    smoothed_means = jnp.copy(filtered_means)\n",
    "    smoothed_covs = jnp.copy(filtered_covs)\n",
    "    for t in reversed(range(T - 1)):\n",
    "        G_t = jnp.linalg.solve(predicted_covs[t + 1].T, (filtered_covs[t] @ A.T).T).T\n",
    "        smoothed_means = smoothed_means.at[t].set(\n",
    "            filtered_means[t] + G_t @ (smoothed_means[t + 1] - predicted_means[t + 1])\n",
    "        )\n",
    "        smoothed_covs = smoothed_covs.at[t].set(\n",
    "            filtered_covs[t]\n",
    "            + G_t @ (smoothed_covs[t + 1] - predicted_covs[t + 1]) @ G_t.T\n",
    "        )\n",
    "    return smoothed_means, smoothed_covs\n",
    "\n",
    "\n",
    "smoothed_means, smoothed_covs = rts_smoother_timevarying_A(\n",
    "    filtered_means, filtered_covs, predicted_means, predicted_covs, A, Q\n",
    ")\n",
    "smoothed_stds = jnp.sqrt(jnp.diagonal(smoothed_covs, axis1=1, axis2=2))\n",
    "\n",
    "# Plot: show observed, true, filtered, and smoothed for the level (state_idx=0)\n",
    "plot_kalman_results(\n",
    "    true_states,\n",
    "    observations,\n",
    "    filtered_means,\n",
    "    filtered_stds,\n",
    "    smoothed_means,\n",
    "    smoothed_stds,\n",
    "    title=\"State Space Model with Trend and Seasonality: Kalman Filter & RTS Smoother\",\n",
    "    state_idx=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35372e7",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "### Interpreting the Results of State Estimation from the Kalman Filter (KF) and RTS Smoother\n",
    "\n",
    "When you run the Kalman Filter (KF) and Rauch-Tung-Striebel (RTS) Smoother on a state space model, you obtain **estimates of the hidden (latent) states** at each time step, not the model parameters themselves. Hereâ€™s how to interpret these results:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **State Estimates vs. Parameter Estimates**\n",
    "\n",
    "- **State Estimates** (`filtered_means`, `smoothed_means`):  \n",
    "    These are the best guesses (means) of the hidden state vector at each time step, given the observations and the current model parameters ($A$, $Q$, $H$, $R$, $m_0$, $P_0$).\n",
    "        - **Kalman Filter:** $p(x_t \\mid y_{1:t})$ â€” uses only past and present data.\n",
    "        - **RTS Smoother:** $p(x_t \\mid y_{1:T})$ â€” uses all data (past and future).\n",
    "\n",
    "- **Parameter Estimates**:  \n",
    "    The matrices $A$, $Q$, $H$, $R$, $m_0$, $P_0$ are typically **assumed known** when running the filter/smoother. If you want to fit (learn) these parameters from data, you need to use a separate procedure (e.g., Expectation-Maximization, maximum likelihood, or Bayesian inference).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **What Do the State Estimates Tell Us?**\n",
    "\n",
    "- **Filtered Means (`filtered_means`)**:  \n",
    "    At each time $t$, this is your best estimate of the state, using all data up to $t$. Useful for real-time tracking and forecasting.\n",
    "\n",
    "- **Smoothed Means (`smoothed_means`)**:  \n",
    "    At each time $t$, this is your best estimate of the state, using all data in the sequence. Useful for retrospective analysis, denoising, and signal reconstruction.\n",
    "\n",
    "- **Uncertainties (`filtered_stds`, `smoothed_stds`)**:  \n",
    "    The standard deviations (square roots of the diagonal of the covariance matrices) quantify your uncertainty about each state component at each time step.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **How to Use These Results**\n",
    "\n",
    "- **Interpretation**:  \n",
    "    - The estimated states can be interpreted according to your model structure. For example, in a local trend + seasonality model:\n",
    "        - The first state might represent the **level** (baseline).\n",
    "        - The second state might represent the **trend** (slope).\n",
    "        - The remaining states might represent **seasonal effects**.\n",
    "    - By plotting these components, you can see how the underlying process evolves over time, separated from noise.\n",
    "\n",
    "- **Diagnostics**:  \n",
    "    - If the filter/smoother estimates track the true states well (in simulation), your model is likely well-specified.\n",
    "    - Large uncertainties or systematic deviations may indicate model mismatch or the need for parameter tuning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Parameter Fitting (Learning)**\n",
    "\n",
    "- **KF/RTS do not fit parameters**:  \n",
    "    The Kalman Filter and RTS Smoother **assume** the model parameters are known. They only estimate the hidden states.\n",
    "\n",
    "- **Parameter Learning**:  \n",
    "    To fit parameters ($A$, $Q$, $H$, $R$, etc.) from data, you typically use:\n",
    "        - **Maximum Likelihood Estimation (MLE)**: Find parameters that maximize the likelihood of the observed data, often using the Expectation-Maximization (EM) algorithm.\n",
    "        - **Bayesian Inference**: Place priors on parameters and infer their posterior distributions.\n",
    "    - After fitting, you can rerun the KF/RTS with the learned parameters for improved state estimation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Summary Table**\n",
    "\n",
    "| Output                | What it means                                      | How to use it                |\n",
    "|-----------------------|----------------------------------------------------|------------------------------|\n",
    "| `filtered_means`      | State estimates using data up to time $t$          | Real-time tracking, forecast |\n",
    "| `smoothed_means`      | State estimates using all data                     | Retrospective analysis       |\n",
    "| `filtered_stds`       | Uncertainty in filtered state estimates            | Confidence intervals         |\n",
    "| `smoothed_stds`       | Uncertainty in smoothed state estimates            | Confidence intervals         |\n",
    "| Model parameters      | Assumed fixed during filtering/smoothing           | Need to fit separately       |\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Key Takeaway**\n",
    "\n",
    "> **The Kalman Filter and RTS Smoother provide optimal estimates of the hidden states, given the model parameters. They do not fit the model parameters themselves. To learn parameters, you need additional optimization or inference procedures.**\n",
    "\n",
    "---\n",
    "\n",
    "**Further Reading:**\n",
    "- [Murphy, \"Probabilistic Machine Learning: An Introduction\", Section 18.5 (Parameter Learning)](https://probml.github.io/pml-book/book2.html#kalman-learning)\n",
    "- [Wikipedia: Kalman Filter â€“ Parameter Estimation](https://en.wikipedia.org/wiki/Kalman_filter#Parameter_estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167891b6",
   "metadata": {},
   "source": [
    "#### 6. Summary\n",
    "\n",
    "To summarize (Slide 30):\n",
    "\n",
    "* **Markov Chains** formalize the notion of a stochastic process with a local finite memory through conditional independence.\n",
    "* **Gauss-Markov models** map this state to linear algebra, where state transitions and observations are linear and Gaussian.\n",
    "* The **Kalman filter** is the name for the corresponding analytic filtering algorithm.\n",
    "* **Bayesian filters** (like the Kalman filter and RTS smoother) are not just for signal processing of \"fast\" signals, but a general tool for inference in a chain of experiments/observations, enabling efficient sequential and retrospective analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a8071a",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "**Exercise 1: Impact of Noise Levels**\n",
    "Experiment with different values for `Q_std` (state noise standard deviation) and `R_std` (observation noise standard deviation) in the `generate_linear_gaussian_data` function. How do changes in these parameters affect the filtered and smoothed estimates and their uncertainties? Discuss why this happens.\n",
    "\n",
    "**Exercise 2: Higher Dimensional State**\n",
    "Modify the `state_dim` to 2 or 3. You will also need to adjust `A`, `Q`, `H`, `R`, `m0`, and `P0` to be matrices/vectors of appropriate dimensions. For example, for `state_dim=2`:\n",
    "```python\n",
    "A = jnp.array([[0.9, 0.1], [0.0, 0.9]])\n",
    "Q_std = 0.1; Q = jnp.eye(state_dim) * Q_std**2\n",
    "H = jnp.array([[1.0, 0.0]]) # Observe only the first state dimension\n",
    "R_std = 0.5; R = jnp.eye(obs_dim) * R_std**2\n",
    "m0 = jnp.array([0.0, 0.0])\n",
    "P0_std = 1.0; P0 = jnp.eye(state_dim) * P0_std**2\n",
    "```\n",
    "Run the filter and smoother. How does the complexity of the matrices change? Plot the results for each state dimension (e.g., `state_idx=0` and `state_idx=1`).\n",
    "\n",
    "**Exercise 3: Non-Identity Observation Matrix**\n",
    "Change the `H` matrix to observe a linear combination of states. For example, if `state_dim=2` and `obs_dim=1`, set `H = jnp.array([[0.5, 0.5]])`. How does this affect the filter's ability to estimate individual state components? Discuss the role of observability.\n",
    "\n",
    "**Exercise 4 (Advanced): Implementing a Simple Non-Linear Filter (e.g., Extended Kalman Filter concept)**\n",
    "The Kalman Filter is for linear systems. Briefly research the concept of the **Extended Kalman Filter (EKF)**, which handles non-linear state transitions and/or observation models by linearizing them around the current mean estimate. Describe conceptually how you would modify the `kalman_filter` function to incorporate a non-linear `f(x_t-1)` or `h(x_t)` function using JAX's `jax.jacobian` for linearization."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
