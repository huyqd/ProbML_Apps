{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71a2144",
   "metadata": {},
   "source": [
    "# The Role of Linear Algebra in Gaussian Processes\n",
    "\n",
    "Welcome to the final installment of our Gaussian Processes (GPs) blog series! Throughout this series, we've explored:\n",
    "\n",
    "- The fundamentals of GPs as distributions over functions\n",
    "- The power and flexibility of kernels\n",
    "- Building complex models with additive kernels\n",
    "- Learning hyperparameters for better model performance\n",
    "\n",
    "In this lecture, we'll shift our focus to the computational heart of Gaussian Processes: **linear algebra**. While GPs provide elegant probabilistic models, their practical application often depends on efficiently solving large linear systems. Understanding the linear algebra behind GPs is crucial for both using them effectively and appreciating their computational challenges.\n",
    "\n",
    "We'll cover:\n",
    "\n",
    "- **Why GPs are computationally expensive, especially for large datasets**\n",
    "- **The key linear algebra operations that power GP inference**\n",
    "- **How these operations relate to numerical methods like LU and Cholesky decomposition**\n",
    "- **The computational complexity of GPs compared to deep learning models**\n",
    "- **How uncertainty quantification in GPs contributes to their computational cost**\n",
    "\n",
    "This lecture will provide a crucial \"behind-the-scenes\" look at what makes GPs tick computationally, equipping you with the knowledge to understand both their power and their limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b8e8f",
   "metadata": {},
   "source": [
    "## Recap from Last Week: Key GP Insights\n",
    "\n",
    "Before diving into the computational details, let's quickly recap the main insights from our previous lecture (**Lecture 11: Understanding Kernels and GPs**):\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **GPs as Function Distributions**\n",
    "\n",
    "- **Gaussian Processes (GPs)** are probability distributions over function spaces.\n",
    "- The exact nature of this probability space is often subtle and depends on the kernel.\n",
    "- Understanding the sample space of a GP requires studying the kernel itself.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Kernel-Covariance Equivalence**\n",
    "\n",
    "- Every covariance function is a kernel, and every positive-definite kernel can serve as the covariance function of a GP.\n",
    "- **Key takeaway:** Specifying a valid kernel is sufficient to define a GP.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Kernels as \"Infinite Matrices\"**\n",
    "\n",
    "- Kernels have eigenfunctions and eigenvalues, analogous to matrices having eigenvectors and eigenvalues.\n",
    "- This allows us to conceptualize kernels as a kind of \"infinite matrix\" that spans a space of functions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Reproducing Kernel Hilbert Space (RKHS)**\n",
    "\n",
    "- The RKHS is the space of all possible posterior mean functions of the GP regression method.\n",
    "- This links GPs to Frequentist kernel machines.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Posterior Variance as Worst-Case Error**\n",
    "\n",
    "- The GP's posterior covariance function (the Bayesian expected squared error) has a Frequentist interpretation as a worst-case squared error in the RKHS.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **GP Samples vs. RKHS**\n",
    "\n",
    "- **Crucial distinction:** Sample paths drawn from a GP generally do **not** lie in the RKHS of the kernel.\n",
    "- GP samples tend to be \"rougher\" and reside in a larger function space than the RKHS.\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** This last point, about GP samples not being in the RKHS, often raises questions, which we'll address further in this lecture.\n",
    "\n",
    "---\n",
    "\n",
    "## What About the Samples? Why Does it Matter if Samples are Outside the RKHS?\n",
    "\n",
    "A common question from beginners:\n",
    "\n",
    "> *\"Why does it matter in practice that our samples can be outside of the RKHS? Aren't most of them concentrated around the mean and thus inside the RKHS anyway?\"*\n",
    "\n",
    "- The intuition that functions \"close\" to each other pointwise should also be \"close\" in a function space norm is appealing but often misleading.\n",
    "- Just because two functions are close at every point, i.e., $|f(x) - g(x)| < \\epsilon$ for all $x \\in X$, they don't have to be close in a norm on the function space, i.e., we could have $\\|f - g\\| \\gg \\epsilon$.\n",
    "- Different norms measure different aspects of \"closeness\" or \"smoothness.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Driscoll's Zero-One Law**\n",
    "\n",
    "In most interesting cases, especially for infinite-dimensional RKHSs, GP samples do **not** lie in the RKHS of the kernel $k$. This is a consequence of a powerful result:\n",
    "\n",
    "> **Theorem (Driscoll's Zero-One Law, simplified; see Kanagawa et al., 2018, Theorem 4.9):**  \n",
    "> Let $f \\sim \\mathcal{GP}(m, k)$ be a Gaussian process with $m \\in \\mathcal{H}_k$ on the probability space $(\\Omega, \\mathcal{F}, P)$. If $\\mathcal{H}_k$ is infinite-dimensional, then:\n",
    ">\n",
    "> $$\n",
    "> P(f \\in \\mathcal{H}_k) = 0\n",
    "> $$\n",
    ">\n",
    "> This means that if the RKHS is infinite-dimensional (which is true for common kernels like the Squared Exponential), the probability of a randomly drawn sample path from the GP actually belonging to the RKHS is zero. This is a \"zero-one law\" because the probability is either 0 or 1.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Should You Care in Practice? Smoothness Properties\n",
    "\n",
    "This theoretical distinction has practical implications, particularly concerning **smoothness properties**:\n",
    "\n",
    "- The RKHS norm often penalizes \"roughness\" or \"complexity.\"\n",
    "- If GP samples almost surely have infinite RKHS norm, it implies they are \"rougher\" than functions typically found in the RKHS.\n",
    "\n",
    "**This matters when:**\n",
    "\n",
    "- **Interpreting samples:** If you're sampling from a GP to visualize possible realizations, understand that these samples are not as \"smooth\" (in the RKHS sense) as the posterior mean.\n",
    "- **Choosing kernels:** If you have prior knowledge about the true underlying function's smoothness, choose a kernel whose sample space (not just its RKHS) aligns with that knowledge. This can lead to better generalization and faster learning.\n",
    "\n",
    "---\n",
    "\n",
    "## What About the Samples? Detailed Look at Sample Spaces of Gaussian Processes\n",
    "\n",
    "It's generally difficult to talk about the precise \"sample space\" of a GP. Instead, one typically identifies other spaces of functions that contain the samples as a subset.\n",
    "\n",
    "- If we know the target function lies in a certain space (e.g., it's known to be continuous or differentiable), we should choose the kernel of the GP such that its sample space matches that space as closely as possible. This can accelerate learning.\n",
    "\n",
    "**Examples of function spaces that might contain GP samples:**\n",
    "\n",
    "- $\\mathbb{R}^X$ (the space of all real-valued functions on $X$, which is too large for practical use).\n",
    "- **Banach space $C(X)$** of continuous functions. Kernels like the Squared Exponential and Matérn ($\\nu > 0$) produce continuous sample paths.\n",
    "- **Banach space $C^k(X)$** of $k$-times continuously differentiable functions (relevant for derivative observations or modeling smooth physical processes).\n",
    "- **Sobolev spaces $W_2^k(X)$** (e.g., for inferring solutions to PDEs, where functions are required to have square-integrable derivatives up to order $k$).\n",
    "\n",
    "---\n",
    "\n",
    "## GP Samples are Not in the RKHS! But Almost...\n",
    "\n",
    "While GP samples are almost surely **not** in the RKHS, they belong to a kind of \"completion\" of the RKHS, often referred to as a **\"power\" of the RKHS**.\n",
    "\n",
    "> **Theorem (Kanagawa, 2018; restricted from Steinwart, 2017, itself generalized from Driscoll, 1973):**  \n",
    "> Let $\\mathcal{H}_k$ be an RKHS and $0 < \\theta \\leq 1$. Consider the $\\theta$-power of $\\mathcal{H}_k$ given by:\n",
    ">\n",
    "> $$\n",
    "> \\mathcal{H}_k^\\theta = \\left\\{ f(x) := \\sum_{i \\in I} \\alpha_i \\lambda_i^{\\theta/2} \\phi_i(x) \\quad \\text{such that} \\quad |f|^2_{\\mathcal{H}_k^\\theta} := \\sum_{i \\in I} \\alpha_i^2 < \\infty \\right\\}\n",
    "> $$\n",
    ">\n",
    "> with $\\langle f, g \\rangle_{\\mathcal{H}_k^\\theta} := \\sum_{i \\in I} \\alpha_i \\beta_i$.\n",
    ">\n",
    "> Then, if $\\sum_{i \\in I} \\lambda_i^{1-\\theta} < \\infty$, it implies that $f \\sim \\mathcal{GP}(0, k) \\in \\mathcal{H}_k^\\theta$ with probability 1.\n",
    "\n",
    "**Interpretation:**  \n",
    "GP samples are \"almost\" in the RKHS; they belong to a slightly larger space where the eigenvalues $\\lambda_i$ decay sufficiently fast such that the sum $\\sum_{i \\in I} \\lambda_i^{1-\\theta}$ converges. This \"power\" of the RKHS can be strictly larger than the RKHS itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6cf0cd",
   "metadata": {},
   "source": [
    "## How Expensive is GP Regression? What if the Dataset is Very Large?\n",
    "\n",
    "Gaussian Process (GP) regression is celebrated for its flexibility and principled uncertainty quantification, but these benefits come at a significant computational cost—especially as datasets grow larger.\n",
    "\n",
    "---\n",
    "\n",
    "### **The Computational Bottleneck**\n",
    "\n",
    "- **Kernel Matrix Construction:**  \n",
    "    In GP regression, we construct the kernel (covariance) matrix $K_{XX}$, which is of size $N \\times N$, where $N$ is the number of training data points.\n",
    "- **Matrix Inversion or Linear Solves:**  \n",
    "    Making predictions and computing the marginal likelihood both require either inverting $K_{XX}$ or solving linear systems involving it.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Expensive?**\n",
    "\n",
    "- **Cubic Complexity:**  \n",
    "    The direct inversion of an $N \\times N$ matrix has computational complexity $\\mathcal{O}(N^3)$.  \n",
    "    - *Implication:* Doubling the dataset size increases computation time by a factor of eight!\n",
    "- **Memory Usage:**  \n",
    "    Storing the kernel matrix requires $\\mathcal{O}(N^2)$ memory, which can also become prohibitive for large $N$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Scalability in Practice**\n",
    "\n",
    "- For small datasets (hundreds of points), standard GP regression is practical and efficient.\n",
    "- For medium to large datasets (thousands or more), the cubic scaling quickly becomes a bottleneck, making naive GP regression infeasible.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Does This Matter?**\n",
    "\n",
    "Understanding these computational challenges is crucial for:\n",
    "- **Choosing the right model for your data size**\n",
    "- **Selecting appropriate approximation methods for scalability**\n",
    "- **Appreciating the trade-offs between model expressiveness and computational feasibility**\n",
    "\n",
    "---\n",
    "\n",
    "> **Next:**  \n",
    "> We'll dive deeper into the linear algebra operations at the heart of GP regression, and explore how modern numerical methods and approximations can help overcome these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2858d",
   "metadata": {},
   "source": [
    "# The Inside View on GP Regression: A Deeper Look into the Probabilistic ML Stack\n",
    "\n",
    "Gaussian Process (GP) regression is a powerful and flexible tool in probabilistic machine learning. To truly understand its computational cost and structure, let's break down the GP regression process into different \"layers\" of abstraction, from the high-level application to the low-level numerical operations.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Application Layer\n",
    "\n",
    "**What problem are we solving?**\n",
    "\n",
    "- **Goal:** Learn a function $f: \\mathcal{X} \\to \\mathbb{R}$ from input-output pairs $\\{(x_i, y_i)\\}_{i=1}^N$.\n",
    "- **Setting:** This is a supervised machine learning problem, where we observe data and want to make predictions at new, unseen points.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Model Layer\n",
    "\n",
    "**How do we model the data probabilistically?**\n",
    "\n",
    "- **Prior:** We assume $f$ is a sample from a Gaussian process:\n",
    "    $$\n",
    "    p(f) = \\mathcal{GP}(\\mu, k)\n",
    "    $$\n",
    "    where $\\mu$ is the mean function and $k$ is the kernel (covariance) function.\n",
    "\n",
    "- **Likelihood:** The observed data $y$ are noisy observations of $f$ at the training inputs $X = [x_1, ..., x_N]$:\n",
    "    $$\n",
    "    p(y \\mid f_X) = \\mathcal{N}(y \\mid f_X, \\sigma^2 I_N)\n",
    "    $$\n",
    "    where $f_X = [f(x_1), ..., f(x_N)]^\\top$ and $\\sigma^2$ is the noise variance.\n",
    "\n",
    "- **Inference:** Our goal is to compute the posterior distribution over functions given the data:\n",
    "    $$\n",
    "    p(f \\mid y)\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Object Layer\n",
    "\n",
    "**How do we represent the model in code?**\n",
    "\n",
    "We can encapsulate the GP model using Python classes. Below is a minimal, modular implementation using JAX for efficient computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81478687",
   "metadata": {},
   "source": [
    "## How Do We Actually Solve Linear Systems of Equations? The LU Decomposition\n",
    "\n",
    "Solving a linear system $A\\mathbf{x} = \\mathbf{b}$ is a fundamental operation in numerical linear algebra. One of the most widely used methods for this is the **LU Decomposition**.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is LU Decomposition?**\n",
    "\n",
    "- **LU Decomposition** expresses any square matrix $A$ as the product of a **lower triangular matrix** $L$ and an **upper triangular matrix** $U$:\n",
    "    $$\n",
    "    A = LU\n",
    "    $$\n",
    "- This factorization allows us to solve linear systems efficiently by breaking the problem into simpler steps.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Does the Decomposition Work?**\n",
    "\n",
    "The process involves recursively partitioning the matrix $A$:\n",
    "\n",
    "- At each step $i$, we write:\n",
    "    $$\n",
    "    A^{(i)} = \\begin{pmatrix}\n",
    "        \\alpha^{(i)} & (u^{(i)})^\\top \\\\\n",
    "        b^{(i)} & B^{(i)}\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "    where:\n",
    "    - $\\alpha^{(i)}$ is the pivot (diagonal element)\n",
    "    - $u^{(i)}$ is the row vector above the diagonal\n",
    "    - $b^{(i)}$ is the column vector below the diagonal\n",
    "    - $B^{(i)}$ is the remaining submatrix\n",
    "\n",
    "- The corresponding $L$ and $U$ blocks are:\n",
    "    $$\n",
    "    L^{(i)} = \\begin{pmatrix}\n",
    "        1 & 0 \\\\\n",
    "        l^{(i)} & L^{(i+1)}\n",
    "    \\end{pmatrix}, \\quad\n",
    "    U^{(i)} = \\begin{pmatrix}\n",
    "        \\alpha^{(i)} & (u^{(i)})^\\top \\\\\n",
    "        0 & U^{(i+1)}\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "\n",
    "- The recursion is defined by:\n",
    "    $$\n",
    "    l^{(i)} = \\frac{1}{\\alpha^{(i)}} b^{(i)}\n",
    "    $$\n",
    "    $$\n",
    "    A^{(i+1)} := L^{(i+1)} U^{(i+1)} = B^{(i)} - l^{(i)} (u^{(i)})^\\top\n",
    "    $$\n",
    "\n",
    "- If all pivots $\\alpha^{(i)}$ are non-zero, the recursion terminates and an LU decomposition exists.\n",
    "\n",
    "- **Pivoting** (reordering rows/columns) is often used in practice for numerical stability, typically by choosing the largest absolute value in the current column as the pivot.\n",
    "\n",
    "---\n",
    "\n",
    "### **Computational Complexity**\n",
    "\n",
    "- **LU decomposition** of an $N \\times N$ matrix requires approximately $\\frac{2}{3}N^3$ floating-point operations (flops).\n",
    "- This is the dominant cost when solving a single linear system.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solving Linear Systems with LU Decomposition**\n",
    "\n",
    "Once $A = LU$ is known, solving $A\\mathbf{x} = \\mathbf{b}$ becomes much more efficient:\n",
    "\n",
    "1. **Forward Substitution:**  \n",
    "     Solve $L\\mathbf{y} = \\mathbf{b}$ for $\\mathbf{y}$ (since $L$ is lower triangular).  \n",
    "     - Cost: $\\mathcal{O}(N^2)$ flops.\n",
    "\n",
    "2. **Backward Substitution:**  \n",
    "     Solve $U\\mathbf{x} = \\mathbf{y}$ for $\\mathbf{x}$ (since $U$ is upper triangular).  \n",
    "     - Cost: $\\mathcal{O}(N^2)$ flops.\n",
    "\n",
    "**Total cost:**  \n",
    "- $\\frac{2}{3}N^3$ (for decomposition) $+$ $2N^2$ (for both substitutions).\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is LU Decomposition Useful?**\n",
    "\n",
    "- If you need to solve multiple systems with the same $A$ but different $\\mathbf{b}$, you only compute the LU decomposition once. Each subsequent solve then costs only $2N^2$ flops.\n",
    "- This is much more efficient than inverting $A$ directly or recomputing the decomposition for each new right-hand side.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**\n",
    "\n",
    "| Step                      | Operation                | Complexity      |\n",
    "|---------------------------|--------------------------|----------------|\n",
    "| LU Decomposition          | $A = LU$                 | $\\frac{2}{3}N^3$ flops |\n",
    "| Forward Substitution      | $L\\mathbf{y} = \\mathbf{b}$ | $N^2$ flops    |\n",
    "| Backward Substitution     | $U\\mathbf{x} = \\mathbf{y}$ | $N^2$ flops    |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "- LU decomposition is a foundational tool for efficiently solving linear systems.\n",
    "- It is especially advantageous when solving for multiple right-hand sides.\n",
    "- Understanding its computational cost is crucial for scaling up to large problems, such as those encountered in Gaussian Process regression and other machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4637830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np  # For creating a simple matrix example\n",
    "\n",
    "\n",
    "def manual_lu_decomposition(A: jnp.ndarray) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Performs LU decomposition manually for a square matrix (without pivoting for simplicity).\n",
    "\n",
    "    Args:\n",
    "        A: A square JAX array of shape (N, N).\n",
    "\n",
    "    Returns:\n",
    "        A tuple (L, U) where L is lower triangular and U is upper triangular,\n",
    "        such that L @ U = A.\n",
    "    \"\"\"\n",
    "    N = A.shape[0]\n",
    "    L = jnp.eye(N, dtype=A.dtype)\n",
    "    U = jnp.copy(A)\n",
    "\n",
    "    for i in range(N):\n",
    "        # Check for zero pivot (simplified, real implementations use pivoting)\n",
    "        if U[i, i] == 0:\n",
    "            print(\n",
    "                f\"Warning: Zero pivot encountered at step {i}. LU decomposition may fail or be unstable.\"\n",
    "            )\n",
    "            # In a real scenario, this would require pivoting or a different decomposition.\n",
    "            return L, U  # Return current state\n",
    "\n",
    "        # Calculate multipliers for the current column\n",
    "        # L[j, i] = U[j, i] / U[i, i] for j > i\n",
    "        for j in range(i + 1, N):\n",
    "            multiplier = U[j, i] / U[i, i]\n",
    "            L = L.at[j, i].set(multiplier)\n",
    "            # Perform row operation on U: U[j, :] = U[j, :] - multiplier * U[i, :]\n",
    "            U = U.at[j, :].set(U[j, :] - multiplier * U[i, :])\n",
    "    return L, U\n",
    "\n",
    "\n",
    "def forward_substitution(L: jnp.ndarray, b: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Solves Ly = b for y where L is a lower triangular matrix.\"\"\"\n",
    "    N = L.shape[0]\n",
    "    y = jnp.zeros(N, dtype=b.dtype)\n",
    "    for i in range(N):\n",
    "        y = y.at[i].set((b[i] - jnp.dot(L[i, :i], y[:i])) / L[i, i])\n",
    "    return y\n",
    "\n",
    "\n",
    "def backward_substitution(U: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Solves Ux = y for x where U is an upper triangular matrix.\"\"\"\n",
    "    N = U.shape[0]\n",
    "    x = jnp.zeros(N, dtype=y.dtype)\n",
    "    for i in range(N - 1, -1, -1):\n",
    "        x = x.at[i].set((y[i] - jnp.dot(U[i, i + 1 :], x[i + 1 :])) / U[i, i])\n",
    "    return x\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Create a sample matrix A\n",
    "np.random.seed(1)\n",
    "A_np = np.random.rand(4, 4)\n",
    "A = jnp.asarray(A_np)\n",
    "\n",
    "print(\"Original Matrix A:\\n\", A)\n",
    "\n",
    "# Perform manual LU decomposition\n",
    "L_manual, U_manual = manual_lu_decomposition(A)\n",
    "print(\"\\nManual L:\\n\", L_manual)\n",
    "print(\"\\nManual U:\\n\", U_manual)\n",
    "\n",
    "# Verify decomposition: L @ U should be close to A\n",
    "A_reconstructed = L_manual @ U_manual\n",
    "print(\"\\nManual Reconstruction (L @ U):\\n\", A_reconstructed)\n",
    "print(\n",
    "    f\"\\nMax absolute difference (Manual LU): {jnp.max(jnp.abs(A - A_reconstructed)):.2e}\"\n",
    ")\n",
    "\n",
    "# Solve a linear system A @ x = b\n",
    "b_np = np.array([10.0, 12.0, 14.0, 16.0])\n",
    "b = jnp.asarray(b_np)\n",
    "\n",
    "print(\"\\nRight-hand side b:\\n\", b)\n",
    "\n",
    "# Solve Ly = b using forward substitution\n",
    "y_solved = forward_substitution(L_manual, b)\n",
    "print(\"\\nSolved y (from Ly=b):\\n\", y_solved)\n",
    "\n",
    "# Solve Ux = y using backward substitution\n",
    "x_solved = backward_substitution(U_manual, y_solved)\n",
    "print(\"\\nSolved x (from Ux=y):\\n\", x_solved)\n",
    "\n",
    "# Verify the solution with direct solve (for comparison)\n",
    "x_direct = jnp.linalg.solve(A, b)\n",
    "print(\"\\nDirectly solved x (from A@x=b):\\n\", x_direct)\n",
    "\n",
    "print(\n",
    "    f\"\\nMax absolute difference (Manual vs Direct solve): {jnp.max(jnp.abs(x_solved - x_direct)):.2e}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820e4d8",
   "metadata": {},
   "source": [
    "## The Cholesky Decomposition: For Symmetric Positive Definite Matrices\n",
    "\n",
    "When working with Gaussian Processes, the kernel matrix $K_{XX}$ (and thus $K_{XX} + \\sigma^2 I_N$) is always **symmetric positive definite**. For such matrices, we can use a more efficient and numerically stable factorization: the **Cholesky decomposition**.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is Cholesky Decomposition?**\n",
    "\n",
    "- If $A \\in \\mathbb{R}^{N \\times N}$ is symmetric positive definite, it can be **uniquely** decomposed as:\n",
    "    $$\n",
    "    A = LL^\\top\n",
    "    $$\n",
    "    where:\n",
    "    - $L$ is a lower triangular matrix with positive diagonal entries,\n",
    "    - $L^\\top$ is its transpose.\n",
    "\n",
    "---\n",
    "\n",
    "### **Recursive Partitioning for Cholesky Decomposition**\n",
    "\n",
    "At each step, we partition $A$ and $L$ as follows:\n",
    "$$\n",
    "A^{(i)} = \\begin{pmatrix}\n",
    "        \\alpha^{(i)} & (b^{(i)})^\\top \\\\\n",
    "        b^{(i)} & B^{(i)}\n",
    "\\end{pmatrix}, \\quad\n",
    "L^{(i)} = \\begin{pmatrix}\n",
    "        \\lambda^{(i)} & 0 \\\\\n",
    "        l^{(i)} & L^{(i+1)}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "such that:\n",
    "$$\n",
    "A^{(i)} = L^{(i)} (L^{(i)})^\\top\n",
    "$$\n",
    "\n",
    "This leads to the following update rules:\n",
    "- $\\lambda^{(i)} = \\sqrt{\\alpha^{(i)}}$  \n",
    "    (since $\\alpha^{(i)} > 0$ by positive definiteness)\n",
    "- $l^{(i)} = \\frac{1}{\\lambda^{(i)}} b^{(i)}$\n",
    "- $A^{(i+1)} := B^{(i)} - l^{(i)} (l^{(i)})^\\top$\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use Cholesky?**\n",
    "\n",
    "- **Efficiency:**  \n",
    "    The Cholesky decomposition requires approximately $\\frac{1}{3}N^3$ floating-point operations (flops), which is **about half the cost** of LU decomposition for general matrices.\n",
    "- **Numerical Stability:**  \n",
    "    It is more stable and less error-prone for symmetric positive definite matrices, which are common in GP regression and many other ML applications.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**\n",
    "\n",
    "| Step                      | Operation                | Complexity      |\n",
    "|---------------------------|--------------------------|----------------|\n",
    "| Cholesky Decomposition    | $A = LL^\\top$            | $\\frac{1}{3}N^3$ flops |\n",
    "| Forward/Backward Substitution | Solve $L\\mathbf{y} = \\mathbf{b}$, $L^\\top\\mathbf{x} = \\mathbf{y}$ | $N^2$ flops each |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "- Cholesky decomposition is the **preferred method** for solving linear systems involving symmetric positive definite matrices.\n",
    "- It is especially important in Gaussian Processes, where the kernel matrix is always symmetric positive definite.\n",
    "- Understanding and using Cholesky decomposition allows for faster, more stable computations in probabilistic machine learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import cholesky as jax_cholesky  # JAX's optimized Cholesky\n",
    "import numpy as np  # For creating a simple SPD matrix example\n",
    "\n",
    "\n",
    "def manual_cholesky_decomposition(A: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Performs Cholesky decomposition manually for a symmetric positive definite matrix.\n",
    "\n",
    "    Args:\n",
    "        A: A symmetric positive definite JAX array of shape (N, N).\n",
    "\n",
    "    Returns:\n",
    "        A lower triangular JAX array L such that L @ L.T = A.\n",
    "    \"\"\"\n",
    "    N = A.shape[0]\n",
    "    L = jnp.zeros((N, N), dtype=A.dtype)\n",
    "\n",
    "    for i in range(N):\n",
    "        # Calculate L[i, i]\n",
    "        # L[i, i] = sqrt(A[i, i] - sum(L[i, k]^2 for k from 0 to i-1))\n",
    "        sum_sq_prev_elements = jnp.sum(L[i, :i] ** 2)\n",
    "        L = L.at[i, i].set(jnp.sqrt(A[i, i] - sum_sq_prev_elements))\n",
    "\n",
    "        # Calculate L[j, i] for j > i\n",
    "        # L[j, i] = (A[j, i] - sum(L[j, k] * L[i, k] for k from 0 to i-1)) / L[i, i]\n",
    "        for j in range(i + 1, N):\n",
    "            sum_prod_prev_elements = jnp.sum(L[j, :i] * L[i, :i])\n",
    "            L = L.at[j, i].set((A[j, i] - sum_prod_prev_elements) / L[i, i])\n",
    "    return L\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Create a symmetric positive definite matrix\n",
    "# A simple way to get an SPD matrix is A = B @ B.T for some matrix B\n",
    "np.random.seed(0)\n",
    "B = np.random.rand(5, 5)  # A 5x5 random matrix\n",
    "A_np = B @ B.T + np.eye(5) * 1e-6  # Add small identity for strict positive definiteness\n",
    "A = jnp.asarray(A_np)\n",
    "\n",
    "print(\"Original Matrix A:\\n\", A)\n",
    "\n",
    "# Perform manual Cholesky decomposition\n",
    "L_manual = manual_cholesky_decomposition(A)\n",
    "print(\"\\nManual Cholesky L:\\n\", L_manual)\n",
    "\n",
    "# Verify the decomposition: L @ L.T should be close to A\n",
    "A_reconstructed_manual = L_manual @ L_manual.T\n",
    "print(\"\\nManual Reconstruction (L @ L.T):\\n\", A_reconstructed_manual)\n",
    "print(\n",
    "    f\"\\nMax absolute difference (Manual): {jnp.max(jnp.abs(A - A_reconstructed_manual)):.2e}\"\n",
    ")\n",
    "\n",
    "# Compare with JAX's built-in Cholesky (for verification)\n",
    "L_jax = jax_cholesky(A, lower=True)\n",
    "print(\"\\nJAX built-in Cholesky L:\\n\", L_jax)\n",
    "\n",
    "# Verify JAX's decomposition\n",
    "A_reconstructed_jax = L_jax @ L_jax.T\n",
    "print(\n",
    "    f\"\\nMax absolute difference (JAX built-in): {jnp.max(jnp.abs(A - A_reconstructed_jax)):.2e}\"\n",
    ")\n",
    "\n",
    "# Note that iteration i of this process is O((N-i)^2). The first step has to “touch” all N data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c099f",
   "metadata": {},
   "source": [
    "# GP Regression: Computational Summary\n",
    "\n",
    "Let's summarize the key computational insights from our exploration of Gaussian Process (GP) regression:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **GP Regression = Matrix Decomposition**\n",
    "\n",
    "- **Training a GP** (i.e., fitting to data) fundamentally reduces to decomposing a matrix.\n",
    "- Specifically, we work with the **kernel matrix**:  \n",
    "    $$\n",
    "    K_{XX} + \\sigma^2 I_N\n",
    "    $$\n",
    "    where $K_{XX}$ is the $N \\times N$ covariance matrix and $\\sigma^2$ is the noise variance.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Cholesky Decomposition: The Workhorse**\n",
    "\n",
    "- For **symmetric positive definite matrices** (which kernel matrices always are), the **Cholesky decomposition** is the gold standard:\n",
    "    $$\n",
    "    K_{XX} + \\sigma^2 I_N = LL^\\top\n",
    "    $$\n",
    "    where $L$ is lower triangular.\n",
    "- **Why Cholesky?**\n",
    "    - **Numerically stable**\n",
    "    - **Efficient**: About half the cost of LU decomposition for general matrices\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Efficient Posterior Computation**\n",
    "\n",
    "- Once the Cholesky factor $L$ is available:\n",
    "    - **Posterior mean** and **posterior covariance** can be computed efficiently.\n",
    "    - Each prediction (for a new test point) requires only $\\mathcal{O}(N^2)$ operations.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **The Real Bottleneck: Cubic Complexity**\n",
    "\n",
    "- **Cholesky decomposition itself is $\\mathcal{O}(N^3)$** in time and $\\mathcal{O}(N^2)$ in memory.\n",
    "- This is the main computational bottleneck for GPs, especially as $N$ grows large (thousands or millions of data points).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Not Just \"Being Bayesian\"**\n",
    "\n",
    "- The $\\mathcal{O}(N^3)$ cost is **not** due to computing the full posterior covariance (i.e., \"being Bayesian\").\n",
    "- **Even if you only want the point estimate** (the posterior mean), you must solve a linear system involving the kernel matrix, which still requires an $\\mathcal{O}(N^3)$ decomposition.\n",
    "- **Key point:**  \n",
    "    - The cost comes from the need to solve for the \"representer weights\" in the kernel expansion, not from uncertainty quantification per se.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Why Are GPs So Much More Expensive Than Deep Learning?**\n",
    "\n",
    "- **Deep learning** (with stochastic gradient descent) often scales as $\\mathcal{O}(N)$ (or even $\\mathcal{O}(1)$ per mini-batch iteration).\n",
    "- **GPs and kernel machines** scale as $\\mathcal{O}(N^3)$.\n",
    "- **Why?**\n",
    "    - GPs require global operations (matrix decompositions) that touch all $N$ data points at once.\n",
    "    - Deep learning leverages local, incremental updates (mini-batches) and does not require global matrix operations.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary Table: Computational Complexity**\n",
    "\n",
    "| Operation                        | Complexity         |\n",
    "|-----------------------------------|-------------------|\n",
    "| Kernel matrix construction        | $\\mathcal{O}(N^2)$|\n",
    "| Cholesky decomposition            | $\\mathcal{O}(N^3)$|\n",
    "| Posterior mean/covariance (per test point) | $\\mathcal{O}(N^2)$|\n",
    "| Deep learning (per mini-batch)    | $\\mathcal{O}(1)$ or $\\mathcal{O}(N)$|\n",
    "\n",
    "---\n",
    "\n",
    "> **Bottom line:**  \n",
    "> The main computational challenge in GP regression is the $\\mathcal{O}(N^3)$ scaling of matrix decomposition. This motivates the development of scalable approximations and specialized algorithms for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f0684",
   "metadata": {},
   "source": [
    "# Grass is Greener on the Deep Side? Deep Learning Scalability\n",
    "\n",
    "To understand why deep learning scales so well compared to Gaussian Processes (GPs), let's briefly examine how deep learning models are trained and contrast this with GP inference.\n",
    "\n",
    "---\n",
    "\n",
    "## Deep Learning: Empirical Risk Minimization and SGD\n",
    "\n",
    "Training a deep neural network is typically formulated as an **empirical risk minimization (ERM)** problem. The goal is to find weights $\\mathbf{w}$ that minimize a loss function $L(\\mathbf{w})$:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^* = \\arg\\min_{\\mathbf{w} \\in \\mathbb{R}^D} L(\\mathbf{w}) = \\arg\\min_{\\mathbf{w} \\in \\mathbb{R}^D} \\sum_{i=1}^N \\ell(y_i, f(\\mathbf{w}, x_i)) + r(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "- $\\ell(y_i, f(\\mathbf{w}, x_i))$: Loss for data point $i$ (e.g., squared error, cross-entropy)\n",
    "- $r(\\mathbf{w})$: Regularization term (e.g., weight decay)\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "This minimization is usually performed using **stochastic, first-order methods** such as SGD or Adam. These methods use **mini-batches** of data to estimate the gradient:\n",
    "\n",
    "$$\n",
    "\\nabla L(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^N \\nabla \\ell(y_i, f(\\mathbf{w}, x_i)) + \\nabla r(\\mathbf{w}) \\approx \\frac{1}{B} \\sum_{j=1}^B \\nabla \\ell(y_{i(j)}, f(\\mathbf{w}, x_{i(j)})) + \\nabla r(\\mathbf{w}) =: g(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "- $B$: Mini-batch size ($B \\ll N$)\n",
    "- $g(\\mathbf{w})$: Stochastic gradient estimate\n",
    "\n",
    "**Key Point:**  \n",
    "- Each iteration's computational cost is $\\mathcal{O}(B)$, independent of the total dataset size $N$.\n",
    "- This allows deep learning to scale to massive datasets.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import optax  # A JAX-based optimization library\n",
    "from jax import grad\n",
    "\n",
    "# --- Conceptual Deep Learning Training Loop ---\n",
    "\n",
    "# 1. Simulate a very simple dataset\n",
    "num_total_samples = 100000  # N: Large dataset\n",
    "input_dim = 10\n",
    "output_dim = 1\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "# Dummy data (inputs and targets)\n",
    "X_data = random.normal(key, (num_total_samples, input_dim))\n",
    "y_data = random.normal(key, (num_total_samples, output_dim))\n",
    "\n",
    "\n",
    "# 2. Define a very simple \"deep learning\" model (e.g., a linear model for simplicity)\n",
    "# In reality, this would be a multi-layer neural network\n",
    "def simple_model(params, x):\n",
    "    \"\"\"A simple linear model: y = x @ W + b\"\"\"\n",
    "    return jnp.dot(x, params[\"W\"]) + params[\"b\"]\n",
    "\n",
    "\n",
    "# 3. Define a simple loss function (Mean Squared Error)\n",
    "def mse_loss(params, x_batch, y_batch):\n",
    "    predictions = simple_model(params, x_batch)\n",
    "    return jnp.mean((predictions - y_batch) ** 2)\n",
    "\n",
    "\n",
    "# 4. Initialize model parameters\n",
    "model_params = {\n",
    "    \"W\": random.normal(key, (input_dim, output_dim)),\n",
    "    \"b\": jnp.zeros(output_dim),\n",
    "}\n",
    "\n",
    "# 5. Setup optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(model_params)\n",
    "\n",
    "# Get the gradient function\n",
    "loss_grad_fn = grad(mse_loss)\n",
    "\n",
    "# --- Training Loop (Illustrating O(B) cost per iteration) ---\n",
    "num_training_steps = 1000\n",
    "batch_size = 64  # B: Mini-batch size (fixed, much smaller than N)\n",
    "\n",
    "print(f\"Simulating Deep Learning Training (N={num_total_samples}, B={batch_size})\")\n",
    "print(\"Cost per iteration is O(B), independent of N.\")\n",
    "\n",
    "for step in range(num_training_steps):\n",
    "    # Randomly select a mini-batch\n",
    "    batch_indices = random.randint(key, (batch_size,), 0, num_total_samples)\n",
    "    x_batch = X_data[batch_indices]\n",
    "    y_batch = y_data[batch_indices]\n",
    "\n",
    "    # Compute loss and gradients for the mini-batch\n",
    "    # This operation's cost depends on B, not N\n",
    "    grads = loss_grad_fn(model_params, x_batch, y_batch)\n",
    "\n",
    "    # Update model parameters\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model_params)\n",
    "    model_params = optax.apply_updates(model_params, updates)\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        current_loss = mse_loss(model_params, x_batch, y_batch)\n",
    "        print(f\"Step {step}, Batch Loss: {current_loss:.4f}\")\n",
    "\n",
    "print(\"\\nDeep Learning training simulation complete.\")\n",
    "print(\n",
    "    \"The key takeaway is that each step's computation depends on batch_size, not total_samples.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7df88ec",
   "metadata": {},
   "source": [
    "\n",
    "## Can GPs Be Trained with SGD? Re-phrasing Inference as Optimization\n",
    "\n",
    "Given the scalability of SGD, a natural question arises:\n",
    "\n",
    "> **Can we train GPs using SGD-like methods?**\n",
    "\n",
    "### Step 1: GP Inference as Optimization\n",
    "\n",
    "The **posterior mean** $\\mu_y(X)$ at the training data $X$ is the mode of $p(f_X \\mid y)$. We can find this mode by minimizing the negative log-posterior:\n",
    "\n",
    "$$\n",
    "\\mu_y(X) = \\arg\\max_{f_X \\in \\mathbb{R}^N} \\log p(f_X \\mid y) = \\arg\\max_{f_X \\in \\mathbb{R}^N} \\log p(y \\mid f_X) + \\log p(f_X)\n",
    "$$\n",
    "\n",
    "Or, equivalently:\n",
    "\n",
    "$$\n",
    "\\mu_y(X) = \\arg\\min_{f_X \\in \\mathbb{R}^N} -\\log p(y \\mid f_X) - \\log p(f_X)\n",
    "$$\n",
    "\n",
    "For a **Gaussian likelihood** and a **GP prior** (with zero mean for simplicity):\n",
    "\n",
    "$$\n",
    "\\mu_y(X) = \\arg\\min_{f_X \\in \\mathbb{R}^N} \\frac{1}{2\\sigma^2} \\sum_{i=1}^N |y_i - (f_X)_i|^2 + \\frac{1}{2} (f_X - \\mu_X)^\\top K_{XX}^{-1} (f_X - \\mu_X) + \\text{const.}\n",
    "$$\n",
    "\n",
    "- The first term is the **data fidelity** (likelihood).\n",
    "- The second term is the **regularization** (prior).\n",
    "\n",
    "This is a **convex optimization problem**.\n",
    "\n",
    "**Note:**  \n",
    "Because of the conditional independence $f(\\cdot) \\perp\\!\\!\\!\\perp y \\mid f(X)$, it suffices to find $\\mu_y(X)$; predictions at new points depend only on $f(X)$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from jax import grad, jit\n",
    "from typing import Callable\n",
    "import optax  # A JAX-based optimization library\n",
    "\n",
    "\n",
    "# --- Re-using kernel definition ---\n",
    "def squared_exponential_kernel(\n",
    "    x1: jnp.ndarray, x2: jnp.ndarray, sigma: float = 1.0, lengthscale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Computes the Squared Exponential (RBF) kernel matrix.\"\"\"\n",
    "    x1 = jnp.atleast_2d(x1)\n",
    "    x2 = jnp.atleast_2d(x2)\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    K = sigma**2 * jnp.exp(-0.5 * sq_dist / lengthscale**2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# --- Optimization Problem for f_X ---\n",
    "# We want to minimize:\n",
    "# L(f_X) = (1 / (2 * sigma_noise^2)) * ||y - f_X||^2 + (1/2) * (f_X - mu_X)^T @ K_XX_inv @ (f_X - mu_X)\n",
    "\n",
    "\n",
    "@jit\n",
    "def gp_posterior_loss(\n",
    "    f_X: jnp.ndarray,  # The function values at training points (our \"parameters\" to optimize)\n",
    "    y_train: jnp.ndarray,\n",
    "    X_train: jnp.ndarray,\n",
    "    mean_func: Callable[[jnp.ndarray], jnp.ndarray],\n",
    "    kernel_func: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray],\n",
    "    noise_variance: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the negative log-posterior of f_X given y_train.\n",
    "    This is the loss function we would minimize to find the posterior mode of f_X.\n",
    "    Note: This direct implementation requires K_XX_inv, which is O(N^3) to compute.\n",
    "    \"\"\"\n",
    "    mu_X = mean_func(X_train)\n",
    "    K_XX = kernel_func(X_train, X_train)\n",
    "\n",
    "    # Add a small jitter for numerical stability if K_XX is nearly singular\n",
    "    jitter = 1e-6 * jnp.eye(X_train.shape[0])\n",
    "    K_XX_stable = K_XX + jitter\n",
    "\n",
    "    # Compute K_XX_inv (O(N^3) operation)\n",
    "    K_XX_inv = jnp.linalg.inv(K_XX_stable)\n",
    "\n",
    "    # Likelihood term\n",
    "    likelihood_term = jnp.sum((y_train - f_X) ** 2) / (2.0 * noise_variance)\n",
    "\n",
    "    # Prior term\n",
    "    prior_diff = f_X - mu_X\n",
    "    prior_term = 0.5 * jnp.dot(prior_diff.T, jnp.dot(K_XX_inv, prior_diff))\n",
    "\n",
    "    return likelihood_term + prior_term\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "key = random.PRNGKey(789)\n",
    "N_train = 50  # Number of training points\n",
    "X_train_opt = jnp.linspace(-5, 5, N_train)[:, None]\n",
    "y_train_opt = jnp.sin(X_train_opt).squeeze() + 0.2 * random.normal(key, (N_train,))\n",
    "\n",
    "zero_mean_opt = lambda x: jnp.zeros(x.shape[0])\n",
    "rbf_kernel_opt = lambda x1, x2: squared_exponential_kernel(\n",
    "    x1, x2, sigma=1.0, lengthscale=1.0\n",
    ")\n",
    "noise_var_opt = 0.1**2\n",
    "\n",
    "# Initialize f_X (our \"parameters\" for optimization)\n",
    "# Start with the prior mean as an initial guess\n",
    "initial_f_X = zero_mean_opt(X_train_opt)\n",
    "\n",
    "# Setup optimizer for f_X\n",
    "learning_rate_fX = 0.1\n",
    "optimizer_fX = optax.adam(learning_rate_fX)\n",
    "opt_state_fX = optimizer_fX.init(initial_f_X)\n",
    "\n",
    "# Get the gradient function for our loss\n",
    "loss_grad_fn_fX = grad(gp_posterior_loss)\n",
    "\n",
    "# Optimization loop\n",
    "num_opt_steps = 1000\n",
    "f_X_current = initial_f_X\n",
    "losses_fX = []\n",
    "\n",
    "print(f\"Optimizing f_X (N={N_train}) to find posterior mode.\")\n",
    "for step in range(num_opt_steps):\n",
    "    loss_value = gp_posterior_loss(\n",
    "        f_X_current,\n",
    "        y_train_opt,\n",
    "        X_train_opt,\n",
    "        zero_mean_opt,\n",
    "        rbf_kernel_opt,\n",
    "        noise_var_opt,\n",
    "    )\n",
    "    losses_fX.append(loss_value)\n",
    "\n",
    "    grads = loss_grad_fn_fX(\n",
    "        f_X_current,\n",
    "        y_train_opt,\n",
    "        X_train_opt,\n",
    "        zero_mean_opt,\n",
    "        rbf_kernel_opt,\n",
    "        noise_var_opt,\n",
    "    )\n",
    "    updates, opt_state_fX = optimizer_fX.update(grads, opt_state_fX, f_X_current)\n",
    "    f_X_current = optax.apply_updates(f_X_current, updates)\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_value:.4f}\")\n",
    "\n",
    "print(\n",
    "    f\"Final Loss: {gp_posterior_loss(f_X_current, y_train_opt, X_train_opt, zero_mean_opt, rbf_kernel_opt, noise_var_opt):.4f}\"\n",
    ")\n",
    "print(\"\\nOptimized f_X (posterior mode at training points):\\n\", f_X_current[:5])\n",
    "\n",
    "# Plot the optimization loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(losses_fX)\n",
    "plt.xlabel(\"Optimization Step\")\n",
    "plt.ylabel(\"Negative Log-Posterior of f_X\")\n",
    "plt.title(\"Optimization of f_X to find Posterior Mode\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compare optimized f_X with analytical posterior mean (from gp_predict)\n",
    "# Need to define gp_predict function here or import it\n",
    "def gp_predict(X_train, y_train, X_test, mean_func, kernel_func, noise_variance):\n",
    "    K_train_train = kernel_func(X_train, X_train) + noise_variance * jnp.eye(\n",
    "        X_train.shape[0]\n",
    "    )\n",
    "    K_test_train = kernel_func(X_test, X_train)\n",
    "    mu_pred = mean_func(X_test) + jnp.dot(\n",
    "        K_test_train, jnp.linalg.solve(K_train_train, y_train - mean_func(X_train))\n",
    "    )\n",
    "    # For this comparison, we only need the mean, not full covariance\n",
    "    return mu_pred, None  # Return None for Sigma_pred\n",
    "\n",
    "\n",
    "analytical_mu_pred_at_train, _ = gp_predict(\n",
    "    X_train_opt, y_train_opt, X_train_opt, zero_mean_opt, rbf_kernel_opt, noise_var_opt\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nMax absolute difference between optimized f_X and analytical posterior mean: {jnp.max(jnp.abs(f_X_current - analytical_mu_pred_at_train)):.2e}\"\n",
    ")\n",
    "print(\n",
    "    \"This shows that optimizing the negative log-posterior of f_X indeed finds the analytical posterior mean.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e9fd0",
   "metadata": {},
   "source": [
    "\n",
    "## The Problem: The Cost of Being Nonparametric\n",
    "\n",
    "The core issue is that the \"weights\" in this optimization are the function values at the training points, $f_X$. This means:\n",
    "\n",
    "- The number of parameters **grows with $N$** (the dataset size).\n",
    "- Each gradient evaluation involves $N$ terms, so even with mini-batching, the cost per gradient step is at least $\\mathcal{O}(N)$.\n",
    "\n",
    "This is **not** due to being Bayesian or probabilistic—it's an inherent property of **nonparametric models** (sometimes called \"infinitely-wide neural networks\").\n",
    "\n",
    "---\n",
    "\n",
    "### Possible Solutions (Beyond This Lecture)\n",
    "\n",
    "- **Return to a parametric representation:** $f(x) = \\phi(x)^\\top \\mathbf{w}$\n",
    "- **Finite approximations:** Use sparse GP regression, inducing points, random features, or spectral methods to reduce the effective number of parameters.\n",
    "\n",
    "For now, we accept that we must deal with $N$ parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## Computing Gradients for GP Optimization\n",
    "\n",
    "Recall the optimization problem for $\\mu_y(X)$:\n",
    "\n",
    "$$\n",
    "\\mu_y(X) = \\arg\\min_{f_X \\in \\mathbb{R}^N} \\frac{1}{2\\sigma^2} \\sum_{i=1}^N |y_i - (f_X)_i|^2 + \\frac{1}{2} (f_X - \\mu_X)^\\top K_{XX}^{-1} (f_X - \\mu_X)\n",
    "$$\n",
    "\n",
    "We could implement this loss directly and compute its gradient, but this still requires $K_{XX}^{-1}$, which is $\\mathcal{O}(N^3)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Analytical Solution for the Posterior Mean\n",
    "\n",
    "We already know the analytical solution:\n",
    "\n",
    "$$\n",
    "\\mu_y(\\cdot) = \\mu(\\cdot) + k_{\\cdot X} (K_{XX} + \\sigma^2 I_N)^{-1} (y - \\mu_X) = \\mu(\\cdot) + k_{\\cdot X} \\alpha\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\alpha = (K_{XX} + \\sigma^2 I_N)^{-1} (y - \\mu_X)\n",
    "$$\n",
    "\n",
    "$\\alpha$ minimizes the quadratic function:\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\frac{1}{2} \\alpha^\\top (K_{XX} + \\sigma^2 I_N) \\alpha - (y - \\mu_X)^\\top \\alpha + \\text{const.}\n",
    "$$\n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\alpha L(\\alpha) = (K_{XX} + \\sigma^2 I_N) \\alpha - (y - \\mu_X)\n",
    "$$\n",
    "\n",
    "Setting this to zero gives the solution for $\\alpha$.\n",
    "\n",
    "- **Computing the gradient:** $\\mathcal{O}(N^2)$ (matrix-vector product)\n",
    "- **Solving for $\\alpha$:** $\\mathcal{O}(N^3)$ (due to matrix decomposition)\n",
    "\n",
    "Modern frameworks like JAX can compute these gradients efficiently, but the underlying matrix operations remain the computational bottleneck.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary Table: Deep Learning vs. GP Regression**\n",
    "\n",
    "| Operation                        | Deep Learning (SGD) | GP Regression         |\n",
    "|-----------------------------------|---------------------|----------------------|\n",
    "| Per-iteration cost                | $\\mathcal{O}(B)$    | $\\mathcal{O}(N)$     |\n",
    "| Parameter count                   | Fixed ($D$)         | Grows with $N$       |\n",
    "| Matrix inversion/decomposition    | Rare                | Required ($\\mathcal{O}(N^3)$) |\n",
    "\n",
    "---\n",
    "\n",
    "> **Bottom line:**  \n",
    "> Deep learning scales well because its per-iteration cost is independent of dataset size, thanks to mini-batching and fixed parameter count.  \n",
    "> GP regression, being nonparametric, fundamentally requires global operations that scale poorly with $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit\n",
    "from jax.scipy.linalg import solve\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "# --- Re-using kernel definition ---\n",
    "def squared_exponential_kernel(\n",
    "    x1: jnp.ndarray, x2: jnp.ndarray, sigma: float = 1.0, lengthscale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Computes the Squared Exponential (RBF) kernel matrix.\"\"\"\n",
    "    x1 = jnp.atleast_2d(x1)\n",
    "    x2 = jnp.atleast_2d(x2)\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    K = sigma**2 * jnp.exp(-0.5 * sq_dist / lengthscale**2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# --- Define the quadratic loss function for alpha ---\n",
    "# L(alpha) = 0.5 * alpha^T @ (K_XX + sigma^2 I) @ alpha - (y - mu_X)^T @ alpha + const.\n",
    "@jit\n",
    "def alpha_loss_function(\n",
    "    alpha: jnp.ndarray,\n",
    "    y_diff: jnp.ndarray,  # (y - mu_X)\n",
    "    K_XX_noisy: jnp.ndarray,  # (K_XX + sigma^2 I)\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the quadratic loss function for alpha.\n",
    "    Minimizing this loss gives the representer weights.\n",
    "    \"\"\"\n",
    "    term1 = 0.5 * jnp.dot(alpha.T, jnp.dot(K_XX_noisy, alpha))\n",
    "    term2 = jnp.dot(y_diff.T, alpha)\n",
    "    return term1 - term2\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Use dummy data from previous examples\n",
    "N_example = 5\n",
    "X_train_ex = jnp.linspace(0, 10, N_example)[:, None]\n",
    "y_train_ex = jnp.sin(X_train_ex).squeeze() + 0.1 * jnp.array(\n",
    "    [0.5, -0.2, 0.1, -0.3, 0.4]\n",
    ")\n",
    "zero_mean_ex = lambda x: jnp.zeros(x.shape[0])\n",
    "rbf_kernel_ex = lambda x1, x2: squared_exponential_kernel(\n",
    "    x1, x2, sigma=1.0, lengthscale=1.0\n",
    ")\n",
    "noise_variance_ex = 0.1**2\n",
    "\n",
    "# Precompute necessary terms\n",
    "mu_X_ex = zero_mean_ex(X_train_ex)\n",
    "y_diff_ex = y_train_ex - mu_X_ex\n",
    "K_XX_ex = rbf_kernel_ex(X_train_ex, X_train_ex)\n",
    "K_XX_noisy_ex = K_XX_ex + noise_variance_ex * jnp.eye(N_example)\n",
    "\n",
    "# Define the gradient function using JAX's grad\n",
    "grad_alpha_loss = grad(alpha_loss_function)\n",
    "\n",
    "# Compute the gradient at an arbitrary alpha (e.g., zeros)\n",
    "alpha_initial = jnp.zeros(N_example)\n",
    "gradient_at_initial_alpha = grad_alpha_loss(alpha_initial, y_diff_ex, K_XX_noisy_ex)\n",
    "\n",
    "print(\"Initial alpha (zeros):\\n\", alpha_initial)\n",
    "print(\"\\nGradient of L(alpha) at initial alpha:\\n\", gradient_at_initial_alpha)\n",
    "\n",
    "# The analytical solution for alpha_star is when the gradient is zero:\n",
    "# (K_XX + sigma^2 I) @ alpha - (y - mu_X) = 0\n",
    "# alpha_star = inv(K_XX + sigma^2 I) @ (y - mu_X)\n",
    "alpha_star_analytical = solve(K_XX_noisy_ex, y_diff_ex)\n",
    "\n",
    "print(\"\\nAnalytical alpha_star:\\n\", alpha_star_analytical)\n",
    "\n",
    "# Compute the gradient at the analytical alpha_star (should be close to zero)\n",
    "gradient_at_alpha_star = grad_alpha_loss(\n",
    "    alpha_star_analytical, y_diff_ex, K_XX_noisy_ex\n",
    ")\n",
    "print(\n",
    "    \"\\nGradient of L(alpha) at analytical alpha_star (should be near zero):\\n\",\n",
    "    gradient_at_alpha_star,\n",
    ")\n",
    "print(\n",
    "    f\"Max absolute value of gradient at alpha_star: {jnp.max(jnp.abs(gradient_at_alpha_star)):.2e}\"\n",
    ")\n",
    "\n",
    "# This illustrates that the gradient computation itself is feasible,\n",
    "# but finding the alpha_star (by setting gradient to zero or optimizing)\n",
    "# still involves solving the linear system, which is the O(N^3) step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa481d13",
   "metadata": {},
   "source": [
    "# What About the Uncertainty? Getting the Posterior Covariance\n",
    "\n",
    "Quantifying uncertainty is a **major advantage of Gaussian Processes (GPs)**. The **posterior covariance** $k_y(\\cdot, \\circ)$ (or $\\operatorname{Cov}(f_X, f_X)$ for training points) provides a principled measure of this uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "## The Posterior Covariance in GP Regression\n",
    "\n",
    "### **1. The Laplace Approximation (Exact for Gaussians)**\n",
    "\n",
    "For Gaussian models, the **Laplace approximation** to the posterior is **exact**. The negative log-posterior is a quadratic function of $f_X$:\n",
    "\n",
    "$$\n",
    "-\\log p(f_X \\mid y) = \\frac{1}{2\\sigma^2} \\|y - f_X\\|^2 + \\frac{1}{2} (f_X - \\mu_X)^\\top K_{XX}^{-1} (f_X - \\mu_X) + \\text{const.}\n",
    "$$\n",
    "\n",
    "- The first term is the **likelihood** (data fit).\n",
    "- The second term is the **prior** (regularization).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Computing the Posterior Covariance**\n",
    "\n",
    "The **Hessian** (second derivative) of the negative log-posterior with respect to $f_X$ gives the **inverse posterior covariance**:\n",
    "\n",
    "$$\n",
    "\\nabla \\nabla^\\top \\log p(f_X \\mid y) = -(\\sigma^{-2} I + K_{XX}^{-1})\n",
    "$$\n",
    "\n",
    "For a Gaussian, we know:\n",
    "\n",
    "$$\n",
    "\\nabla \\nabla^\\top \\log \\mathcal{N}(x; m, V) = -V^{-1}\n",
    "$$\n",
    "\n",
    "Thus, the **posterior covariance** on $f_X$ is:\n",
    "\n",
    "$$\n",
    "\\operatorname{Cov}(f_X, f_X) = (\\sigma^{-2} I + K_{XX}^{-1})^{-1}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **3. The Woodbury Matrix Identity**\n",
    "\n",
    "Using the **Woodbury matrix identity**, we can rewrite the posterior covariance as:\n",
    "\n",
    "$$\n",
    "\\operatorname{Cov}(f_X, f_X) = K_{XX} - K_{XX} (\\sigma^2 I + K_{XX})^{-1} K_{XX}\n",
    "$$\n",
    "\n",
    "- This form is often more numerically stable and interpretable.\n",
    "- However, we **still need to invert** $(K_{XX} + \\sigma^2 I)$ (or solve a linear system involving it), which is $\\mathcal{O}(N^3)$ in time.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary Table: Posterior Covariance Computation**\n",
    "\n",
    "| Step                              | Formula                                                                 | Computational Cost      |\n",
    "|------------------------------------|------------------------------------------------------------------------|------------------------|\n",
    "| Inverse posterior covariance       | $(\\sigma^{-2} I + K_{XX}^{-1})$                                        | $\\mathcal{O}(N^3)$     |\n",
    "| Posterior covariance (Woodbury)    | $K_{XX} - K_{XX} (\\sigma^2 I + K_{XX})^{-1} K_{XX}$                    | $\\mathcal{O}(N^3)$     |\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Does This Matter?**\n",
    "\n",
    "- The ability to **quantify uncertainty** is a key reason to use GPs.\n",
    "- Computing the posterior covariance is **computationally expensive** for large datasets, motivating scalable approximations.\n",
    "\n",
    "---\n",
    "\n",
    "## **Let's Illustrate with Code**\n",
    "\n",
    "We'll use a simple RBF kernel and some dummy data to demonstrate how to compute the posterior covariance in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf556945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import solve\n",
    "from jax.linalg import (\n",
    "    inv,\n",
    ")  # For direct inverse illustration, but solve is preferred for stability\n",
    "\n",
    "\n",
    "# Assume squared_exponential_kernel is defined as in previous notebooks\n",
    "def squared_exponential_kernel(\n",
    "    x1: jnp.ndarray, x2: jnp.ndarray, sigma: float = 1.0, lengthscale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the Squared Exponential (RBF) kernel matrix.\n",
    "    \"\"\"\n",
    "    x1 = jnp.atleast_2d(x1)\n",
    "    x2 = jnp.atleast_2d(x2)\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    K = sigma**2 * jnp.exp(-0.5 * sq_dist / lengthscale**2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# Dummy data and parameters\n",
    "N = 5  # Number of training points\n",
    "X_train = jnp.linspace(0, 10, N)[:, None]\n",
    "y_train = jnp.sin(X_train).squeeze() + 0.1 * jnp.array(\n",
    "    [0.5, -0.2, 0.1, -0.3, 0.4]\n",
    ")  # Some dummy observations\n",
    "mean_func = lambda x: jnp.zeros(x.shape[0])\n",
    "noise_variance = 0.1**2  # sigma^2\n",
    "\n",
    "# Compute K_XX\n",
    "K_XX = squared_exponential_kernel(X_train, X_train, sigma=1.0, lengthscale=1.0)\n",
    "\n",
    "# K_XX_noisy = K_XX + sigma^2 * I\n",
    "K_XX_noisy = K_XX + noise_variance * jnp.eye(N)\n",
    "\n",
    "# Method 1: Using the Woodbury Identity form for posterior covariance\n",
    "# Cov(fX, fX) = K_XX - K_XX @ inv(sigma^2 * I + K_XX) @ K_XX\n",
    "# This is equivalent to K_XX - K_XX @ inv(K_XX_noisy) @ K_XX\n",
    "# For numerical stability, use solve instead of inv where possible\n",
    "term_inv_K_XX_noisy_K_XX = solve(K_XX_noisy, K_XX)\n",
    "post_cov_fX_woodbury = K_XX - jnp.dot(K_XX, term_inv_K_XX_noisy_K_XX)\n",
    "\n",
    "print(\"Posterior Covariance (Woodbury Identity):\\n\", post_cov_fX_woodbury)\n",
    "\n",
    "# Alternative form: (sigma^-2 * I + K_XX^-1)^-1\n",
    "# This requires K_XX_inv, which is also O(N^3)\n",
    "K_XX_inv = inv(K_XX)\n",
    "post_cov_fX_laplace_direct = inv((1 / noise_variance) * jnp.eye(N) + K_XX_inv)\n",
    "\n",
    "print(\"\\nPosterior Covariance (Laplace Direct Form):\\n\", post_cov_fX_laplace_direct)\n",
    "\n",
    "# Check if they are numerically close\n",
    "print(\n",
    "    f\"\\nMax absolute difference between Woodbury and Laplace forms: {jnp.max(jnp.abs(post_cov_fX_woodbury - post_cov_fX_laplace_direct)):.2e}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e6559",
   "metadata": {},
   "source": [
    "## Second Approach: Jacobian/Sensitivity of the Minimizer\n",
    "\n",
    "Recall that the optimal $\\alpha^\\star$ for the GP regression problem is given by:\n",
    "\n",
    "$$\n",
    "\\alpha^\\star = (K_{XX} + \\sigma^2 I_N)^{-1}(y - \\mu_X)\n",
    "$$\n",
    "\n",
    "This is the minimizer of the quadratic loss $L(\\alpha)$. The **Jacobian** of this minimizer with respect to the observations $y$ is:\n",
    "\n",
    "$$\n",
    "\\frac{d\\alpha^\\star}{dy} = (K_{XX} + \\sigma^2 I_N)^{-1}\n",
    "$$\n",
    "\n",
    "This Jacobian tells us how sensitive the optimal weights $\\alpha^\\star$ are to changes in the observed data $y$.\n",
    "\n",
    "---\n",
    "\n",
    "### Posterior Covariance via the Jacobian\n",
    "\n",
    "The posterior covariance function $k_y(\\cdot, \\circ)$ can be expressed in terms of this Jacobian:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "k_y(\\cdot, \\circ) &= k_{\\cdot \\circ} - k_{\\cdot X}(K_{XX} + \\sigma^2 I_N)^{-1}k_{X \\circ} \\\\\n",
    "                  &= k_{\\cdot \\circ} - k_{\\cdot X} \\left( \\frac{d\\alpha^\\star}{dy} \\right) k_{X \\circ}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $k_{\\cdot X}$: Covariance vector between test point(s) and training points\n",
    "- $k_{X \\circ}$: Covariance vector between training points and another test point\n",
    "\n",
    "This formulation highlights that the posterior covariance is directly related to the sensitivity of the solution $\\alpha^\\star$ to the data $y$.\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:**  \n",
    "> Even with this alternative perspective, computing the posterior covariance **exactly** still requires operations with at least $\\Omega(N^3)$ time complexity, due to the need to invert or solve linear systems involving the $N \\times N$ kernel matrix.\n",
    "\n",
    "---\n",
    "\n",
    "## Let's Illustrate the Jacobian $\\frac{d\\alpha^\\star}{dy}$ in Code\n",
    "\n",
    "We'll use JAX to compute and visualize the Jacobian, and show how it relates to the posterior covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc5c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import solve\n",
    "from jax import jacfwd  # For forward-mode automatic differentiation\n",
    "\n",
    "# Re-using K_XX_noisy from the previous code block\n",
    "# K_XX_noisy = K_XX + noise_variance * jnp.eye(N)\n",
    "\n",
    "\n",
    "# Define the function for alpha_star\n",
    "# alpha_star(y) = inv(K_XX_noisy) @ (y - mean_func(X_train))\n",
    "def alpha_star_func(y_obs: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Computes alpha_star given observations y_obs.\n",
    "    K_XX_noisy and mean_func(X_train) are assumed to be fixed (closure over outer scope).\n",
    "    \"\"\"\n",
    "    # Using solve for numerical stability instead of direct inv\n",
    "    return solve(K_XX_noisy, y_obs - mean_func(X_train))\n",
    "\n",
    "\n",
    "# Compute the Jacobian of alpha_star_func with respect to y_obs\n",
    "# jacfwd computes the Jacobian matrix by applying forward-mode AD.\n",
    "jacobian_alpha_star_dy = jacfwd(alpha_star_func)(y_train)\n",
    "\n",
    "print(\"\\nJacobian d(alpha_star)/dy:\\n\", jacobian_alpha_star_dy)\n",
    "\n",
    "# Compare with the inverse of K_XX_noisy\n",
    "K_XX_noisy_inv_computed = inv(K_XX_noisy)\n",
    "print(\"\\nInverse of (K_XX + sigma^2 I):\\n\", K_XX_noisy_inv_computed)\n",
    "\n",
    "print(\n",
    "    f\"\\nMax absolute difference between Jacobian and direct inverse: {jnp.max(jnp.abs(jacobian_alpha_star_dy - K_XX_noisy_inv_computed)):.2e}\"\n",
    ")\n",
    "\n",
    "# This confirms that the Jacobian of alpha_star with respect to y is indeed the inverse of (K_XX + sigma^2 I).\n",
    "# The full posterior covariance calculation (k_y(bullet, circ)) then uses this inverse.\n",
    "# These code snippets demonstrate the core linear algebra operations involved in calculating the posterior covariance, highlighting the O(N3) complexity due to matrix inversion or solving linear systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ebc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import solve\n",
    "from jax.linalg import (\n",
    "    inv,\n",
    ")  # For direct inverse illustration, but solve is preferred for stability\n",
    "from jax import jacfwd  # For forward-mode automatic differentiation\n",
    "\n",
    "\n",
    "# Assume squared_exponential_kernel is defined as in previous notebooks\n",
    "def squared_exponential_kernel(\n",
    "    x1: jnp.ndarray, x2: jnp.ndarray, sigma: float = 1.0, lengthscale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the Squared Exponential (RBF) kernel matrix.\n",
    "    \"\"\"\n",
    "    x1 = jnp.atleast_2d(x1)\n",
    "    x2 = jnp.atleast_2d(x2)\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    K = sigma**2 * jnp.exp(-0.5 * sq_dist / lengthscale**2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# Dummy data and parameters\n",
    "N = 5  # Number of training points\n",
    "X_train = jnp.linspace(0, 10, N)[:, None]\n",
    "y_train = jnp.sin(X_train).squeeze() + 0.1 * jnp.array(\n",
    "    [0.5, -0.2, 0.1, -0.3, 0.4]\n",
    ")  # Some dummy observations\n",
    "mean_func = lambda x: jnp.zeros(x.shape[0])\n",
    "noise_variance = 0.1**2  # sigma^2\n",
    "\n",
    "# Compute K_XX\n",
    "K_XX = squared_exponential_kernel(X_train, X_train, sigma=1.0, lengthscale=1.0)\n",
    "\n",
    "# K_XX_noisy = K_XX + sigma^2 * I\n",
    "K_XX_noisy = K_XX + noise_variance * jnp.eye(N)\n",
    "\n",
    "# Method 1: Using the Woodbury Identity form for posterior covariance\n",
    "# Cov(fX, fX) = K_XX - K_XX @ inv(sigma^2 * I + K_XX) @ K_XX\n",
    "# This is equivalent to K_XX - K_XX @ inv(K_XX_noisy) @ K_XX\n",
    "# For numerical stability, use solve instead of inv where possible\n",
    "term_inv_K_XX_noisy_K_XX = solve(K_XX_noisy, K_XX)\n",
    "post_cov_fX_woodbury = K_XX - jnp.dot(K_XX, term_inv_K_XX_noisy_K_XX)\n",
    "\n",
    "print(\"Posterior Covariance (Woodbury Identity):\\n\", post_cov_fX_woodbury)\n",
    "\n",
    "# Alternative form: (sigma^-2 * I + K_XX^-1)^-1\n",
    "# This requires K_XX_inv, which is also O(N^3)\n",
    "K_XX_inv = inv(K_XX)\n",
    "post_cov_fX_laplace_direct = inv((1 / noise_variance) * jnp.eye(N) + K_XX_inv)\n",
    "\n",
    "print(\"\\nPosterior Covariance (Laplace Direct Form):\\n\", post_cov_fX_laplace_direct)\n",
    "\n",
    "# Check if they are numerically close\n",
    "print(\n",
    "    f\"\\nMax absolute difference between Woodbury and Laplace forms: {jnp.max(jnp.abs(post_cov_fX_woodbury - post_cov_fX_laplace_direct)):.2e}\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- Illustrate Jacobian/Sensitivity approach ---\n",
    "# Define the function for alpha_star\n",
    "# alpha_star(y) = inv(K_XX_noisy) @ (y - mean_func(X_train))\n",
    "# We wrap K_XX_noisy and mean_func(X_train) in a closure for jacfwd\n",
    "def alpha_star_func_closure(y_obs: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Computes alpha_star given observations y_obs, using precomputed K_XX_noisy.\n",
    "    \"\"\"\n",
    "    return solve(K_XX_noisy, y_obs - mean_func(X_train))\n",
    "\n",
    "\n",
    "# Compute the Jacobian of alpha_star_func_closure with respect to y_obs\n",
    "# jacfwd computes the Jacobian matrix by applying forward-mode AD.\n",
    "jacobian_alpha_star_dy = jacfwd(alpha_star_func_closure)(y_train)\n",
    "\n",
    "print(\"\\nJacobian d(alpha_star)/dy:\\n\", jacobian_alpha_star_dy)\n",
    "\n",
    "# Compare with the inverse of K_XX_noisy\n",
    "K_XX_noisy_inv_computed = inv(K_XX_noisy)\n",
    "print(\"\\nInverse of (K_XX + sigma^2 I):\\n\", K_XX_noisy_inv_computed)\n",
    "\n",
    "print(\n",
    "    f\"\\nMax absolute difference between Jacobian and direct inverse: {jnp.max(jnp.abs(jacobian_alpha_star_dy - K_XX_noisy_inv_computed)):.2e}\"\n",
    ")\n",
    "\n",
    "# This confirms that the Jacobian of alpha_star with respect to y is indeed the inverse of (K_XX + sigma^2 I).\n",
    "# The full posterior covariance calculation (k_y(bullet, circ)) then uses this inverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7fce7",
   "metadata": {},
   "source": [
    "# Summary: The Role of Linear Algebra in Gaussian Processes\n",
    "\n",
    "This lecture provided a comprehensive exploration of the computational foundations of Gaussian Processes (GPs), with a special focus on the pivotal role of linear algebra. Here are the key takeaways:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **GP Model Instantiation is Computationally Free**\n",
    "\n",
    "- **Defining a GP** (specifying the mean and kernel functions) incurs negligible computational cost.\n",
    "- The real computational challenges arise only when fitting the model to data.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **\"Training\" a GP = Solving an Optimization Problem**\n",
    "\n",
    "- **Training a GP** means finding the posterior mean function, which is equivalent to minimizing the negative log-posterior.\n",
    "- This is a **convex quadratic optimization problem** and can be solved analytically or with gradient-based methods.\n",
    "- The optimization involves the kernel (covariance) matrix, which encodes the relationships between all pairs of training points.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Efficient Point Predictions (After Training)**\n",
    "\n",
    "- Once the kernel matrix has been decomposed (e.g., via Cholesky decomposition), **making predictions at new test points is efficient**:\n",
    "    - **Posterior mean:** $\\mathcal{O}(N)$ per test point\n",
    "    - **Posterior variance:** $\\mathcal{O}(N^2)$ per test point\n",
    "- The initial matrix decomposition is the main computational hurdle.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **The $\\mathcal{O}(N^3)$ Bottleneck**\n",
    "\n",
    "- **Both the posterior mean and covariance** require solving linear systems involving the $N \\times N$ kernel matrix.\n",
    "- **Cholesky decomposition** (or similar matrix factorization) has a computational complexity of $\\mathcal{O}(N^3)$.\n",
    "- There is currently **no known linear-time algorithm** for exact GP inference with general kernels.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Nonparametric Nature Drives the Cost**\n",
    "\n",
    "- The $\\mathcal{O}(N^3)$ scaling is a consequence of the **nonparametric** nature of GPs:\n",
    "    - The number of \"parameters\" (function values at training points) grows with the dataset size $N$.\n",
    "- This is **not** a result of being Bayesian or probabilistic.\n",
    "- In contrast, **deep learning models** have a fixed number of parameters and leverage stochastic optimization, allowing for $\\mathcal{O}(N)$ or even $\\mathcal{O}(1)$ per-iteration scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## **Looking Ahead**\n",
    "\n",
    "- In the next lecture, we will:\n",
    "    - Dive deeper into **Cholesky decomposition** and its practical implementation.\n",
    "    - Discuss **data loading strategies** for large-scale GP models.\n",
    "    - Explore advanced methods for **uncertainty estimation** in GPs, especially for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "> **Bottom line:**  \n",
    "> Mastery of linear algebra is essential for understanding both the power and the computational limitations of Gaussian Processes. Efficient matrix operations are at the heart of scalable and robust GP inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
