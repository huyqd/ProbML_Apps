{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0211a83f",
   "metadata": {},
   "source": [
    "# Computation and Inference in Gaussian Processes\n",
    "\n",
    "Welcome to the final lecture in our **Probabilistic Machine Learning** series! In previous sessions, we established **Gaussian Processes (GPs)** as powerful probabilistic models for functions, explored various kernel functions, and delved into the theoretical underpinnings of **RKHS** and the connection between Bayesian GPs and Frequentist kernel methods.\n",
    "\n",
    "---\n",
    "\n",
    "## Lecture Overview\n",
    "\n",
    "In this lecture, we'll bring together the concepts of computation and inference. We'll challenge the common perception of computational complexity in GPs versus deep learning and show how core linear algebra routines are, in essence, learning algorithms themselves.\n",
    "\n",
    "**Specifically, we will:**\n",
    "\n",
    "- **Revisit** the computational bottleneck of GPs and address common misunderstandings about their $O(N^3)$ complexity.\n",
    "- **Explore** how matrix decompositions, particularly Cholesky, can be viewed as iterative \"data loading\" or \"learning\" processes.\n",
    "- **Introduce** Schur complements and their role in efficiently updating inverse matrices and solutions.\n",
    "- **Demonstrate** how iterative methods like Conjugate Gradients (via the Lanczos process) can provide efficient ways to update GP posteriors, offering a bridge between exact Bayesian inference and scalable iterative optimization.\n",
    "- **Conclude** that for GPs, there is no fundamental separation between \"computing\" and \"learning\"; numerical algorithms are learning machines.\n",
    "\n",
    "This lecture will provide a holistic view of the interplay between mathematical theory, numerical methods, and the practical implications for probabilistic machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## The Training Metaphor: Context of the Course\n",
    "\n",
    "Our overarching goal for this course has been to develop a **probabilistic perspective** on contemporary machine learning. In this view, the process of \"learning\" is re-phrased as **inference**, which involves manipulating probability distributions on hypothesis spaces (e.g., function spaces for GPs).\n",
    "\n",
    "So far, we have developed the analytic framework of **Gaussian Process (GP) regression** from first principles. This yields a clean and elegant picture, offering exact posterior distributions over functions. However, this analytic framework is practically limited in a few key ways:\n",
    "\n",
    "- It's primarily limited to real-valued functions $f: X \\to \\mathbb{R}^C$ (though extensions exist for multi-output GPs).\n",
    "- It inherently requires low-level linear algebra operations, particularly matrix inversions or solving linear systems involving the $N \\times N$ kernel matrix, which leads to $O(N^3)$ computational complexity.\n",
    "\n",
    "Over the coming lectures (and in the broader context of machine learning research), we trace a path all the way to contemporary deep learning. This involves thinking both about the **model** (what we're trying to learn) and about the **computation** (how we actually perform the learning).\n",
    "\n",
    "------\n",
    "\n",
    "### Addressing a Common Misunderstanding: Computational Complexity\n",
    "\n",
    "- **GPs are $O(N^3)$**\n",
    "- **Deep learning is $O(1)$ (per training step, for a fixed batch size)**\n",
    "\n",
    "This comparison is often misleading because it compares a hard upper bound (for exact GP inference) to a loose lower bound (for a single stochastic gradient descent step in deep learning). The true picture is more nuanced, and we'll explore why this distinction is critical.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6048104",
   "metadata": {},
   "source": [
    "## The Cholesky Decomposition: Recap & Pseudocode\n",
    "\n",
    "In the previous lecture, we introduced the **Cholesky decomposition** as a fundamental tool for exact Gaussian Process (GP) inference. This method efficiently and stably decomposes a symmetric positive definite (SPD) matrix $A$ into a lower triangular matrix $L$ such that:\n",
    "\n",
    "$$\n",
    "A = L L^\\top\n",
    "$$\n",
    "### Mathematical Formulation: Iterative Cholesky Decomposition\n",
    "\n",
    "Given a symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$, the Cholesky decomposition finds a lower triangular matrix $L$ such that:\n",
    "\n",
    "$$\n",
    "A = L L^\\top\n",
    "$$\n",
    "\n",
    "The entries of $L$ are computed iteratively as follows:\n",
    "\n",
    "- **Diagonal entries:**\n",
    "  $$\n",
    "  L_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2}\n",
    "  $$\n",
    "\n",
    "- **Off-diagonal entries (for $j > i$):**\n",
    "  $$\n",
    "  L_{ji} = \\frac{1}{L_{ii}} \\left( A_{ji} - \\sum_{k=1}^{i-1} L_{jk} L_{ik} \\right)\n",
    "  $$\n",
    "\n",
    "**Iterative Process:**\n",
    "\n",
    "1. For $i = 1$ to $n$:\n",
    "    - Compute $L_{ii}$ using the formula above.\n",
    "    - For each $j = i+1$ to $n$, compute $L_{ji}$.\n",
    "\n",
    "This process \"loads\" one column of $L$ at a time, updating the remaining submatrix at each step. The algorithm is numerically stable and efficient for SPD matrices, making it the standard approach for GP inference and many other applications in scientific computing.\n",
    "### Conceptual Pseudocode for Cholesky Decomposition\n",
    "\n",
    "Below is a step-by-step pseudocode outlining the Cholesky decomposition process:\n",
    "\n",
    "---\n",
    "\n",
    "**Algorithm 1: Cholesky Decomposition**\n",
    "\n",
    "- **Input:** Symmetric Positive Definite (SPD) matrix $A \\in \\mathbb{R}^{n \\times n}$\n",
    "- **Output:** Lower triangular matrix $L$, such that $LL^\\top = A$\n",
    "\n",
    "  ```python\n",
    "  def cholesky_decomposition(A):\n",
    "    \"\"\"\n",
    "    Perform Cholesky decomposition on a symmetric positive definite (SPD) matrix A.\n",
    "    Returns lower triangular matrix L such that A = L @ L.T\n",
    "\n",
    "    Args:\n",
    "      A (jax.numpy.ndarray): SPD matrix of shape (n, n)\n",
    "\n",
    "    Returns:\n",
    "      L (jax.numpy.ndarray): Lower triangular matrix of shape (n, n)\n",
    "    \"\"\"\n",
    "    import jax.numpy as jnp\n",
    "\n",
    "    n = A.shape[0]\n",
    "    L = jnp.zeros_like(A)\n",
    "\n",
    "    for i in range(n):\n",
    "      # Compute the diagonal element\n",
    "      sum_k = jnp.sum(L[i, :i] ** 2)\n",
    "      L = L.at[i, i].set(jnp.sqrt(A[i, i] - sum_k))\n",
    "\n",
    "      # Compute the off-diagonal elements\n",
    "      for j in range(i + 1, n):\n",
    "        sum_k = jnp.sum(L[j, :i] * L[i, :i])\n",
    "        L = L.at[j, i].set((A[j, i] - sum_k) / L[i, i])\n",
    "\n",
    "    return L\n",
    "  ```\n",
    "\n",
    "  **Step-by-step Explanation:**\n",
    "\n",
    "  - **Initialization:**  \n",
    "    - Create a zero matrix $L$ of the same shape as $A$.\n",
    "  - **Iterative Construction:**  \n",
    "    - For each row $i$:\n",
    "    - Compute the diagonal entry $L_{ii}$ using previously computed values.\n",
    "    - For each row $j > i$, compute the off-diagonal entries $L_{ji}$.\n",
    "  - **Return:**  \n",
    "    - The lower triangular matrix $L$ such that $A = LL^\\top$.\n",
    "\n",
    "  **Key Observations:**\n",
    "  - The cost of each iteration decreases as $i$ increases, since the submatrix being updated shrinks.\n",
    "  - The overall computational complexity is $O(N^3)$, dominated by the first few iterations.\n",
    "\n",
    "  **Visual Intuition:**\n",
    "  - Imagine the matrix as a grid:\n",
    "    - **Blue areas:** Already processed (loaded) into $L$.\n",
    "    - **Gray area:** Remaining submatrix to be decomposed.\n",
    "  - As the algorithm proceeds, the blue area grows and the gray area shrinks, mirroring the learning process.\n",
    "\n",
    "  ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9e7ba",
   "metadata": {},
   "source": [
    "## Computing the Inverse Alongside the Decomposition: Cholesky as a Linear Solver\n",
    "\n",
    "The **Cholesky decomposition** is not just a tool for factorizing a matrix; it is also deeply connected to solving linear systems and, perhaps surprisingly, to iteratively building an approximation of the matrix inverse.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Suppose we want to find a low-rank approximation $C_i \\approx A^{-1}$ as we proceed through the Cholesky decomposition. This is useful in Gaussian Processes and other kernel methods, where inverting large matrices is a computational bottleneck.\n",
    "\n",
    "### Key Observation\n",
    "\n",
    "If we have an approximation $L_i L_i^\\top \\approx A$, can we also approximate $A^{-1}$?\n",
    "\n",
    "- If $L_i L_i^\\top \\approx A$, then $(A^{-1} L_i)(A^{-1} L_i)^\\top \\approx A^{-1}$.\n",
    "- Define $C_i = (A^{-1} L_i)(A^{-1} L_i)^\\top$ as our inverse approximation.\n",
    "\n",
    "The core idea is to **track the effect of the inverse on the columns of $L_i$**. Consider the last column: $(A^{-1} L_i)_{:i} = A^{-1} l_i$. This term can be related to previous steps:\n",
    "\n",
    "$$\n",
    "A^{-1} l_i = A^{-1} \\frac{A'_{:i}}{A'_{ii}}\n",
    "= A^{-1} (A - L_{i-1} L_{i-1}^\\top) e_i / \\|e_i\\|_{A'}\n",
    "= (I - A^{-1} L_{i-1} L_{i-1}^\\top) e_i / \\|e_i\\|_{A'}\n",
    "= (I - C_{i-1} A) e_i / \\|e_i\\|_{A'}\n",
    "$$\n",
    "\n",
    "This shows that the inverse's action on the current column of $L$ is related to the **residual of the previous inverse approximation**.\n",
    "\n",
    "---\n",
    "### Algorithm 2: Cholesky Decomposition\n",
    "\n",
    "**Input:** spd matrix $A$\n",
    "\n",
    "```code\n",
    "procedure CHOLESKY(A):\n",
    "    A_prime = A\n",
    "    L = []\n",
    "    for i in range(n):\n",
    "        e_i = canonical_basis_vector(i, n)\n",
    "        l_i = A_prime[:, i] / sqrt(A_prime[i, i])\n",
    "        A_prime = A_prime - jnp.outer(l_i, l_i)\n",
    "        L.append(l_i)\n",
    "    return stack_columns(L)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Step-by-Step Explanation:**\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Set $A'$ (A\\_prime) to the input matrix $A$.\n",
    "   - Initialize an empty list $L$ to store the columns of the Cholesky factor.\n",
    "\n",
    "2. **Iterative Decomposition:**\n",
    "   - For each index $i$ from $0$ to $n-1$:\n",
    "     - Select the $i$-th canonical basis vector $e_i$ (a vector with $1$ at position $i$, $0$ elsewhere).\n",
    "     - Compute the $i$-th column of $L$:\n",
    "       - $l_i = A'_{:, i} / \\sqrt{A'_{i, i}}$\n",
    "       - This normalizes the $i$-th column of the current residual matrix.\n",
    "     - Update the residual matrix $A'$:\n",
    "       - $A' = A' - l_i l_i^\\top$\n",
    "       - This subtracts the outer product of $l_i$ with itself, removing the contribution of the $i$-th component.\n",
    "     - Append $l_i$ to the list $L$.\n",
    "\n",
    "3. **Return:**\n",
    "   - After all iterations, stack the columns in $L$ to form the lower-triangular Cholesky factor.\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "- At each step, we extract the next column of $L$ by normalizing the current column of the residual matrix.\n",
    "- We then \"deflate\" the matrix by removing the contribution of this column, ensuring that subsequent columns are orthogonalized.\n",
    "- This process continues until all columns are processed, resulting in a lower-triangular matrix $L$ such that $A \\approx LL^\\top$.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- The Cholesky decomposition builds $L$ one column at a time, each time updating the matrix to account for what has already been \"explained.\"\n",
    "- The normalization by $\\sqrt{A'_{i, i}}$ ensures numerical stability and that $L$ is lower-triangular.\n",
    "- This iterative approach mirrors how information is sequentially incorporated in probabilistic inference and learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01ebc6",
   "metadata": {},
   "source": [
    "\n",
    "### Algorithm 3: Cholesky with Inverse Approximation\n",
    "\n",
    "This algorithm integrates the computation of a low-rank inverse approximation $C_i$ directly into the Cholesky iterations.\n",
    "\n",
    "**Input:** SPD matrix $A$\n",
    "\n",
    "**Output:** Lower triangular $L_i$ (such that $L_i L_i^\\top \\approx A$), and low-rank $C_i \\approx A^{-1}$\n",
    "\n",
    "#### Step-by-Step Procedure: Cholesky with Inverse Approximation\n",
    "\n",
    "Below is a clear, stepwise breakdown of the algorithm, with explanations for each step:\n",
    "\n",
    "---\n",
    "\n",
    "**Procedure:** `CHOLESKY(A)`\n",
    "\n",
    "1. **Initialization**\n",
    "    - Set $A' \\leftarrow A$ (copy of the input matrix to be updated at each step)\n",
    "    - Set $C_0 = 0$ (initialize the inverse approximation as a zero matrix)\n",
    "    - Set $L_0 = [\\ ]$ (initialize an empty list for columns of $L$)\n",
    "\n",
    "2. **Iterative Updates (for $i = 1$ to $n$):**\n",
    "    - **a. Select Action Vector**\n",
    "        - $s_i \\leftarrow e_i$  \n",
    "          *(Choose the $i$-th canonical basis vector; this \"loads\" the $i$-th data point/column)*\n",
    "    - **b. Compute Residual Direction**\n",
    "        - $d_i \\leftarrow (I - C_{i-1}A) s_i$  \n",
    "          *(Find the part of $s_i$ not yet explained by the current inverse approximation)*\n",
    "    - **c. Compute Normalization (Schur Complement)**\n",
    "        - $\\eta_i \\leftarrow s_i^\\top A d_i = e_i^\\top A' e_i = \\|e_i\\|_{A'}^2$  \n",
    "          *(Measures the \"new information\" added by $d_i$; ensures numerical stability)*\n",
    "    - **d. Compute New Cholesky Column**\n",
    "        - $l_i \\leftarrow A \\left(\\frac{1}{\\sqrt{\\eta_i}}\\right) d_i$  \n",
    "          *(Construct the $i$-th column of $L$; scales $d_i$ appropriately)*\n",
    "    - **e. Update Inverse Approximation**\n",
    "        - $C_i \\leftarrow C_{i-1} + \\frac{1}{\\eta_i} d_i d_i^\\top$  \n",
    "          *(Rank-1 update to the inverse estimate using the new direction)*\n",
    "    - **f. Update Residual Matrix**\n",
    "        - $A' \\leftarrow A - L_i L_i^\\top = A(A^{-1} - C_i)A = A(I - C_i A)$  \n",
    "          *(Deflates $A$ by removing the contribution of the new column; prepares for next step)*\n",
    "    - **g. Store Cholesky Column**\n",
    "        - $L_i = (L_{i-1},\\ l_i)$  \n",
    "          *(Append $l_i$ as a new column to $L$)*\n",
    "\n",
    "3. **Return**\n",
    "    - Output the final lower-triangular matrix $L_n$ and the inverse approximation $C_n$.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table of Steps**\n",
    "\n",
    "| Step | Operation | Purpose |\n",
    "|------|-----------|---------|\n",
    "| 1    | $s_i \\leftarrow e_i$ | Selects the $i$-th data direction |\n",
    "| 2    | $d_i \\leftarrow (I - C_{i-1}A) s_i$ | Finds unexplained component |\n",
    "| 3    | $\\eta_i \\leftarrow s_i^\\top A d_i$ | Normalizes update (Schur complement) |\n",
    "| 4    | $l_i \\leftarrow A (1/\\sqrt{\\eta_i}) d_i$ | Forms new Cholesky column |\n",
    "| 5    | $C_i \\leftarrow C_{i-1} + (1/\\eta_i) d_i d_i^\\top$ | Updates inverse approximation |\n",
    "| 6    | $A' \\leftarrow A - L_i L_i^\\top$ | Updates residual matrix |\n",
    "| 7    | $L_i = (L_{i-1},\\ l_i)$ | Appends new column to $L$ |\n",
    "```python\n",
    "def cholesky_with_inverse_approximation(A):\n",
    "  \"\"\"\n",
    "  Perform Cholesky decomposition with simultaneous low-rank inverse approximation.\n",
    "  Args:\n",
    "    A (jax.numpy.ndarray): SPD matrix of shape (n, n)\n",
    "  Returns:\n",
    "    L (jax.numpy.ndarray): Lower triangular matrix (Cholesky factor)\n",
    "    C (jax.numpy.ndarray): Approximate inverse of A\n",
    "  \"\"\"\n",
    "  import jax.numpy as jnp\n",
    "  n = A.shape[0]\n",
    "  A_prime = A.copy()\n",
    "  C = jnp.zeros_like(A)\n",
    "  L_cols = []\n",
    "  for i in range(n):\n",
    "    s_i = jnp.eye(n)[i]  # canonical basis vector\n",
    "    d_i = s_i - C @ (A @ s_i)\n",
    "    eta_i = s_i.T @ A @ d_i\n",
    "    l_i = (A @ d_i) / jnp.sqrt(eta_i)\n",
    "    C = C + (1 / eta_i) * jnp.outer(d_i, d_i)\n",
    "    L_cols.append(l_i)\n",
    "    # Optionally update A_prime if needed for further analysis\n",
    "  L = jnp.stack(L_cols, axis=1)\n",
    "  return L, C\n",
    "```\n",
    "---\n",
    "\n",
    "**Intuition:**  \n",
    "- Each iteration \"loads\" a new data direction, refines the inverse approximation, and updates the Cholesky factor.\n",
    "- The process is analogous to sequentially learning from each data point, with uncertainty and mean estimates improving at every step.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaway:**  \n",
    "This algorithm not only factorizes the matrix $A$ but also builds an increasingly accurate approximation to $A^{-1}$, step by stepâ€”mirroring the learning process in Gaussian Process inference.\n",
    "\n",
    "**Computational Complexity:** The computational complexity of each step $i$ is $O(iN^2)$, which is still $O(N^3)$ overall.\n",
    "\n",
    "**Key Insight:** Cholesky can be seen as an iterative learning algorithm for the kernel matrix and its inverse. Each step refines the approximation based on processing one \"action\" vector $s_i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax.scipy.linalg import solve\n",
    "\n",
    "\n",
    "# Assume squared_exponential_kernel is defined as in previous notebooks\n",
    "def squared_exponential_kernel(\n",
    "    x1: jnp.ndarray, x2: jnp.ndarray, sigma: float = 1.0, lengthscale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Computes the Squared Exponential (RBF) kernel matrix.\"\"\"\n",
    "    x1 = jnp.atleast_2d(x1)\n",
    "    x2 = jnp.atleast_2d(x2)\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    K = sigma**2 * jnp.exp(-0.5 * sq_dist / lengthscale**2)\n",
    "    return K\n",
    "\n",
    "\n",
    "def iterative_cholesky_with_inverse_approx(A: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    Conceptual implementation of Cholesky with inverse approximation (Algorithm 3).\n",
    "    This is for illustration and might not be numerically stable for large N.\n",
    "    \"\"\"\n",
    "    N = A.shape[0]\n",
    "    C_i = jnp.zeros((N, N), dtype=A.dtype)  # C_0 = 0\n",
    "    L_i_cols = []  # To store columns of L\n",
    "\n",
    "    # For verification later\n",
    "    A_inv_true = jnp.linalg.inv(A)\n",
    "\n",
    "    print(\"--- Iterative Cholesky with Inverse Approximation ---\")\n",
    "    for i in range(N):\n",
    "        s_i = jnp.eye(N, dtype=A.dtype)[i, :]  # e_i (canonical basis vector)\n",
    "\n",
    "        # d_i = (I - C_{i-1}A)s_i\n",
    "        d_i = s_i - jnp.dot(C_i, jnp.dot(A, s_i))\n",
    "\n",
    "        # eta_i = s_i^T A d_i\n",
    "        eta_i = jnp.dot(s_i.T, jnp.dot(A, d_i))\n",
    "\n",
    "        # l_i = A (1/sqrt(eta_i)) d_i\n",
    "        # Note: This step is simplified. In the actual algorithm, l_i is a column of L.\n",
    "        # For this conceptual code, we'll focus on C_i update.\n",
    "        # The true l_i would be derived from the Cholesky update directly.\n",
    "        # For inverse approximation, d_i is the key.\n",
    "\n",
    "        # C_i = C_{i-1} + (1/eta_i) d_i d_i^T\n",
    "        # Add a small epsilon to eta_i to prevent division by zero if it's too small\n",
    "        eta_i_stable = eta_i + 1e-12  # Jitter for stability\n",
    "        C_i_new = C_i + (1 / eta_i_stable) * jnp.outer(d_i, d_i)\n",
    "        C_i = C_i_new  # Update C_i for next iteration\n",
    "\n",
    "        print(f\"\\nIteration {i + 1}:\")\n",
    "        print(f\"  eta_{i + 1}: {eta_i:.4f}\")\n",
    "        print(\n",
    "            f\"  Max abs diff C_{i + 1} vs True A_inv: {jnp.max(jnp.abs(C_i - A_inv_true)):.2e}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n--- Final Results ---\")\n",
    "    print(\"Final approximate inverse C_N:\\n\", C_i)\n",
    "    print(\"\\nTrue Inverse A_inv:\\n\", A_inv_true)\n",
    "    print(\n",
    "        f\"\\nFinal Max abs diff C_N vs True A_inv: {jnp.max(jnp.abs(C_i - A_inv_true)):.2e}\"\n",
    "    )\n",
    "    return C_i\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Create a symmetric positive definite matrix (e.g., from an RBF kernel)\n",
    "N_matrix = 10  # Size of the matrix\n",
    "X_data_matrix = jnp.linspace(-2, 2, N_matrix)[:, None]\n",
    "K_XX_matrix = squared_exponential_kernel(\n",
    "    X_data_matrix, X_data_matrix, sigma=1.0, lengthscale=0.5\n",
    ")\n",
    "# Add noise for positive definiteness (like in GP K_XX + sigma^2 I)\n",
    "A_example = K_XX_matrix + 0.1**2 * jnp.eye(N_matrix)\n",
    "\n",
    "iterative_cholesky_with_inverse_approx(A_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1804c0",
   "metadata": {},
   "source": [
    "# Takeaways: Cholesky Iterations and Uncertainty\n",
    "\n",
    "The iterative nature of the Cholesky decomposition reveals important insights about its computational properties and its role in Gaussian Process (GP) inference:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ **Key Insights**\n",
    "\n",
    "- **Superlinear Expense:**  \n",
    "    - Each iteration $i$ of Cholesky is $\\mathcal{O}((N-i)^2)$, but the total computational complexity is $\\mathcal{O}(N^3)$.\n",
    "    - This means that as the matrix gets wider, each step becomes more expensive, and the overall cost grows rapidly with $N$.\n",
    "\n",
    "- **Comparison to SGD:**  \n",
    "    - Cholesky is computationally more expensive than a single step of Stochastic Gradient Descent (SGD), which is typically $\\mathcal{O}(B)$ (where $B$ is the batch size) and does **not** depend on the total dataset size $N$.\n",
    "    - In contrast, Cholesky's cost is tied directly to the full dataset (or kernel matrix).\n",
    "\n",
    "- **Key Difference: Uncertainty Quantification:**  \n",
    "    - **Cholesky** provides direct access to uncertainty: we obtain the full posterior covariance matrix (or its inverse), which quantifies our uncertainty about predictions.\n",
    "    - **SGD** (in its basic form) only provides a point estimate (the mean), without uncertainty.\n",
    "\n",
    "- **One \"Epoch\":**  \n",
    "    - Cholesky completes \"training\" (i.e., exact inference) in a single pass (one \"epoch\") over the data or matrix columns.\n",
    "    - SGD typically requires many epochs to converge to a good solution.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§® **Posterior Mean and Covariance in GP Regression**\n",
    "\n",
    "The posterior mean $\\mu_y(\\cdot)$ and posterior covariance $k_y(\\cdot, \\circ)$ in GP regression are computed as:\n",
    "\n",
    "$$\n",
    "\\mu_y(\\cdot) = \\mu_\\cdot + k_{\\cdot X} (K_{XX} + \\sigma^2 I)^{-1} (y - \\mu_X)\n",
    "$$\n",
    "\n",
    "$$\n",
    "k_y(\\cdot, \\circ) = k_{\\cdot \\circ} - k_{\\cdot X} (K_{XX} + \\sigma^2 I)^{-1} k_{X \\circ}\n",
    "$$\n",
    "\n",
    "- Here, $(K_{XX} + \\sigma^2 I)^{-1}$ is the inverse of the noisy kernel matrix, typically computed via Cholesky decomposition.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”„ **Iterative Approximation with Cholesky**\n",
    "\n",
    "If we denote $C_i$ as our iterative approximation to $(K_{XX} + \\sigma^2 I)^{-1}$ at step $i$, then at each step we can approximate the posterior mean and covariance as:\n",
    "\n",
    "$$\n",
    "\\mu_y(\\cdot) \\approx \\mu_\\cdot + k_{\\cdot X} C_i (y - \\mu_X)\n",
    "$$\n",
    "\n",
    "$$\n",
    "k_y(\\cdot, \\circ) \\approx k_{\\cdot \\circ} - k_{\\cdot X} C_i k_{X \\circ}\n",
    "$$\n",
    "\n",
    "- **Interpretation:**  \n",
    "    As the Cholesky decomposition proceeds, we are **iteratively refining our estimates** of the posterior mean and covariance.\n",
    "    - Early iterations give rough approximations.\n",
    "    - As more columns are processed, the estimates become more accurate.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ **Summary**\n",
    "\n",
    "- Cholesky decomposition is more computationally intensive than SGD, but it provides **exact inference** and **uncertainty quantification** in one pass.\n",
    "- The iterative process allows us to see how uncertainty estimates improve as we process more data.\n",
    "- This perspective highlights the deep connection between numerical linear algebra and probabilistic inference in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a43ac9",
   "metadata": {},
   "source": [
    "# Computing the Solution Alongside the Decomposition: Cholesky as a Dataloader\n",
    "\n",
    "Building on the idea of iteratively approximating the inverse, we can also iteratively compute the solution vector $\\alpha = A^{-1}y$ (which corresponds to the representer weights for the posterior mean in GPs).\n",
    "\n",
    "## Algorithm 4: Cholesky with Inverse $C$ and Solution $\\alpha$\n",
    "\n",
    "This algorithm extends Algorithm 3 by also tracking an estimate of the solution vector $\\alpha_i \\approx A^{-1}y$.\n",
    "\n",
    "**Input:** SPD matrix $A$, vector $y$\n",
    "\n",
    "**Output:** Lower triangular $L_i$, low-rank $C_i \\approx A^{-1}$, solution estimate $\\alpha_i \\approx A^{-1}y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea90ae1",
   "metadata": {},
   "source": [
    "The iterative update for the solution vector $\\alpha_i$ in the Cholesky/IterGP algorithm is:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\alpha_{i-1} + \\frac{1}{\\eta_i} d_i \\left( d_i^\\top y \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha_{i-1}$ is the previous solution estimate,\n",
    "- $d_i = (I - C_{i-1}A) e_i$ is the update direction,\n",
    "- $\\eta_i = e_i^\\top A d_i$ is the normalization (Schur complement),\n",
    "- $y$ is the data vector.\n",
    "\n",
    "**Expanded for clarity:**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "d_i &= (I - C_{i-1}A) e_i \\\\\n",
    "\\eta_i &= e_i^\\top A d_i \\\\\n",
    "\\alpha_i &= \\alpha_{i-1} + \\frac{1}{\\eta_i} d_i (d_i^\\top y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This formula shows how each iteration refines the solution $\\alpha$ by projecting the data $y$ onto the new direction $d_i$, scaled by the Schur complement $\\eta_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e917e",
   "metadata": {},
   "source": [
    "\n",
    "## Iterative Solution Update\n",
    "\n",
    "At each iteration, the algorithm refines its estimate of the solution vector $\\alpha = A^{-1}y$ using the current approximation of the inverse $C_i \\approx A^{-1}$:\n",
    "\n",
    "$$\n",
    "\\alpha_i = C_i y = \\left(C_{i-1} + \\frac{1}{\\eta_i} d_i d_i^\\top\\right) y = \\alpha_{i-1} + \\frac{1}{\\eta_i} d_i d_i^\\top y\n",
    "$$\n",
    "\n",
    "This update can be further interpreted in terms of the residual of the previous estimate:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\alpha_{i-1} + \\frac{1}{\\eta_i} d_i \\left[ y - A \\alpha_{i-1} \\right]_i\n",
    "$$\n",
    "\n",
    "where $[\\,\\cdot\\,]_i$ denotes the $i$-th component (or projection) of the residual vector.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "- **Data Loading:**  \n",
    "    Each step \"loads\" new information from the data vector $y$ by projecting the current residual (the difference between $y$ and the current prediction $A \\alpha_{i-1}$) onto the update direction $d_i$.\n",
    "- **Refinement:**  \n",
    "    The update incrementally improves the solution, making $\\alpha_i$ a better approximation to $A^{-1}y$ as more columns are processed.\n",
    "- **Connection to Learning:**  \n",
    "    This process mirrors how learning algorithms iteratively refine their predictions as they see more data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table: Iterative Solution Update**\n",
    "\n",
    "| Step | What Happens? | Mathematical Operation |\n",
    "|------|---------------|-----------------------|\n",
    "| 1    | Compute residual | $r_{i-1} = y - A \\alpha_{i-1}$ |\n",
    "| 2    | Project onto $d_i$ | $d_i^\\top r_{i-1}$ |\n",
    "| 3    | Scale and update | $\\alpha_i = \\alpha_{i-1} + \\frac{1}{\\eta_i} d_i (d_i^\\top r_{i-1})$ |\n",
    "\n",
    "---\n",
    "\n",
    "> **In summary:**  \n",
    "> The Cholesky decomposition, when paired with this iterative solution update, acts as a \"dataloader\"â€”sequentially incorporating information from $y$ to refine the solution $\\alpha$. This perspective highlights the deep connection between numerical linear algebra and the learning process in probabilistic models like Gaussian Processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a60a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax.scipy.linalg import solve\n",
    "\n",
    "\n",
    "# Assume squared_exponential_kernel is defined as in previous notebooks\n",
    "def squared_exponential_kernel(\n",
    "    x1: jnp.ndarray, x2: jnp.ndarray, sigma: float = 1.0, lengthscale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Computes the Squared Exponential (RBF) kernel matrix.\"\"\"\n",
    "    x1 = jnp.atleast_2d(x1)\n",
    "    x2 = jnp.atleast_2d(x2)\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    K = sigma**2 * jnp.exp(-0.5 * sq_dist / lengthscale**2)\n",
    "    return K\n",
    "\n",
    "\n",
    "def iterative_cholesky_with_solution_approx(A: jnp.ndarray, y_vec: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    Conceptual implementation of Cholesky with inverse and solution approximation (Algorithm 4).\n",
    "    This is for illustration and might not be numerically stable for large N.\n",
    "    \"\"\"\n",
    "    N = A.shape[0]\n",
    "    C_i = jnp.zeros((N, N), dtype=A.dtype)  # C_0 = 0\n",
    "    alpha_i = jnp.zeros(N, dtype=y_vec.dtype)  # alpha_0 = 0\n",
    "\n",
    "    # For verification later\n",
    "    alpha_true = jnp.linalg.solve(A, y_vec)\n",
    "\n",
    "    print(\"--- Iterative Cholesky with Solution Approximation ---\")\n",
    "    for i in range(N):\n",
    "        s_i = jnp.eye(N, dtype=A.dtype)[i, :]  # e_i (canonical basis vector)\n",
    "\n",
    "        # d_i = (I - C_{i-1}A)s_i\n",
    "        d_i = s_i - jnp.dot(C_i, jnp.dot(A, s_i))\n",
    "\n",
    "        # eta_i = s_i^T A d_i\n",
    "        eta_i = jnp.dot(s_i.T, jnp.dot(A, d_i))\n",
    "\n",
    "        # Add a small epsilon to eta_i to prevent division by zero if it's too small\n",
    "        eta_i_stable = eta_i + 1e-12\n",
    "\n",
    "        # C_i = C_{i-1} + (1/eta_i) d_i d_i^T\n",
    "        C_i_new = C_i + (1 / eta_i_stable) * jnp.outer(d_i, d_i)\n",
    "        C_i = C_i_new\n",
    "\n",
    "        # alpha_i = alpha_{i-1} + (1/eta_i) d_i d_i^T y\n",
    "        alpha_i_new = alpha_i + (1 / eta_i_stable) * jnp.dot(d_i, jnp.dot(d_i.T, y_vec))\n",
    "        alpha_i = alpha_i_new\n",
    "\n",
    "        print(f\"\\nIteration {i + 1}:\")\n",
    "        print(f\"  eta_{i + 1}: {eta_i:.4f}\")\n",
    "        print(\n",
    "            f\"  Max abs diff alpha_{i + 1} vs True alpha: {jnp.max(jnp.abs(alpha_i - alpha_true)):.2e}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n--- Final Results ---\")\n",
    "    print(\"Final approximate alpha_N:\\n\", alpha_i)\n",
    "    print(\"\\nTrue alpha:\\n\", alpha_true)\n",
    "    print(\n",
    "        f\"\\nFinal Max abs diff alpha_N vs True alpha: {jnp.max(jnp.abs(alpha_i - alpha_true)):.2e}\"\n",
    "    )\n",
    "    return alpha_i\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Create a symmetric positive definite matrix (e.g., from an RBF kernel)\n",
    "N_matrix = 10  # Size of the matrix\n",
    "X_data_matrix = jnp.linspace(-2, 2, N_matrix)[:, None]\n",
    "K_XX_matrix = squared_exponential_kernel(\n",
    "    X_data_matrix, X_data_matrix, sigma=1.0, lengthscale=0.5\n",
    ")\n",
    "# Add noise for positive definiteness (like in GP K_XX + sigma^2 I)\n",
    "A_example = K_XX_matrix + 0.1**2 * jnp.eye(N_matrix)\n",
    "\n",
    "# Create a corresponding y vector\n",
    "y_example = jnp.sin(X_data_matrix).squeeze() + 0.2 * jnp.array(np.random.rand(N_matrix))\n",
    "\n",
    "iterative_cholesky_with_solution_approx(A_example, y_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeca542",
   "metadata": {},
   "source": [
    "## Cholesky as Iterative Book-Keeping: Adding Data Points One-by-One\n",
    "\n",
    "The iterative process of Cholesky decompositionâ€”especially when used to compute the inverse and solution estimatesâ€”can be elegantly interpreted as a form of **effective book-keeping**. In this view, Cholesky is akin to **sequentially adding datapoints** to update a Gaussian Process (GP) posterior.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sequential GP Posterior Updates: The Big Picture**\n",
    "\n",
    "Suppose we have:\n",
    "- A \"training set\" $f_X$ with a Gaussian prior:  \n",
    "    $$\n",
    "    p(f_X) = \\mathcal{N}(f_X; \\mu_X, k_{XX})\n",
    "    $$\n",
    "- A Gaussian likelihood:  \n",
    "    $$\n",
    "    p(y \\mid f_X) = \\mathcal{N}(y; f_X, \\sigma^2 I)\n",
    "    $$\n",
    "- Define $K := k_{XX} + \\sigma^2 I$ and $\\tilde{y} := y - \\mu_X$.\n",
    "\n",
    "Now, imagine the observations in $y$ **arrive one at a time**. At iteration $i-1$, we've processed the first $i-1$ data points and have a posterior mean based on them:\n",
    "$$\n",
    "\\mu_{i-1}(X) = \\mu_X + k_{X, X_{[:i-1]}} K_{[:i-1], [:i-1]}^{-1} \\tilde{y}_{[:i-1]}\n",
    "$$\n",
    "\n",
    "When the $i$-th data point arrives, we update to the full posterior mean after observing $i$ points:\n",
    "$$\n",
    "\\mu_i(X) = \\mu_X + \n",
    "\\begin{pmatrix}\n",
    "k_{X, X_{[:i]}} & k_{X, X_i}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "K_{[:i], [:i]} & K_{[:i], i} \\\\\n",
    "K_{i, [:i]} & K_{i, i}\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "\\tilde{y}_{[:i]} \\\\\n",
    "\\tilde{y}_i\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Cholesky = Sequential Posterior Updates**\n",
    "\n",
    "- **Each step of Cholesky** effectively incorporates the information from one new \"dimension\" or \"data point\" into the overall system.\n",
    "- This is achieved by updating the inverse and solution estimates using the **Schur complement**, which efficiently handles the addition of new rows/columns to the kernel matrix.\n",
    "- The process mirrors how we would update the GP posterior if we received data points one at a time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "- **Cholesky decomposition is not just a matrix factorization**â€”it is a *learning process* that sequentially \"loads\" information from each data point.\n",
    "- This perspective helps us understand why Cholesky is so powerful for GP inference: it provides a principled, efficient way to update our beliefs as new data arrives.\n",
    "- The connection to Schur complements highlights the deep interplay between linear algebra and probabilistic inference.\n",
    "\n",
    "---\n",
    "\n",
    "> **In summary:**  \n",
    "> The Cholesky decomposition, when viewed through the lens of probabilistic machine learning, is a beautiful example of how numerical algorithms can be interpreted as iterative learning or inference proceduresâ€”each step refining our understanding as if we were adding one more observation to our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58d254",
   "metadata": {},
   "source": [
    "# Schur Complements: Low-Rank Updates to Matrices, Inverses, and Solutions\n",
    "\n",
    "*By Issai Schur (1875â€“1941)*\n",
    "\n",
    "---\n",
    "\n",
    "Schur complements are a fundamental concept in block matrix algebra. They provide a powerful framework for understanding how the inverse of a matrix changes when we add or remove data points (or dimensions). This is especially important in **Gaussian Processes (GPs)**, where we often need to update the inverse of a kernel matrix as new data arrives.\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Schur Complement?\n",
    "\n",
    "Suppose we have a block matrix $A$ partitioned as:\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "P & Q \\\\\n",
    "R & S\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $P$ is invertible.\n",
    "\n",
    "The **Schur complement** of $P$ in $A$ is defined as:\n",
    "\n",
    "$$\n",
    "M = S - R P^{-1} Q\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Inverse of a Block Matrix Using Schur Complements\n",
    "\n",
    "The inverse of $A$ can be written in terms of its blocks and the Schur complement $M$:\n",
    "\n",
    "$$\n",
    "A^{-1} = \n",
    "\\begin{pmatrix}\n",
    "P^{-1} + P^{-1} Q M^{-1} R P^{-1} & -P^{-1} Q M^{-1} \\\\\n",
    "- M^{-1} R P^{-1} & M^{-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This formula allows us to compute the inverse of $A$ efficiently if we already know $P^{-1}$ and $M^{-1}$.\n",
    "\n",
    "Alternatively, the inverse can be expressed to highlight the **low-rank update** structure:\n",
    "\n",
    "$$\n",
    "A^{-1} = \n",
    "\\begin{pmatrix}\n",
    "P^{-1} & 0 \\\\\n",
    "0 & 0\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "- P^{-1} Q \\\\\n",
    "I\n",
    "\\end{pmatrix}\n",
    "M^{-1}\n",
    "\\begin{pmatrix}\n",
    "- R P^{-1} & I\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Application to Gaussian Processes: Adding a Data Point\n",
    "\n",
    "In GPs, when we add a new data point, we augment the kernel matrix. Suppose our kernel matrix is:\n",
    "\n",
    "$$\n",
    "K = \\begin{pmatrix}\n",
    "K_{XX} & k_{X i} \\\\\n",
    "k_{i X} & k_{ii}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- $K_{XX}$: Kernel matrix for the first $N-1$ points\n",
    "- $k_{X i}$: Cross-covariance vector between existing points and the new point\n",
    "- $k_{ii}$: Kernel value at the new point\n",
    "\n",
    "The inverse $K^{-1}$ can be updated using the Schur complement:\n",
    "\n",
    "$$\n",
    "K^{-1} = \n",
    "\\begin{pmatrix}\n",
    "K_{XX}^{-1} + K_{XX}^{-1} k_{X i} \\eta^{-1} k_{i X} K_{XX}^{-1} & -K_{XX}^{-1} k_{X i} \\eta^{-1} \\\\\n",
    "- \\eta^{-1} k_{i X} K_{XX}^{-1} & \\eta^{-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\eta = k_{ii} - k_{i X} K_{XX}^{-1} k_{X i}\n",
    "$$\n",
    "\n",
    "is the **Schur complement**.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is This Important?\n",
    "\n",
    "- **Efficiency:**  \n",
    "    Schur complements allow us to update the inverse of a matrix when adding (or removing) a data point, without recomputing the entire inverse from scratch. This avoids the full $\\mathcal{O}(N^3)$ cost of matrix inversion.\n",
    "\n",
    "- **Low-Rank Updates:**  \n",
    "    The update to the inverse in Algorithm 3 ($C_i = C_{i-1} + \\frac{1}{\\eta_i} d_i d_i^\\top$) is a direct application of the **rank-1 update formula** derived from Schur complements.\n",
    "\n",
    "- **Probabilistic Interpretation:**  \n",
    "    In GPs, this means we can efficiently update our posterior as new data arrives, making online or sequential inference practical.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Schur complements** provide the mathematical machinery for efficient, incremental updates to matrix inverses.\n",
    "- They are central to scalable Gaussian Process inference and many other areas in numerical linear algebra and statistics.\n",
    "- Understanding Schur complements gives deep insight into how algorithms like Cholesky and iterative GP updates work under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "> **Further Reading:**  \n",
    "> - [Wikipedia: Schur complement](https://en.wikipedia.org/wiki/Schur_complement)  \n",
    "> - Rasmussen & Williams, \"Gaussian Processes for Machine Learning\", Section 2.2.2  \n",
    "> - Matrix Cookbook: [Schur Complement Section](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46ee240",
   "metadata": {},
   "source": [
    "Cleaned-up Algorithm: Iterative GP Regression (Numerics Layer)\n",
    "\n",
    "This algorithm presents a more generalized and cleaned-up version of the iterative process for GP regression, focusing on the numerics layer. It shows how the inverse estimate (Ciâ€‹) and the solution estimate (Î±iâ€‹) are updated iteratively.\n",
    "\n",
    "---\n",
    "### Algorithm 6: Iterative GP Regression (Numerics Layer)\n",
    "\n",
    "**Input:**  \n",
    "- $K = k_{XX} + \\sigma^2 I$ (kernel matrix with noise)\n",
    "- $\\bar{y} = y - \\mu_X$ (centered targets)\n",
    "\n",
    "**Output:**  \n",
    "- $S = [s_j]_{j \\leq n}$ (list of processed vectors)\n",
    "- $C_n \\approx K^{-1}$ (approximate inverse)\n",
    "- $\\alpha_n \\approx K^{-1} \\bar{y}$ (approximate solution)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Procedure: TRAIN($K$, $\\bar{y}$)**\n",
    "\n",
    "1. **Initialization**\n",
    "  - $C_0 \\leftarrow 0$ &nbsp;&nbsp;*(Initial inverse estimate)*\n",
    "  - $\\alpha_0 \\leftarrow 0$ &nbsp;&nbsp;*(Initial solution estimate)*\n",
    "\n",
    "2. **Iterative Updates (for $i = 1$ to $n$):**\n",
    "  - **a. Select Action Vector**\n",
    "    - $s_i \\leftarrow e_i$  \n",
    "      *(Load: Select the $i$-th canonical basis vector)*\n",
    "  - **b. Compute Observation**\n",
    "    - $z_i \\leftarrow K s_i$  \n",
    "      *(Apply $K$ to the action vector)*\n",
    "  - **c. Compute Low-Rank Update / Residual**\n",
    "    - $d_i \\leftarrow (I - C_{i-1} K) s_i = s_i - C_{i-1} z_i$  \n",
    "      *(Residual direction for update)*\n",
    "  - **d. Compute Schur Complement / Normalization**\n",
    "    - $\\eta_i \\leftarrow s_i^\\top K d_i = z_i^\\top d_i$  \n",
    "      *(Normalization constant for stability)*\n",
    "  - **e. Update Inverse Estimate**\n",
    "    - $C_i \\leftarrow C_{i-1} + \\frac{1}{\\eta_i} d_i d_i^\\top$  \n",
    "      *(Rank-1 update to inverse estimate)*\n",
    "  - **f. Update Solution Estimate**\n",
    "    - $\\alpha_i \\leftarrow \\alpha_{i-1} + \\frac{1}{\\eta_i} d_i (d_i^\\top \\bar{y})$  \n",
    "      *(Update solution using new direction)*\n",
    "\n",
    "3. **Return**\n",
    "  - $S = [s_j]_{j \\leq n}$, $\\alpha_n$, $C_n$\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition:**  \n",
    "- Each iteration \"loads\" a new data direction, refines the inverse and solution estimates, and updates the Cholesky factor.\n",
    "- This process is analogous to sequentially learning from each data point, with uncertainty and mean estimates improving at every step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405dc7f5",
   "metadata": {},
   "source": [
    "\n",
    "## GP Prediction: Mean and Uncertainty Formulae\n",
    "\n",
    "After training a Gaussian Process (GP) model, making predictions at new test points $x$ is efficient and interpretable. The prediction step uses the statistics computed during training to provide both the **predictive mean** and **predictive uncertainty**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "\n",
    "Given:\n",
    "- $k[x, S]$: Covariance vector (or matrix) between test points $x$ and the set of processed training vectors $S$.\n",
    "- $\\alpha$: Solution vector (posterior weights), typically $\\alpha = K^{-1}(y - \\mu_X)$.\n",
    "- $C$: Inverse covariance estimate, typically $C \\approx K^{-1}$.\n",
    "- $k_{xx}$: Prior covariance at the test points.\n",
    "\n",
    "The predictive mean and variance are:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu_x &= \\mu(x) + k[x, S]\\, \\alpha \\\\\n",
    "v_{xx} &= k_{xx} - k[x, S]\\, C\\, k[S, x]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $\\mu(x)$: Mean function evaluated at $x$ (often zero).\n",
    "- $k[x, S]$: Covariance between test points and training points.\n",
    "- $k[S, x]$: Transpose of $k[x, S]$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Explanation**\n",
    "\n",
    "1. **Compute Covariance to Training Data**\n",
    "  - $k[x, S]$ gives how similar each test point is to each training point.\n",
    "\n",
    "2. **Predictive Mean**\n",
    "  - $\\mu_x = \\mu(x) + k[x, S]\\, \\alpha$\n",
    "  - This is the expected value of the function at $x$ given the observed data.\n",
    "\n",
    "3. **Predictive Variance (Uncertainty)**\n",
    "  - $v_{xx} = k_{xx} - k[x, S]\\, C\\, k[S, x]$\n",
    "  - This quantifies the model's uncertainty at $x$, accounting for both prior uncertainty and information gained from the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition**\n",
    "\n",
    "- The **predictive mean** is a weighted sum of the observed data, where the weights reflect both the similarity (via the kernel) and the influence of each training point.\n",
    "- The **predictive variance** starts with the prior uncertainty ($k_{xx}$) and subtracts the amount of uncertainty explained by the training data (the second term).\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**\n",
    "\n",
    "| Step                | Formula                                         | Interpretation                       |\n",
    "|---------------------|-------------------------------------------------|--------------------------------------|\n",
    "| Predictive Mean     | $\\mu_x = \\mu(x) + k[x, S]\\, \\alpha$             | Expected value at $x$                |\n",
    "| Predictive Variance | $v_{xx} = k_{xx} - k[x, S]\\, C\\, k[S, x]$       | Model uncertainty at $x$             |\n",
    "\n",
    "---\n",
    "\n",
    "> **In summary:**  \n",
    "> GP prediction is efficient and interpretable: you get both a mean prediction and a principled uncertainty estimate at every test point, using only matrix-vector products with the statistics computed during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e2d1f",
   "metadata": {},
   "source": [
    "\n",
    "## Key Takeaways from the Iterative GP Algorithm\n",
    "\n",
    "- **Iterative Book-Keeping:**  \n",
    "    The algorithm updates both the inverse estimate $C_i$ and the solution estimate $\\alpha_i$ in an iterative fashion. At each step, it processes one \"action\" vector $s_i$ (often chosen as a canonical basis vector $e_i$), incrementally refining both the mean and uncertainty estimates.\n",
    "\n",
    "- **Computational Cost per Step:**  \n",
    "    - Each iteration involves matrix-vector products and outer products.\n",
    "    - For $s_i = e_i$, the cost of computing $d_i$ is $\\mathcal{O}(N^2)$ (since $C_{i-1} z_i$ is a matrix-vector product), and updating $C_i$ is also $\\mathcal{O}(N^2)$.\n",
    "    - With $N$ iterations, the total computational complexity is $\\mathcal{O}(N^3)$, matching the cost of standard Cholesky decomposition.\n",
    "\n",
    "- **Direct Uncertainty Quantification:**  \n",
    "    Unlike stochastic gradient descent (SGD), this iterative process provides both:\n",
    "    - The point estimate $\\alpha_i$ (used for the predictive mean)\n",
    "    - The uncertainty estimate $C_i$ (an approximation to the inverse covariance)\n",
    "    \n",
    "    This means that, at every step, you have access to both the mean and the uncertainty of your predictionsâ€”one of the key strengths of Gaussian Process inference.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Feature                | Iterative GP Algorithm      | SGD                        |\n",
    "|------------------------|----------------------------|----------------------------|\n",
    "| Updates                | $C_i$, $\\alpha_i$          | Parameter vector only      |\n",
    "| Per-step cost          | $\\mathcal{O}(N^2)$         | $\\mathcal{O}(N)$           |\n",
    "| Total cost (for $N$)   | $\\mathcal{O}(N^3)$         | $\\mathcal{O}(N \\cdot T)$   |\n",
    "| Uncertainty estimate   | Yes ($C_i$)                | No                         |\n",
    "| Exact solution         | Yes (after $N$ steps)      | No (approximate)           |\n",
    "\n",
    "---\n",
    "\n",
    "> **In summary:**  \n",
    "> The iterative GP algorithm is a principled, numerically stable way to compute both the predictive mean and uncertainty in Gaussian Process regression. Each iteration incrementally \"loads\" information from the data, making it a powerful alternative to standard optimization methods that only provide point estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da22ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "# --- Re-using kernel definition ---\n",
    "def squared_exponential_kernel(\n",
    "    x1: jnp.ndarray, x2: jnp.ndarray, sigma: float = 1.0, lengthscale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Computes the Squared Exponential (RBF) kernel matrix.\"\"\"\n",
    "    x1 = jnp.atleast_2d(x1)\n",
    "    x2 = jnp.atleast_2d(x2)\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    K = sigma**2 * jnp.exp(-0.5 * sq_dist / lengthscale**2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# --- Iterative GP Training Procedure (Algorithm 6) ---\n",
    "def iterative_gp_train(\n",
    "    K_matrix: jnp.ndarray,  # K_XX + sigma^2 I\n",
    "    y_bar: jnp.ndarray,  # y - mu_X\n",
    "    num_iterations: int,  # Corresponds to N for exact Cholesky\n",
    "    mean_func_train_data: Callable[\n",
    "        [jnp.ndarray], jnp.ndarray\n",
    "    ],  # Mean function at training points\n",
    "    X_train_data: jnp.ndarray,  # Original training inputs\n",
    "):\n",
    "    N = K_matrix.shape[0]\n",
    "    C_i = jnp.zeros((N, N), dtype=K_matrix.dtype)  # Inverse estimate\n",
    "    alpha_i = jnp.zeros(N, dtype=y_bar.dtype)  # Solution estimate\n",
    "\n",
    "    # Store intermediate results for plotting\n",
    "    all_alpha_estimates = [alpha_i]\n",
    "    all_C_estimates = [C_i]\n",
    "\n",
    "    print(\"--- Starting Iterative GP Training ---\")\n",
    "    for i in range(num_iterations):\n",
    "        # s_i: Action - load (here, canonical basis vector e_i)\n",
    "        s_i = jnp.eye(N, dtype=K_matrix.dtype)[i, :]\n",
    "\n",
    "        # z_i: Observation - compute (K @ s_i)\n",
    "        z_i = jnp.dot(K_matrix, s_i)\n",
    "\n",
    "        # d_i: Low-rank update (s_i - C_{i-1} @ z_i)\n",
    "        d_i = s_i - jnp.dot(C_i, z_i)\n",
    "\n",
    "        # eta_i: Schur complement (s_i^T @ K @ d_i = z_i^T @ d_i)\n",
    "        eta_i = jnp.dot(z_i.T, d_i)\n",
    "\n",
    "        # Add small jitter for numerical stability\n",
    "        eta_i_stable = eta_i + 1e-12\n",
    "\n",
    "        # C_i: Inverse estimate update\n",
    "        C_i_new = C_i + (1 / eta_i_stable) * jnp.outer(d_i, d_i)\n",
    "        C_i = C_i_new\n",
    "\n",
    "        # alpha_i: Solution estimate update\n",
    "        alpha_i_new = alpha_i + (1 / eta_i_stable) * jnp.dot(d_i, jnp.dot(d_i.T, y_bar))\n",
    "        alpha_i = alpha_i_new\n",
    "\n",
    "        all_alpha_estimates.append(alpha_i)\n",
    "        all_C_estimates.append(C_i)\n",
    "\n",
    "        if (i + 1) % (N // 5) == 0 or (i + 1) == N:  # Print progress\n",
    "            print(\n",
    "                f\"  Iteration {i + 1}/{N}: Max abs diff alpha vs true: {jnp.max(jnp.abs(alpha_i - jnp.linalg.solve(K_matrix, y_bar))):.2e}\"\n",
    "            )\n",
    "\n",
    "    print(\"--- Iterative GP Training Complete ---\")\n",
    "    return all_alpha_estimates, all_C_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GP Prediction Procedure (Algorithm 6, PREDICT) ---\n",
    "def iterative_gp_predict(\n",
    "    X_test: jnp.ndarray,\n",
    "    X_train_data: jnp.ndarray,\n",
    "    mean_func_test: Callable[[jnp.ndarray], jnp.ndarray],\n",
    "    kernel_func: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray],\n",
    "    alpha_final: jnp.ndarray,\n",
    "    C_final: jnp.ndarray,\n",
    ") -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Performs GP prediction using the final alpha and C estimates from iterative training.\n",
    "    \"\"\"\n",
    "    # k_xS: Covariance to Observations\n",
    "    k_xS = kernel_func(X_test, X_train_data)\n",
    "\n",
    "    # mu_x: Point estimate\n",
    "    mu_x = mean_func_test(X_test) + jnp.dot(k_xS, alpha_final)\n",
    "\n",
    "    # v_xx: Uncertainty\n",
    "    k_xx = kernel_func(X_test, X_test)\n",
    "    v_xx = k_xx - jnp.dot(k_xS, jnp.dot(C_final, k_xS.T))\n",
    "\n",
    "    return mu_x, v_xx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage (Simulated Data) ---\n",
    "key = random.PRNGKey(10)\n",
    "N_data = 20  # Number of training points for this example\n",
    "X_train_sim = jnp.linspace(-5, 5, N_data)[:, None]\n",
    "y_true = jnp.sin(X_train_sim) * jnp.exp(-0.1 * X_train_sim**2)\n",
    "y_train_sim = y_true.squeeze() + 0.1 * random.normal(key, (N_data,))\n",
    "\n",
    "# GP parameters\n",
    "mean_func_sim = lambda x: jnp.zeros(x.shape[0])\n",
    "kernel_func_sim = lambda x1, x2: squared_exponential_kernel(\n",
    "    x1, x2, sigma=1.0, lengthscale=1.0\n",
    ")\n",
    "noise_var_sim = 0.1**2\n",
    "\n",
    "# Precompute K_matrix and y_bar\n",
    "K_matrix_sim = kernel_func_sim(X_train_sim, X_train_sim) + noise_var_sim * jnp.eye(\n",
    "    N_data\n",
    ")\n",
    "y_bar_sim = y_train_sim - mean_func_sim(X_train_sim)\n",
    "\n",
    "# Run iterative training\n",
    "all_alpha_estimates, all_C_estimates = iterative_gp_train(\n",
    "    K_matrix_sim, y_bar_sim, N_data, mean_func_sim, X_train_sim\n",
    ")\n",
    "\n",
    "# Get final alpha and C estimates\n",
    "alpha_final_sim = all_alpha_estimates[-1]\n",
    "C_final_sim = all_C_estimates[-1]\n",
    "\n",
    "# Generate test points for prediction\n",
    "X_test_sim = jnp.linspace(-6, 6, 100)[:, None]\n",
    "\n",
    "# Perform prediction using the final estimates\n",
    "mu_pred_sim, cov_pred_sim = iterative_gp_predict(\n",
    "    X_test_sim,\n",
    "    X_train_sim,\n",
    "    mean_func_sim,\n",
    "    kernel_func_sim,\n",
    "    alpha_final_sim,\n",
    "    C_final_sim,\n",
    ")\n",
    "std_pred_sim = jnp.sqrt(jnp.diag(cov_pred_sim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create the main trace for the predictive mean\n",
    "trace_mean = go.Scatter(\n",
    "    x=X_test_sim.squeeze(),\n",
    "    y=mu_pred_sim,\n",
    "    mode=\"lines\",\n",
    "    name=\"GP Predictive Mean (Iterative)\",\n",
    "    line=dict(color=\"red\"),\n",
    ")\n",
    "\n",
    "# Create the confidence interval as a filled area\n",
    "trace_ci = go.Scatter(\n",
    "    x=jnp.concatenate([X_test_sim.squeeze(), X_test_sim.squeeze()[::-1]]),\n",
    "    y=jnp.concatenate(\n",
    "        [mu_pred_sim - 2 * std_pred_sim, (mu_pred_sim + 2 * std_pred_sim)[::-1]]\n",
    "    ),\n",
    "    fill=\"toself\",\n",
    "    fillcolor=\"rgba(255,0,0,0.2)\",\n",
    "    line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name=\"95% Confidence Interval\",\n",
    ")\n",
    "\n",
    "# Training data as scatter points\n",
    "trace_train = go.Scatter(\n",
    "    x=X_train_sim.squeeze(),\n",
    "    y=y_train_sim,\n",
    "    mode=\"markers\",\n",
    "    name=\"Training Data\",\n",
    "    marker=dict(color=\"blue\", size=7, opacity=0.8),\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Iterative Gaussian Process Regression\",\n",
    "    xaxis=dict(title=\"X\"),\n",
    "    yaxis=dict(title=\"Y\"),\n",
    "    legend=dict(x=0.01, y=0.99),\n",
    "    template=\"plotly_white\",\n",
    "    hovermode=\"closest\",\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace_ci, trace_mean, trace_train], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589935d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Iterative Posterior Updates (Conceptual) ---\n",
    "# This part visualizes how the posterior mean and variance evolve with each iteration.\n",
    "# It can be computationally intensive for many iterations/large N.\n",
    "# We'll plot a few intermediate steps.\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "num_plots = min(N_data, 5)  # Plot up to 5 intermediate steps + final\n",
    "plot_indices = jnp.linspace(0, N_data, num_plots, endpoint=True, dtype=int)\n",
    "\n",
    "plotly_figs = []\n",
    "for k, idx in enumerate(plot_indices):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "\n",
    "    current_alpha = all_alpha_estimates[idx]\n",
    "    current_C = all_C_estimates[idx]\n",
    "\n",
    "    # Predict with current estimates\n",
    "    mu_iter, cov_iter = iterative_gp_predict(\n",
    "        X_test_sim,\n",
    "        X_train_sim,\n",
    "        mean_func_sim,\n",
    "        kernel_func_sim,\n",
    "        current_alpha,\n",
    "        current_C,\n",
    "    )\n",
    "    std_iter = jnp.sqrt(jnp.diag(cov_iter))\n",
    "\n",
    "    # Plotly traces\n",
    "    trace_mean = go.Scatter(\n",
    "        x=X_test_sim.squeeze(),\n",
    "        y=mu_iter,\n",
    "        mode=\"lines\",\n",
    "        name=\"Posterior Mean\",\n",
    "        line=dict(color=\"red\"),\n",
    "    )\n",
    "    trace_ci = go.Scatter(\n",
    "        x=jnp.concatenate([X_test_sim.squeeze(), X_test_sim.squeeze()[::-1]]),\n",
    "        y=jnp.concatenate([mu_iter - 2 * std_iter, (mu_iter + 2 * std_iter)[::-1]]),\n",
    "        fill=\"toself\",\n",
    "        fillcolor=\"rgba(255,0,0,0.2)\",\n",
    "        line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "        hoverinfo=\"skip\",\n",
    "        showlegend=True,\n",
    "        name=\"95% Confidence Interval\",\n",
    "    )\n",
    "    trace_train_processed = go.Scatter(\n",
    "        x=X_train_sim[:idx, 0],\n",
    "        y=y_train_sim[:idx],\n",
    "        mode=\"markers\",\n",
    "        name=\"Processed Data\",\n",
    "        marker=dict(color=\"blue\", size=8, opacity=0.9),\n",
    "    )\n",
    "    traces = [trace_ci, trace_mean, trace_train_processed]\n",
    "\n",
    "    if idx < N_data:\n",
    "        trace_train_unprocessed = go.Scatter(\n",
    "            x=X_train_sim[idx:, 0],\n",
    "            y=y_train_sim[idx:],\n",
    "            mode=\"markers\",\n",
    "            name=\"Unprocessed Data\",\n",
    "            marker=dict(color=\"gray\", size=7, opacity=0.5),\n",
    "        )\n",
    "        traces.append(trace_train_unprocessed)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=f\"Iterative GP Posterior Updates<br>Iteration {idx}/{N_data}\",\n",
    "        xaxis=dict(title=\"X\"),\n",
    "        yaxis=dict(title=\"Y\"),\n",
    "        legend=dict(x=0.01, y=0.99),\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"closest\",\n",
    "        height=400,\n",
    "        width=700,\n",
    "    )\n",
    "    fig_iter = go.Figure(data=traces, layout=layout)\n",
    "    plotly_figs.append(fig_iter)\n",
    "    fig_iter.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605f510",
   "metadata": {},
   "source": [
    "# Why Load Individual Data Points? Generalization to Projections\n",
    "\n",
    "In the iterative Cholesky algorithm (Algorithm 6), we used the **canonical basis vectors** $s_i = e_i$ (where $e_i$ is a vector with $1$ at the $i$-th position and $0$ elsewhere). This corresponds to sequentially \"loading\" individual data points into our computation.\n",
    "\n",
    "---\n",
    "\n",
    "## How Does the Algorithm Interact with the Data?\n",
    "\n",
    "The core operations in each iteration are:\n",
    "\n",
    "- $z_i = K s_i$  \n",
    "    *This computes a column of the kernel matrix $K$ if $s_i = e_i$.*\n",
    "\n",
    "- $y_i = d_i^\\top y$  \n",
    "    *This computes a component of the residual vector.*\n",
    "\n",
    "**Key Insight:**  \n",
    "These are both **linear projections** of the data!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Is This Profound?\n",
    "\n",
    "- The Cholesky algorithm, in this context, is not just a matrix factorization.  \n",
    "- It is a process of **sequentially loading information** from individual data points (or, more precisely, their corresponding columns in the kernel matrix).\n",
    "- Each step treats these as new \"observations\" and performs a stable, rank-1 update to both the inverse and the solution.\n",
    "\n",
    "---\n",
    "\n",
    "## Generalizing Beyond Individual Data Points\n",
    "\n",
    "This realization opens up powerful new directions for scalable and flexible GP inference:\n",
    "\n",
    "### 1. **Batch Loading**\n",
    "- Instead of $s_i \\in \\mathbb{R}^N$ being a single canonical basis vector, we could use $s_i \\in \\mathbb{R}^{N \\times b}$, representing a **batch of $b$ vectors**.\n",
    "- This leads to **rank-$b$ updates** at each step, potentially speeding up convergence and making better use of modern hardware.\n",
    "\n",
    "### 2. **Random Projections**\n",
    "- Instead of loading specific data points or batches, we could project the data onto **random directions**.\n",
    "- This is the foundation of **random feature approximations** in Gaussian Processes, which can dramatically reduce computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "## The Role of the \"Policy\" for $s_i$\n",
    "\n",
    "- The **choice of policy** for selecting $s_i$ (which vectors to load at each step) becomes a **critical design decision** in scalable GP algorithms.\n",
    "- Different choices can lead to different trade-offs between computational efficiency, statistical accuracy, and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**\n",
    "\n",
    "| Approach                | $s_i$ Choice                | Update Type   | Example Use Case                  |\n",
    "|-------------------------|-----------------------------|--------------|-----------------------------------|\n",
    "| Sequential (classic)    | $e_i$ (canonical basis)     | Rank-1       | Exact Cholesky, classic GP        |\n",
    "| Batch                   | Multiple $e_i$'s            | Rank-$b$     | Mini-batch Cholesky, scalable GP  |\n",
    "| Random Projections      | Random directions           | Rank-$b$     | Random features, fast GP approx.  |\n",
    "\n",
    "---\n",
    "\n",
    "> **In summary:**  \n",
    "> The iterative Cholesky algorithm is more than just a numerical routineâ€”it is a *framework for loading and processing information from data*. By generalizing the way we select and load $s_i$, we can design more scalable and flexible Gaussian Process algorithms that are well-suited to modern machine learning challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd803abf",
   "metadata": {},
   "source": [
    "# IterGP: Generalized Iterative Gaussian Process Regression Algorithm\n",
    "\n",
    "The **IterGP algorithm** (Wenger, Pleiss, PfÃ¶rtner, Hennig, Cunningham, NeurIPS 2022) generalizes iterative GP regression by allowing **flexible projections**. Instead of always using the canonical basis vectors $e_i$, it introduces a **POLICY** function to select the projection vectors $s_i$ at each step.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ **Algorithm 7: Iterative GP Regression (Numerics Layer â€” Generalized)**\n",
    "\n",
    "### **Key Idea**\n",
    "\n",
    "- Instead of always loading one data point at a time (using $e_i$), we can load **arbitrary projections** $s_i$ at each step.\n",
    "- The choice of $s_i$ is governed by a **policy**, which can be designed for efficiency, scalability, or statistical optimality.\n",
    "\n",
    "---\n",
    "\n",
    "### **Inputs**\n",
    "\n",
    "- $K = k_{XX} + \\sigma^2 I$  \n",
    "  *Kernel matrix with noise (size $N \\times N$).*\n",
    "- $\\bar{y} = y - \\mu_X$  \n",
    "  *Centered targets (subtract mean function at training points).*\n",
    "- Initial guesses: $\\alpha_0$, $C_0$  \n",
    "  *Typically zeros.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Outputs**\n",
    "\n",
    "- $S = [s_j]_{j \\leq n}$  \n",
    "  *List of all projection vectors used (each $s_j \\in \\mathbb{R}^{N \\times k_j}$).*\n",
    "- $C_n \\approx K^{-1}$  \n",
    "  *Approximate inverse of the kernel matrix after $n$ iterations.*\n",
    "- $\\alpha_n \\approx K^{-1} \\bar{y}$  \n",
    "  *Approximate solution (posterior mean weights).*\n",
    "\n",
    "---\n",
    "\n",
    "### **IterGP Update Steps**\n",
    "\n",
    "At each iteration $i = 1, \\ldots, n$:\n",
    "\n",
    "1. **Select Projection(s):**\n",
    "   $$\n",
    "   s_i \\leftarrow \\text{POLICY}(S_{<i}, Z_{<i})\n",
    "   $$\n",
    "   - $s_i \\in \\mathbb{R}^{N \\times k_i}$ (can be a single vector or a batch).\n",
    "   - The policy can use all previous projections $S_{<i}$ and kernel columns $Z_{<i}$.\n",
    "\n",
    "2. **Compute Projected Kernel Column(s):**\n",
    "   $$\n",
    "   z_i = K s_i\n",
    "   $$\n",
    "   - $z_i \\in \\mathbb{R}^{N \\times k_i}$.\n",
    "\n",
    "3. **Compute Low-Rank Update Direction:**\n",
    "   $$\n",
    "   d_i = (I - C_{i-1} K) s_i = s_i - C_{i-1} z_i\n",
    "   $$\n",
    "   - $d_i \\in \\mathbb{R}^{N \\times k_i}$.\n",
    "   - This is the \"new information\" not yet explained by previous steps.\n",
    "\n",
    "4. **Compute Schur Complement (Normalization Matrix):**\n",
    "   $$\n",
    "   H_i = s_i^\\top K d_i = z_i^\\top d_i\n",
    "   $$\n",
    "   - $H_i \\in \\mathbb{R}^{k_i \\times k_i}$.\n",
    "   - Ensures numerical stability and proper scaling.\n",
    "\n",
    "5. **Update Inverse Estimate (Rank-$k_i$ Update):**\n",
    "   $$\n",
    "   C_i = C_{i-1} + d_i H_i^{-1} d_i^\\top\n",
    "   $$\n",
    "   - $C_i$ is the new approximation to $K^{-1}$.\n",
    "\n",
    "6. **Update Solution Estimate (Posterior Mean Weights):**\n",
    "   $$\n",
    "   \\alpha_i = \\alpha_{i-1} + d_i H_i^{-1} d_i^\\top \\bar{y}\n",
    "   $$\n",
    "   - $\\alpha_i$ is the new approximation to $K^{-1} \\bar{y}$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table of IterGP Steps**\n",
    "\n",
    "| Step | Operation | Purpose |\n",
    "|------|-----------|---------|\n",
    "| 1    | $s_i \\leftarrow \\text{POLICY}(S_{<i}, Z_{<i})$ | Choose projection(s) |\n",
    "| 2    | $z_i = K s_i$ | Project kernel |\n",
    "| 3    | $d_i = s_i - C_{i-1} z_i$ | Find unexplained direction |\n",
    "| 4    | $H_i = z_i^\\top d_i$ | Normalize (Schur complement) |\n",
    "| 5    | $C_i = C_{i-1} + d_i H_i^{-1} d_i^\\top$ | Update inverse estimate |\n",
    "| 6    | $\\alpha_i = \\alpha_{i-1} + d_i H_i^{-1} d_i^\\top \\bar{y}$ | Update solution estimate |\n",
    "\n",
    "---\n",
    "\n",
    "### **Prediction Step (Same as Standard GP)**\n",
    "\n",
    "Given a new test point $x$:\n",
    "\n",
    "- **Covariance to Projections:**\n",
    "  $$\n",
    "  k_{xS} = k(x, S)\n",
    "  $$\n",
    "- **Predictive Mean:**\n",
    "  $$\n",
    "  \\mu_x = \\mu(x) + k_{xS} \\alpha\n",
    "  $$\n",
    "- **Predictive Variance:**\n",
    "  $$\n",
    "  v_{xx} = k_{xx} - k_{xS} C k_{Sx}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition & Practical Impact**\n",
    "\n",
    "- **Flexibility:**  \n",
    "  The policy for $s_i$ can be tailored: sequential, batch, random, or conjugate directions (Lanczos/CG).\n",
    "- **Scalability:**  \n",
    "  Batch or random projections can dramatically speed up convergence and leverage modern hardware.\n",
    "- **Uncertainty Quantification:**  \n",
    "  At every step, $C_i$ provides an up-to-date uncertainty estimate, not just a point prediction.\n",
    "\n",
    "---\n",
    "\n",
    "> **In summary:**  \n",
    "> The IterGP framework unifies and generalizes iterative GP regression. By allowing flexible, policy-driven projections, it enables scalable, efficient, and uncertainty-aware inference for modern probabilistic machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16ce91",
   "metadata": {},
   "source": [
    "# What is the Optimal Projection?  \n",
    "## Data Loading as a Training Policy\n",
    "\n",
    "The **choice of POLICY** for selecting the projection vectors $s_i$ is crucial in iterative Gaussian Process (GP) regression. Ideally, we want to choose projections that are **maximally informative** at each step, leading to the fastest reduction in the error of our inverse and solution estimates.\n",
    "\n",
    "---\n",
    "\n",
    "### The Ideal Case: Eigenvectors of $K$\n",
    "\n",
    "Suppose we could choose the projections $s_i = u_i$ along the **eigenvectors** of the kernel matrix $K = U \\Lambda U^\\top$, where $U$ contains the eigenvectors and $\\Lambda$ contains the eigenvalues. The iterative algorithm would then simplify beautifully:\n",
    "\n",
    "- **Projection:**  \n",
    "    $$\n",
    "    z_i = K s_i = U \\Lambda U^\\top u_i = \\lambda_i u_i\n",
    "    $$\n",
    "- **Residual Update:**  \n",
    "    $$\n",
    "    d_i = (I - C_{i-1} K) s_i = u_i - \\lambda_i C_{i-1} u_i\n",
    "    $$\n",
    "    If $C_{i-1}$ is the inverse of the first $i-1$ eigencomponents, then $d_i = u_i$.\n",
    "- **Schur Complement:**  \n",
    "    $$\n",
    "    H_i = s_i^\\top K d_i = u_i^\\top U \\Lambda U^\\top u_i = \\lambda_i\n",
    "    $$\n",
    "- **Inverse Update:**  \n",
    "    $$\n",
    "    C_i = C_{i-1} + d_i H_i^{-1} d_i^\\top = C_{i-1} + u_i \\lambda_i^{-1} u_i^\\top = \\sum_{j \\leq i} u_j \\lambda_j^{-1} u_j^\\top\n",
    "    $$\n",
    "- **Solution Estimate:**  \n",
    "    $$\n",
    "    \\alpha = C_i y = \\sum_{j \\leq i} u_j \\lambda_j^{-1} (u_j^\\top y)\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "### Why Is This Optimal?\n",
    "\n",
    "If the $u_i$ are sorted by decreasing $\\lambda_i$, this policy will **maximally reduce the residual** $\\lvert K^{-1}y - \\alpha \\rvert_{K^2}$ in each iteration.  \n",
    "- The largest eigenvalues correspond to the most dominant modes of variation in the kernel.\n",
    "- Incorporating them first provides the most significant reduction in uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "### The Catch\n",
    "\n",
    "However, we **do not have the eigenvectors of $K$ available upfront**. Computing them is itself an $\\mathcal{O}(N^3)$ operation, which defeats the purpose of an iterative, scalable algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### The Solution: Iterative Methods\n",
    "\n",
    "This is where **iterative methods** like the **Lanczos process** come into play. These methods can efficiently approximate the dominant eigenvectors and eigenvalues of $K$ without computing the full eigendecomposition, enabling scalable and effective projection policies for large-scale GP inference.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary Table: Projection Choices in Iterative GP Regression**\n",
    "\n",
    "| Approach                | $s_i$ Choice                | Update Type   | Example Use Case                  |\n",
    "|-------------------------|-----------------------------|--------------|-----------------------------------|\n",
    "| Sequential (classic)    | $e_i$ (canonical basis)     | Rank-1       | Exact Cholesky, classic GP        |\n",
    "| Batch                   | Multiple $e_i$'s            | Rank-$b$     | Mini-batch Cholesky, scalable GP  |\n",
    "| Random Projections      | Random directions           | Rank-$b$     | Random features, fast GP approx.  |\n",
    "| **Optimal (theoretical)** | Eigenvectors of $K$         | Rank-1       | Fastest convergence (in theory)   |\n",
    "\n",
    "---\n",
    "\n",
    "> **In summary:**  \n",
    "> The optimal projection policy would use the eigenvectors of $K$, but this is computationally infeasible for large $N$. Iterative methods like Lanczos provide practical alternatives, allowing us to approximate these optimal directions and achieve efficient, scalable GP inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a6a34",
   "metadata": {},
   "source": [
    "# The Lanczos Process: Iterative Construction of Conjugate Projections\n",
    "\n",
    "When working with large symmetric matrices (like the kernel matrix $K$ in Gaussian Processes), directly computing their eigenvectors is computationally infeasible. However, we still want to construct \"good\" projection vectors $s_i$ that help us efficiently solve linear systems or approximate matrix functions. This is where the **Lanczos process** comes in.\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Lanczos Process?\n",
    "\n",
    "The **Lanczos process** (KornÃ©l LÃ¡nczos, 1950) is an iterative algorithm that builds an **orthonormal basis** for the **Krylov subspace** associated with a symmetric matrix $K$ and an initial vector $s_0$.\n",
    "\n",
    "- **Krylov Subspace:**  \n",
    "    For a matrix $K$ and vector $s_0$, the Krylov subspace of order $n$ is:\n",
    "    $$\n",
    "    \\mathcal{K}_n(K, s_0) = \\text{span}\\{s_0, Ks_0, K^2s_0, \\ldots, K^{n-1}s_0\\}\n",
    "    $$\n",
    "\n",
    "- **Goal:**  \n",
    "    Construct a sequence of vectors $s_1, s_2, \\ldots, s_N$ that are **orthonormal** and **K-conjugate** (i.e., $s_i^\\top K s_j = 0$ for $i \\neq j$).\n",
    "\n",
    "---\n",
    "\n",
    "## How Does the Lanczos Process Work?\n",
    "\n",
    "- **Initialization:**  \n",
    "    Start with a normalized vector $s_0$.\n",
    "\n",
    "- **Iteration:**  \n",
    "    At each step, generate a new vector by applying $K$ to the previous vector, then orthogonalize it against the previous two vectors. This ensures the new vector is orthogonal (in the $K$-inner product sense) to all previous ones.\n",
    "\n",
    "- **Result:**  \n",
    "    After $N$ steps, you obtain a set of vectors $S = [s_1, s_2, \\ldots, s_N]$ such that:\n",
    "    $$\n",
    "    S^\\top K S = T\n",
    "    $$\n",
    "    where $T$ is a **tridiagonal matrix** with diagonal entries $\\alpha_i$ and sub-diagonal entries $\\beta_i$.\n",
    "\n",
    "---\n",
    "\n",
    "## The Tridiagonal Matrix $T$\n",
    "\n",
    "The matrix $T$ has the following structure:\n",
    "$$\n",
    "T = \n",
    "\\begin{pmatrix}\n",
    "\\alpha_1 & \\beta_2 & 0 & \\cdots & 0 \\\\\n",
    "\\beta_2 & \\alpha_2 & \\beta_3 & \\ddots & \\vdots \\\\\n",
    "0 & \\beta_3 & \\alpha_3 & \\ddots & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\beta_N \\\\\n",
    "0 & \\cdots & 0 & \\beta_N & \\alpha_N\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- $S = [s_1, s_2, \\ldots, s_N]$ is the matrix of Lanczos vectors.\n",
    "- $\\alpha_i$ are the diagonal elements.\n",
    "- $\\beta_i$ are the sub-diagonal elements.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is the Lanczos Process Important?\n",
    "\n",
    "- **Efficient Projections:**  \n",
    "    The Lanczos vectors $s_i$ are \"good\" directions for iterative algorithms like Conjugate Gradients, as they are K-orthogonal and span the most relevant subspace for solving $K\\alpha = y$.\n",
    "- **Dimensionality Reduction:**  \n",
    "    By working in the Krylov subspace, we can approximate solutions to large linear systems or eigenvalue problems using only a small number of vectors.\n",
    "- **Foundation for CG:**  \n",
    "    The Conjugate Gradient (CG) method is essentially the Lanczos process applied to solving linear systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The Lanczos process provides a principled way to iteratively construct an orthonormal basis of projections for a symmetric matrix $K$.\n",
    "- These projections are **K-orthogonal** (conjugate), making them ideal for efficient numerical algorithms in Gaussian Processes and other areas of scientific computing.\n",
    "- The process transforms $K$ into a much simpler tridiagonal matrix $T$, capturing the essential structure needed for computation.\n",
    "\n",
    "---\n",
    "\n",
    "> **In essence:**  \n",
    "> The Lanczos process is a powerful tool for scalable computation with large symmetric matrices, enabling efficient iterative algorithms for both mean and uncertainty estimation in probabilistic models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c82f1d",
   "metadata": {},
   "source": [
    "### Simplified Lanczos Process (Algorithm)\n",
    "\n",
    "The **Lanczos process** is an efficient iterative algorithm for constructing an orthonormal basis of the Krylov subspace for a symmetric matrix $K$ and an initial vector $s_0$. It is widely used in numerical linear algebra for eigenvalue problems and for solving large linear systems (e.g., in Conjugate Gradients).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Algorithm: Simplified Lanczos Process**\n",
    "\n",
    "**Input:**  \n",
    "- Symmetric matrix $K \\in \\mathbb{R}^{N \\times N}$\n",
    "- Initial vector $s_0 \\in \\mathbb{R}^N$\n",
    "\n",
    "**Output:**  \n",
    "- Sequences of scalars $\\alpha_i$, $\\beta_i$\n",
    "- Lanczos vectors $s_i$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Algorithm (with Math and Intuition)**\n",
    "\n",
    "1. **Initialization**\n",
    "  - Normalize the initial vector:\n",
    "    $$\n",
    "    s_1 = \\frac{s_0}{\\|s_0\\|}\n",
    "    $$\n",
    "    *Start with a unit-norm vector $s_1$.*\n",
    "\n",
    "2. **First Iteration**\n",
    "  - Compute the first matrix-vector product:\n",
    "    $$\n",
    "    z_1 = K s_1\n",
    "    $$\n",
    "    *Apply $K$ to the starting vector.*\n",
    "\n",
    "  - Compute the first diagonal element:\n",
    "    $$\n",
    "    \\alpha_1 = s_1^\\top z_1 = s_1^\\top K s_1\n",
    "    $$\n",
    "    *$\\alpha_1$ captures how much $K$ stretches $s_1$ along itself.*\n",
    "\n",
    "  - Compute the first residual:\n",
    "    $$\n",
    "    l_1 = z_1 - \\alpha_1 s_1\n",
    "    $$\n",
    "    *Remove the component of $z_1$ along $s_1$ to get the part orthogonal to $s_1$.*\n",
    "\n",
    "3. **Subsequent Iterations ($i = 2, \\ldots, N$)**\n",
    "  - Compute the sub-diagonal element:\n",
    "    $$\n",
    "    \\beta_i = \\|l_{i-1}\\|\n",
    "    $$\n",
    "    *$\\beta_i$ measures the norm of the new direction; if it's zero, the process terminates.*\n",
    "\n",
    "  - Normalize to get the next Lanczos vector:\n",
    "    $$\n",
    "    s_i = \\frac{l_{i-1}}{\\beta_i}\n",
    "    $$\n",
    "    *$s_i$ is orthogonal to all previous $s_j$.*\n",
    "\n",
    "  - Apply $K$ to the new vector:\n",
    "    $$\n",
    "    z_i = K s_i\n",
    "    $$\n",
    "\n",
    "  - Compute the next diagonal element:\n",
    "    $$\n",
    "    \\alpha_i = s_i^\\top z_i = s_i^\\top K s_i\n",
    "    $$\n",
    "    *$\\alpha_i$ is the Rayleigh quotient for $s_i$.*\n",
    "\n",
    "  - Compute the new residual:\n",
    "    $$\n",
    "    l_i = z_i - \\alpha_i s_i - \\beta_i s_{i-1}\n",
    "    $$\n",
    "    *Subtract projections onto $s_i$ and $s_{i-1}$ to maintain orthogonality.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table of Steps**\n",
    "\n",
    "| Step | Formula | Explanation |\n",
    "|------|---------|-------------|\n",
    "| 1 | $s_1 = \\frac{s_0}{\\|s_0\\|}$ | Normalize initial vector |\n",
    "| 2 | $z_1 = K s_1$ | Matrix-vector product |\n",
    "| 3 | $\\alpha_1 = s_1^\\top K s_1$ | Diagonal entry of tridiagonal matrix |\n",
    "| 4 | $l_1 = z_1 - \\alpha_1 s_1$ | Orthogonalize against $s_1$ |\n",
    "| 5 | $\\beta_i = \\|l_{i-1}\\|$ | Sub-diagonal entry (norm) |\n",
    "| 6 | $s_i = \\frac{l_{i-1}}{\\beta_i}$ | Next orthonormal vector |\n",
    "| 7 | $z_i = K s_i$ | Matrix-vector product |\n",
    "| 8 | $\\alpha_i = s_i^\\top K s_i$ | Diagonal entry |\n",
    "| 9 | $l_i = z_i - \\alpha_i s_i - \\beta_i s_{i-1}$ | Orthogonalize against previous two vectors |\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition**\n",
    "\n",
    "- **Orthonormal Basis:** Each $s_i$ is orthogonal to all previous $s_j$ (for $j < i$), forming a basis for the Krylov subspace $\\mathcal{K}_n(K, s_0)$.\n",
    "- **Tridiagonalization:** The process builds a tridiagonal matrix $T$ such that $S^\\top K S = T$, where $S = [s_1, \\ldots, s_n]$.\n",
    "- **Efficiency:** Only requires matrix-vector products and inner productsâ€”no full matrix inversion or eigendecomposition.\n",
    "- **Termination:** If $\\beta_i = 0$, the process has found an invariant subspace (exact for $K$).\n",
    "\n",
    "---\n",
    "\n",
    "> **In summary:**  \n",
    "> The Lanczos process is a powerful, efficient way to extract the most \"informative\" directions for $K$ using only matrix-vector products, making it essential for scalable Gaussian Process inference and large-scale linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f12d62",
   "metadata": {},
   "source": [
    "## Key Points: The Lanczos Process in Iterative GP Inference\n",
    "\n",
    "- **Computational Cost:**  \n",
    "    - Each iteration of the Lanczos process requires a single matrix-vector product $K s_i$, which is $\\mathcal{O}(N^2)$ for a dense matrix $K$.\n",
    "    - If $K$ is sparse or admits a fast matrix-vector product (e.g., via structure or approximation), this cost can be significantly reduced.\n",
    "\n",
    "- **Numerical Stability:**  \n",
    "    - The basic Lanczos algorithm, as presented, is a simplified version and can become numerically unstable for long sequences due to loss of orthogonality among the generated vectors.\n",
    "    - In practice, **re-orthogonalization techniques** are used to maintain numerical stability and ensure the orthogonality of the Lanczos vectors.\n",
    "\n",
    "- **Combination with IterGP:**  \n",
    "    - The Lanczos process can be seamlessly integrated with the iterative GP algorithm (Algorithm 7).\n",
    "    - The **POLICY** function in IterGP can be implemented by generating Lanczos vectors at each step.\n",
    "    - This integration adds only one extra line of $\\mathcal{O}(N)$ (or $\\mathcal{O}(N^2)$ for the matrix-vector product) per iteration for selecting $s_i$.\n",
    "\n",
    "- **Conjugate Gradients (CG):**  \n",
    "    - A particularly important choice for the initial vector $s_0$ is the **residual of the linear system**:\n",
    "      $$\n",
    "      s_0 = K \\alpha_0 - y\n",
    "      $$\n",
    "      or, more commonly for the gradient of the quadratic form,\n",
    "      $$\n",
    "      s_0 = y - K \\alpha_0\n",
    "      $$\n",
    "    - This choice leads to the **Conjugate Gradient (CG) method**, a powerful iterative solver for symmetric positive definite linear systems.\n",
    "    - CG implicitly constructs Lanczos vectors and is widely used for large-scale problems due to its efficiency and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "The Lanczos process is a foundational tool for efficient iterative inference in Gaussian Processes. By leveraging fast matrix-vector products and careful numerical techniques, it enables scalable computation of both the mean and uncertainty in GP regression. When combined with the IterGP framework, it provides a principled and practical approach to large-scale probabilistic machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np  # For random initial vector\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def conceptual_lanczos_process(\n",
    "    K_matrix: jnp.ndarray, s0: jnp.ndarray, num_iterations: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Conceptual implementation of the Lanczos process.\n",
    "    This version is simplified and may be numerically unstable for many iterations.\n",
    "    \"\"\"\n",
    "    N = K_matrix.shape[0]\n",
    "    if num_iterations > N:\n",
    "        num_iterations = N  # Cannot generate more than N Lanczos vectors\n",
    "\n",
    "    # Initialize lists to store alpha and beta values\n",
    "    alphas = []\n",
    "    betas = []\n",
    "\n",
    "    # Store Lanczos vectors (orthonormal basis)\n",
    "    S_vectors = []\n",
    "\n",
    "    # Step 1: Initialize\n",
    "    s_prev = s0 / jnp.linalg.norm(s0)  # Normalize initial vector\n",
    "    S_vectors.append(s_prev)\n",
    "\n",
    "    z_curr = jnp.dot(K_matrix, s_prev)\n",
    "    alpha_curr = jnp.dot(s_prev.T, z_curr)\n",
    "    alphas.append(alpha_curr)\n",
    "\n",
    "    l_curr = z_curr - alpha_curr * s_prev\n",
    "\n",
    "    print(\"--- Starting Conceptual Lanczos Process ---\")\n",
    "    for i in range(1, num_iterations):\n",
    "        beta_curr = jnp.linalg.norm(l_curr)\n",
    "\n",
    "        if beta_curr < 1e-10:  # Break if l_curr is zero (exact subspace found)\n",
    "            print(f\"  Converged at iteration {i} (beta is near zero).\")\n",
    "            break\n",
    "\n",
    "        s_curr = l_curr / beta_curr\n",
    "        S_vectors.append(s_curr)\n",
    "        betas.append(beta_curr)\n",
    "\n",
    "        z_next = jnp.dot(K_matrix, s_curr)\n",
    "        alpha_next = jnp.dot(s_curr.T, z_next)\n",
    "        alphas.append(alpha_next)\n",
    "\n",
    "        l_next = (\n",
    "            z_next - alpha_next * s_curr - beta_curr * s_prev\n",
    "        )  # Note: beta_curr is beta_{i+1}\n",
    "        l_curr = l_next\n",
    "        s_prev = s_curr\n",
    "\n",
    "        print(f\"  Iteration {i + 1}: alpha={alpha_next:.4f}, beta={beta_curr:.4f}\")\n",
    "\n",
    "    # Construct the tridiagonal matrix T for verification\n",
    "    T = jnp.diag(jnp.array(alphas))\n",
    "    if len(betas) > 0:\n",
    "        T += jnp.diag(jnp.array(betas), k=1)\n",
    "        T += jnp.diag(jnp.array(betas), k=-1)\n",
    "\n",
    "    # Convert list of vectors to a matrix\n",
    "    S_matrix = jnp.stack(S_vectors, axis=1)\n",
    "\n",
    "    print(\"\\n--- Lanczos Process Results ---\")\n",
    "    print(\"Tridiagonal Matrix T:\\n\", T)\n",
    "    print(\n",
    "        \"\\nVerification: S^T @ K @ S (should be close to T):\\n\",\n",
    "        jnp.dot(S_matrix.T, jnp.dot(K_matrix, S_matrix)),\n",
    "    )\n",
    "\n",
    "    return alphas, betas, S_vectors\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Create a symmetric positive definite matrix\n",
    "N_matrix = 10\n",
    "X_data_matrix = jnp.linspace(-2, 2, N_matrix)[:, None]\n",
    "K_XX_matrix = squared_exponential_kernel(\n",
    "    X_data_matrix, X_data_matrix, sigma=1.0, lengthscale=0.5\n",
    ")\n",
    "A_example = K_XX_matrix + 0.1**2 * jnp.eye(N_matrix)  # Add jitter for SPD\n",
    "\n",
    "# Random initial vector\n",
    "key = random.PRNGKey(2)\n",
    "s0_example = random.normal(key, (N_matrix,))\n",
    "\n",
    "# Run Lanczos process for a few iterations\n",
    "num_lanczos_iterations = 5\n",
    "alphas, betas, S_vectors = conceptual_lanczos_process(\n",
    "    A_example, s0_example, num_lanczos_iterations\n",
    ")\n",
    "\n",
    "# Plot the Lanczos vectors (basis functions)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, s_vec in enumerate(S_vectors):\n",
    "    plt.plot(jnp.arange(N_matrix), s_vec, label=f\"s_{i + 1}\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"First few Lanczos Vectors\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "# --- Re-using kernel definition ---\n",
    "def squared_exponential_kernel(\n",
    "    x1: jnp.ndarray, x2: jnp.ndarray, sigma: float = 1.0, lengthscale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Computes the Squared Exponential (RBF) kernel matrix.\"\"\"\n",
    "    x1 = jnp.atleast_2d(x1)\n",
    "    x2 = jnp.atleast_2d(x2)\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    K = sigma**2 * jnp.exp(-0.5 * sq_dist / lengthscale**2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# --- Iterative GP Training Procedure (Algorithm 7, adapted for CG) ---\n",
    "def iterative_gp_train_cg(\n",
    "    K_matrix: jnp.ndarray,  # K_XX + sigma^2 I\n",
    "    y_bar: jnp.ndarray,  # y - mu_X\n",
    "    num_iterations: int,\n",
    "    mean_func_train_data: Callable[[jnp.ndarray], jnp.ndarray],\n",
    "    X_train_data: jnp.ndarray,\n",
    "):\n",
    "    N = K_matrix.shape[0]\n",
    "\n",
    "    # Initialize alpha and C (from Algorithm 7, with alpha_0=0, C_0=0)\n",
    "    alpha_i = jnp.zeros(N, dtype=y_bar.dtype)\n",
    "    C_i = jnp.zeros((N, N), dtype=K_matrix.dtype)\n",
    "\n",
    "    # Conjugate Gradient specific initializations\n",
    "    r_i = y_bar - jnp.dot(\n",
    "        K_matrix, alpha_i\n",
    "    )  # Initial residual (gradient of L(alpha) at alpha_0)\n",
    "    p_i = r_i  # Initial search direction\n",
    "\n",
    "    # Store intermediate results for plotting\n",
    "    all_alpha_estimates = [alpha_i]\n",
    "    all_C_estimates = [C_i]\n",
    "    all_residual_norms = [jnp.linalg.norm(r_i)]\n",
    "\n",
    "    print(\"--- Starting Iterative GP Training (Conjugate Gradients) ---\")\n",
    "    for i in range(num_iterations):\n",
    "        # This is where the 'POLICY' for s_i comes in.\n",
    "        # For CG, s_i is the search direction p_i.\n",
    "        s_i = p_i\n",
    "\n",
    "        # z_i = K @ s_i (matrix-vector product)\n",
    "        z_i = jnp.dot(K_matrix, s_i)\n",
    "\n",
    "        # d_i: Low-rank update (s_i - C_{i-1} @ z_i)\n",
    "        # For CG, this is related to the update direction.\n",
    "        # In the context of Algorithm 7, d_i is the part of s_i that is K-orthogonal to previous s_j.\n",
    "        # For CG, the search directions p_i are K-orthogonal by construction.\n",
    "        # Here, we'll use the definition from Algorithm 7 for C_i and alpha_i updates.\n",
    "        d_i = s_i - jnp.dot(\n",
    "            C_i, z_i\n",
    "        )  # This d_i is equivalent to p_i if C_i is the inverse of the subspace.\n",
    "\n",
    "        # eta_i: Schur complement (s_i^T @ K @ d_i = z_i^T @ d_i)\n",
    "        eta_i = jnp.dot(z_i.T, d_i)  # This will be p_i^T @ K @ p_i for CG\n",
    "\n",
    "        # Add small jitter for numerical stability\n",
    "        eta_i_stable = eta_i + 1e-12\n",
    "\n",
    "        # Alpha update (from CG algorithm)\n",
    "        alpha_step = jnp.dot(r_i.T, r_i) / eta_i_stable\n",
    "        alpha_i_new = alpha_i + alpha_step * p_i\n",
    "        alpha_i = alpha_i_new\n",
    "\n",
    "        # Residual update (from CG algorithm)\n",
    "        r_i_new = r_i - alpha_step * z_i\n",
    "\n",
    "        # Beta for next search direction (from CG algorithm)\n",
    "        beta_next = jnp.dot(r_i_new.T, r_i_new) / jnp.dot(r_i.T, r_i)\n",
    "\n",
    "        # Update search direction\n",
    "        p_i_new = r_i_new + beta_next * p_i\n",
    "\n",
    "        r_i = r_i_new\n",
    "        p_i = p_i_new\n",
    "\n",
    "        # C_i: Inverse estimate update (from Algorithm 7)\n",
    "        # This update is O(N^2) and is what makes the C_i matrix grow.\n",
    "        # In practical CG, C_i is not explicitly formed.\n",
    "        C_i_new = C_i + (1 / eta_i_stable) * jnp.outer(d_i, d_i)\n",
    "        C_i = C_i_new\n",
    "\n",
    "        all_alpha_estimates.append(alpha_i)\n",
    "        all_C_estimates.append(C_i)\n",
    "        all_residual_norms.append(jnp.linalg.norm(r_i))\n",
    "\n",
    "        if (i + 1) % (N // 5) == 0 or (i + 1) == N:\n",
    "            print(f\"  Iteration {i + 1}/{N}: Residual norm: {jnp.linalg.norm(r_i):.2e}\")\n",
    "            if jnp.linalg.norm(r_i) < 1e-6:\n",
    "                print(\"  Residual norm very small, likely converged.\")\n",
    "                break\n",
    "\n",
    "    print(\"--- Iterative GP Training (Conjugate Gradients) Complete ---\")\n",
    "    return all_alpha_estimates, all_C_estimates, all_residual_norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GP Prediction Procedure (Algorithm 6, PREDICT) ---\n",
    "# Re-using the same prediction function as before\n",
    "def iterative_gp_predict(\n",
    "    X_test: jnp.ndarray,\n",
    "    X_train_data: jnp.ndarray,\n",
    "    mean_func_test: Callable[[jnp.ndarray], jnp.ndarray],\n",
    "    kernel_func: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray],\n",
    "    alpha_final: jnp.ndarray,\n",
    "    C_final: jnp.ndarray,\n",
    ") -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Performs GP prediction using the final alpha and C estimates from iterative training.\n",
    "    \"\"\"\n",
    "    k_xS = kernel_func(X_test, X_train_data)\n",
    "    mu_x = mean_func_test(X_test) + jnp.dot(k_xS, alpha_final)\n",
    "    k_xx = kernel_func(X_test, X_test)\n",
    "    v_xx = k_xx - jnp.dot(k_xS, jnp.dot(C_final, k_xS.T))\n",
    "    return mu_x, v_xx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18782451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage (Simulated Data) ---\n",
    "key = random.PRNGKey(11)\n",
    "N_data = 20  # Number of training points for this example\n",
    "X_train_sim = jnp.linspace(-5, 5, N_data)[:, None]\n",
    "y_true = jnp.sin(X_train_sim) * jnp.exp(-0.1 * X_train_sim**2)\n",
    "y_train_sim = y_true.squeeze() + 0.1 * random.normal(key, (N_data,))\n",
    "\n",
    "# GP parameters\n",
    "mean_func_sim = lambda x: jnp.zeros(x.shape[0])\n",
    "kernel_func_sim = lambda x1, x2: squared_exponential_kernel(\n",
    "    x1, x2, sigma=1.0, lengthscale=1.0\n",
    ")\n",
    "noise_var_sim = 0.1**2\n",
    "\n",
    "# Precompute K_matrix and y_bar\n",
    "K_matrix_sim = kernel_func_sim(X_train_sim, X_train_sim) + noise_var_sim * jnp.eye(\n",
    "    N_data\n",
    ")\n",
    "y_bar_sim = y_train_sim - mean_func_sim(X_train_sim)\n",
    "\n",
    "# Run iterative training with CG-like updates\n",
    "all_alpha_estimates_cg, all_C_estimates_cg, all_residual_norms_cg = (\n",
    "    iterative_gp_train_cg(K_matrix_sim, y_bar_sim, N_data, mean_func_sim, X_train_sim)\n",
    ")\n",
    "\n",
    "# Get final alpha and C estimates\n",
    "alpha_final_cg = all_alpha_estimates_cg[-1]\n",
    "C_final_cg = all_C_estimates_cg[-1]\n",
    "\n",
    "# Generate test points for prediction\n",
    "X_test_sim = jnp.linspace(-6, 6, 100)[:, None]\n",
    "\n",
    "# Perform prediction using the final estimates\n",
    "mu_pred_cg, cov_pred_cg = iterative_gp_predict(\n",
    "    X_test_sim, X_train_sim, mean_func_sim, kernel_func_sim, alpha_final_cg, C_final_cg\n",
    ")\n",
    "std_pred_cg = jnp.sqrt(jnp.diag(cov_pred_cg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e5b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create the main trace for the predictive mean (CG Iterative)\n",
    "trace_mean_cg = go.Scatter(\n",
    "    x=X_test_sim.squeeze(),\n",
    "    y=mu_pred_cg,\n",
    "    mode=\"lines\",\n",
    "    name=\"GP Predictive Mean (CG Iterative)\",\n",
    "    line=dict(color=\"red\"),\n",
    ")\n",
    "\n",
    "# Create the confidence interval as a filled area\n",
    "trace_ci_cg = go.Scatter(\n",
    "    x=jnp.concatenate([X_test_sim.squeeze(), X_test_sim.squeeze()[::-1]]),\n",
    "    y=jnp.concatenate(\n",
    "        [mu_pred_cg - 2 * std_pred_cg, (mu_pred_cg + 2 * std_pred_cg)[::-1]]\n",
    "    ),\n",
    "    fill=\"toself\",\n",
    "    fillcolor=\"rgba(255,0,0,0.2)\",\n",
    "    line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name=\"95% Confidence Interval\",\n",
    ")\n",
    "\n",
    "# Training data as scatter points\n",
    "trace_train_cg = go.Scatter(\n",
    "    x=X_train_sim.squeeze(),\n",
    "    y=y_train_sim,\n",
    "    mode=\"markers\",\n",
    "    name=\"Training Data\",\n",
    "    marker=dict(color=\"blue\", size=7, opacity=0.8),\n",
    ")\n",
    "\n",
    "layout_cg = go.Layout(\n",
    "    title=\"Iterative Gaussian Process Regression with Conjugate Gradients\",\n",
    "    xaxis=dict(title=\"X\"),\n",
    "    yaxis=dict(title=\"Y\"),\n",
    "    legend=dict(x=0.01, y=0.99),\n",
    "    template=\"plotly_white\",\n",
    "    hovermode=\"closest\",\n",
    ")\n",
    "\n",
    "fig_cg = go.Figure(data=[trace_ci_cg, trace_mean_cg, trace_train_cg], layout=layout_cg)\n",
    "fig_cg.show()\n",
    "\n",
    "# Plot the residual norm convergence (log scale)\n",
    "fig_residual = go.Figure()\n",
    "fig_residual.add_trace(\n",
    "    go.Scatter(\n",
    "        x=jnp.arange(len(all_residual_norms_cg)),\n",
    "        y=all_residual_norms_cg,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Residual Norm\",\n",
    "        line=dict(color=\"purple\"),\n",
    "    )\n",
    ")\n",
    "fig_residual.update_layout(\n",
    "    title=\"Convergence of Residual Norm in CG-like Iteration\",\n",
    "    xaxis_title=\"Iteration\",\n",
    "    yaxis_title=\"Residual Norm (log scale)\",\n",
    "    yaxis_type=\"log\",\n",
    "    template=\"plotly_white\",\n",
    "    hovermode=\"closest\",\n",
    ")\n",
    "fig_residual.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd002529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Iterative Posterior Updates (Conceptual) ---\n",
    "# This part visualizes how the posterior mean and variance evolve with each iteration.\n",
    "# It can be computationally intensive for many iterations/large N.\n",
    "# We'll plot a few intermediate steps.\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Plot up to 5 intermediate steps + final\n",
    "num_plots = min(len(all_alpha_estimates_cg), 5)\n",
    "plot_indices = jnp.linspace(\n",
    "    0, len(all_alpha_estimates_cg) - 1, num_plots, endpoint=True, dtype=int\n",
    ")\n",
    "\n",
    "plotly_figs_cg = []\n",
    "for k, idx in enumerate(plot_indices):\n",
    "    current_alpha = all_alpha_estimates_cg[idx]\n",
    "    current_C = all_C_estimates_cg[idx]\n",
    "\n",
    "    # Predict with current estimates\n",
    "    mu_iter, cov_iter = iterative_gp_predict(\n",
    "        X_test_sim,\n",
    "        X_train_sim,\n",
    "        mean_func_sim,\n",
    "        kernel_func_sim,\n",
    "        current_alpha,\n",
    "        current_C,\n",
    "    )\n",
    "    std_iter = jnp.sqrt(jnp.diag(cov_iter))\n",
    "\n",
    "    # Plotly traces\n",
    "    trace_mean = go.Scatter(\n",
    "        x=X_test_sim.squeeze(),\n",
    "        y=mu_iter,\n",
    "        mode=\"lines\",\n",
    "        name=\"Posterior Mean\",\n",
    "        line=dict(color=\"red\"),\n",
    "    )\n",
    "    trace_ci = go.Scatter(\n",
    "        x=jnp.concatenate([X_test_sim.squeeze(), X_test_sim.squeeze()[::-1]]),\n",
    "        y=jnp.concatenate([mu_iter - 2 * std_iter, (mu_iter + 2 * std_iter)[::-1]]),\n",
    "        fill=\"toself\",\n",
    "        fillcolor=\"rgba(255,0,0,0.2)\",\n",
    "        line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "        hoverinfo=\"skip\",\n",
    "        showlegend=True,\n",
    "        name=\"95% Confidence Interval\",\n",
    "    )\n",
    "    trace_train = go.Scatter(\n",
    "        x=X_train_sim.squeeze(),\n",
    "        y=y_train_sim,\n",
    "        mode=\"markers\",\n",
    "        name=\"Training Data\",\n",
    "        marker=dict(color=\"blue\", size=7, opacity=0.8),\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=f\"Iterative GP Posterior Updates (CG)<br>Iteration {idx}/{len(all_alpha_estimates_cg) - 1}\",\n",
    "        xaxis=dict(title=\"X\"),\n",
    "        yaxis=dict(title=\"Y\"),\n",
    "        legend=dict(x=0.01, y=0.99),\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"closest\",\n",
    "        height=400,\n",
    "        width=700,\n",
    "    )\n",
    "    fig_iter = go.Figure(data=[trace_ci, trace_mean, trace_train], layout=layout)\n",
    "    plotly_figs_cg.append(fig_iter)\n",
    "    fig_iter.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbadddf8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fea939b",
   "metadata": {},
   "source": [
    "# ðŸš€ Takeaways: Iterative GP Updates with Lanczos\n",
    "\n",
    "The synergy between **iterative Gaussian Process (GP) algorithms** and **projection methods** like the **Lanczos process** (which underpins Conjugate Gradients) offers both deep theoretical insights and practical computational advantages.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Projection-Based Interaction\n",
    "\n",
    "- **Beyond Individual Data Points:**  \n",
    "    Instead of loading one data point at a time, we can load **linear projections** of the data.  \n",
    "    This flexibility allows for more efficient and informative updates in each iteration.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Optimal Projections with Lanczos\n",
    "\n",
    "- **Maximally Informative Directions:**  \n",
    "    The **Lanczos process** efficiently constructs projections that are \"maximally informative\"â€”they reduce the residual of the linear system as quickly as possible.\n",
    "- **K-Orthogonality:**  \n",
    "    These projections are **K-orthogonal** (conjugate with respect to the kernel matrix $K$), which is ideal for optimization and ensures rapid convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Native Mean and Uncertainty Quantification\n",
    "\n",
    "- **Matching Mean and Uncertainty:**  \n",
    "    By tracking projections and associated statistics (like $C_i$ and $\\alpha_i$), we can compute both the **predictive mean** and **uncertainty** natively.\n",
    "- **No Trade-off:**  \n",
    "    This means we do **not** have to sacrifice uncertainty quantification for scalabilityâ€”both are achieved together.\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Halving Compute Time (for Exact Solve)\n",
    "\n",
    "- **Complexity:**  \n",
    "    Exact methods like Cholesky and Conjugate Gradients (CG) have $\\mathcal{O}(N^3)$ complexity in the worst case.\n",
    "- **Practical Speed:**  \n",
    "    However, **CG often converges to a good solution in far fewer than $N$ iterations**, making it much faster for large datasets in practice.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ¤– Computation and Inference: There Is Really No Difference\n",
    "\n",
    "This lecture culminates in a profound realization:\n",
    "\n",
    "> **For Gaussian Processes, \"computing\" and \"learning\" are fundamentally the same. Numerical algorithms are, in essence, learning machines.**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¢ Least-Squares as Linear Algebra\n",
    "\n",
    "- In **least-squares regression** (equivalent to the GP posterior mean), \"training\" reduces to solving a linear system $A x = b$.\n",
    "- The solution is a direct outcome of linear algebraic computation.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ï¸ Numerical Methods as Data Loaders\n",
    "\n",
    "- **Cholesky decomposition** and **Conjugate Gradients** can be viewed as **smart data loaders**:\n",
    "        - They load projections of the data in a specific, efficient order.\n",
    "        - For example:\n",
    "                - Choosing $s_i = e_{j(i)}$ (canonical basis vectors with a pivoting policy $j(i)$) yields the **pivoted Cholesky decomposition**.\n",
    "                - The **Lanczos process**, initialized with $s_0 = K \\alpha_0 - y$ (the gradient of the objective), yields the **preconditioned Conjugate Gradient (CG) method**. Here, $C_0$ acts as a preconditioner (an initial guess for $K^{-1}$).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Prior Guesses as Initializers\n",
    "\n",
    "- Think of $\\alpha_0$ and $C_0$ as **prior guesses** for the solution $\\alpha$ and the inverse $K^{-1}$.\n",
    "- The iterative algorithms then **refine these estimates** step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Bayesian Interpretations of Iterative Solvers\n",
    "\n",
    "- There are **deep Bayesian interpretations** of these iterative solvers:\n",
    "        - The point estimates for $\\alpha$ and $K^{-1}$ can be seen as **posterior means**.\n",
    "        - The associated uncertainty can also be quantified (see Hennig, 2015; Wenger & Hennig, 2021; Hennig, Osborne, Kersting, 2022).\n",
    "- This further **blurs the line** between numerical optimization and Bayesian inference.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Summary\n",
    "\n",
    "- The **choice of numerical algorithm** is not just an implementation detailâ€”it fundamentally shapes how the model \"learns\" from data and quantifies uncertainty.\n",
    "- **Computation = Inference:**  \n",
    "    In GPs, the act of computation is inseparable from the act of learning.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“– **References for Further Reading**\n",
    "\n",
    "- Hennig, P. (2015). Probabilistic Interpretation of Linear Solvers. *SIAM Journal on Optimization*.\n",
    "- Wenger, J., & Hennig, P. (2021). Probabilistic Linear Solvers for Machine Learning. *Proceedings of the IEEE*.\n",
    "- Hennig, P., Osborne, M. A., & Kersting, H. (2022). Probabilistic Numerics: Computation as Machine Learning. *Cambridge University Press*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db050f",
   "metadata": {},
   "source": [
    "# IterGP (Final Algorithm with Initial Guesses)\n",
    "\n",
    "This is the most general form of the IterGP algorithm, incorporating initial guesses for the inverse $C_0$ and solution $\\alpha_0$.\n",
    "\n",
    "## Algorithm 8: Iterative GP Regression (Numerics Layer)\n",
    "\n",
    "**Input:** Sufficient statistics $K = k_{XX} + \\sigma^2 I$, $\\bar{y} = y - \\mu_X$, initial guesses $\\alpha_0$, $C_0$\n",
    "\n",
    "**Output:** Defragmented statistics $S$ (set of processed vectors), $C_n \\approx K^{-1}$, $\\alpha_n \\approx K^{-1}y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a66ed",
   "metadata": {},
   "source": [
    "\n",
    "### IterGP Training Procedure: Mathematical Formulation and Step-by-Step Explanation\n",
    "\n",
    "The IterGP algorithm iteratively refines estimates of the inverse covariance and solution vector in Gaussian Process regression. Below, we present the procedure as a sequence of mathematical updates, with clear explanations for each step.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Initialization**\n",
    "\n",
    "- **Inverse Estimate:**  \n",
    "    $$\n",
    "    C_0 \\leftarrow \\text{initial guess (often } 0 \\text{)}\n",
    "    $$\n",
    "- **Solution Estimate:**  \n",
    "    $$\n",
    "    \\alpha_0 \\leftarrow \\text{initial guess (often } 0 \\text{)}\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Iterative Updates (for $i = 1$ to $n$):**\n",
    "\n",
    "1. **Select Projection Vector(s) (Action/Policy):**\n",
    "     $$\n",
    "     s_i \\leftarrow \\text{POLICY}(S_{<i}, Z_{<i})\n",
    "     $$\n",
    "     - $s_i \\in \\mathbb{R}^{N \\times k_i}$  \n",
    "     - *Choose the next direction(s) to load, possibly using previous projections and observations.*\n",
    "\n",
    "2. **Compute Projected Kernel Column(s) (Observation):**\n",
    "     $$\n",
    "     z_i = K s_i\n",
    "     $$\n",
    "     - $z_i \\in \\mathbb{R}^{N \\times k_i}$  \n",
    "     - *Apply the kernel matrix to the projection(s).*\n",
    "\n",
    "3. **Compute Low-Rank Update Direction:**\n",
    "     $$\n",
    "     d_i = (I - C_{i-1} K) s_i = s_i - C_{i-1} z_i\n",
    "     $$\n",
    "     - $d_i \\in \\mathbb{R}^{N \\times k_i}$  \n",
    "     - *Find the component of $s_i$ not yet explained by previous updates.*\n",
    "\n",
    "4. **Compute Schur Complement (Normalization Matrix):**\n",
    "     $$\n",
    "     H_i = s_i^\\top K d_i = z_i^\\top d_i\n",
    "     $$\n",
    "     - $H_i \\in \\mathbb{R}^{k_i \\times k_i}$  \n",
    "     - *Normalization for numerical stability and correct scaling.*\n",
    "\n",
    "5. **Update Inverse Estimate:**\n",
    "     $$\n",
    "     C_i = C_{i-1} + d_i H_i^{-1} d_i^\\top\n",
    "     $$\n",
    "     - *Rank-$k_i$ update to the inverse estimate.*\n",
    "\n",
    "6. **Update Solution Estimate:**\n",
    "     $$\n",
    "     \\alpha_i = \\alpha_{i-1} + d_i H_i^{-1} d_i^\\top \\bar{y}\n",
    "     $$\n",
    "     - *Refine the solution vector using the new direction.*\n",
    "\n",
    "---\n",
    "\n",
    "#### **Return**\n",
    "\n",
    "- **All Projections:** $S = [s_j]_{j \\leq n}$\n",
    "- **Final Solution Estimate:** $\\alpha_n$\n",
    "- **Final Inverse Estimate:** $C_n$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step-by-Step Intuition**\n",
    "\n",
    "- **Step 1:** *Choose the next direction(s) to load information from the data. This can be a single data point, a batch, or a projection (e.g., from the Lanczos process).*\n",
    "- **Step 2:** *Compute how the kernel \"sees\" this directionâ€”i.e., the corresponding column(s) of the kernel matrix.*\n",
    "- **Step 3:** *Determine what part of this direction is new (not already explained by previous steps).*\n",
    "- **Step 4:** *Calculate a normalization factor to ensure updates are stable and properly scaled.*\n",
    "- **Step 5:** *Update the running estimate of the inverse covariance matrix using a low-rank correction.*\n",
    "- **Step 6:** *Update the solution vector (posterior mean weights) using the new information.*\n",
    "\n",
    "---\n",
    "\n",
    "This formulation provides a flexible, modular, and scalable framework for iterative Gaussian Process regression, enabling both mean and uncertainty estimation at every step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f157f",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Inputs:**\n",
    "    - $x$: Test point(s) where we want to make predictions.\n",
    "    - $S$: Set of processed training vectors (could be data points or projections).\n",
    "    - $\\alpha$: Solution vector (posterior weights), typically $\\alpha = K^{-1}(y - \\mu_X)$.\n",
    "    - $C$: Inverse covariance estimate, typically $C \\approx K^{-1}$.\n",
    "\n",
    "- **Steps:**\n",
    "    1. **Covariance to Observations:**  \n",
    "        $k[x, S]$ computes the similarity between $x$ and each vector in $S$ using the kernel function.\n",
    "    2. **Predictive Mean:**  \n",
    "        $\\mu_x = \\mu(x) + k[x, S]\\, \\alpha$  \n",
    "        This gives the expected value of the function at $x$ given the observed data.\n",
    "    3. **Predictive Variance:**  \n",
    "        $v_{xx} = k_{xx} - k[x, S]\\, C\\, k[S, x]$  \n",
    "        This quantifies the model's uncertainty at $x$, accounting for both prior uncertainty and information gained from the data.\n",
    "\n",
    "---\n",
    "\n",
    "This algorithm forms the basis for highly scalable Gaussian Process (GP) methods, where the **POLICY** function can be designed to efficiently explore the data and approximate the necessary quantities. By updating $S$, $\\alpha$, and $C$ iteratively, we can make fast, uncertainty-aware predictions at any test location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea62c23",
   "metadata": {},
   "source": [
    "# Relationship to Gradient Descent\n",
    "\n",
    "Let's explore the connection and key differences between **iterative Cholesky/Conjugate Gradient (CG) methods** and **Gradient Descent (GD)** in the context of Gaussian Process (GP) regression.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Setup: GP Posterior Mean as a Quadratic Minimization\n",
    "\n",
    "Recall that the solution $\\alpha^\\star$ for the GP posterior mean is the minimizer of the following quadratic objective:\n",
    "\n",
    "$$\n",
    "\\alpha^\\star = \\arg\\min_{\\alpha \\in \\mathbb{R}^N} L(\\alpha) = \\arg\\min_{\\alpha \\in \\mathbb{R}^N} \\frac{1}{2} \\alpha^\\top (K_{XX} + \\sigma^2 I_N) \\alpha - (y - \\mu_X)^\\top \\alpha\n",
    "$$\n",
    "\n",
    "- $K_{XX}$: Kernel matrix for the training data\n",
    "- $\\sigma^2 I_N$: Noise term (diagonal matrix)\n",
    "- $y$: Observed targets\n",
    "- $\\mu_X$: Mean function evaluated at training points\n",
    "\n",
    "The **analytical solution** is:\n",
    "\n",
    "$$\n",
    "\\alpha^\\star = (K_{XX} + \\sigma^2 I_N)^{-1} (y - \\mu_X)\n",
    "$$\n",
    "\n",
    "The **Jacobian** of this solution with respect to $y$ is:\n",
    "\n",
    "$$\n",
    "\\frac{d\\alpha^\\star}{dy} = (K_{XX} + \\sigma^2 I_N)^{-1}\n",
    "$$\n",
    "\n",
    "This Jacobian is also the **posterior covariance** in GP regression.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent (GD) Update Rule\n",
    "\n",
    "Gradient Descent iteratively updates the parameter vector $\\alpha$ using the gradient of the loss:\n",
    "\n",
    "$$\n",
    "\\alpha_{t+1} = \\alpha_t - \\eta \\nabla_\\alpha L(\\alpha_t)\n",
    "$$\n",
    "\n",
    "where the gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\alpha L(\\alpha) = (K_{XX} + \\sigma^2 I_N)\\alpha - (y - \\mu_X)\n",
    "$$\n",
    "\n",
    "- $\\eta$: Learning rate (step size)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences: Cholesky/CG vs. Gradient Descent\n",
    "\n",
    "### 1. **Nature of the Update**\n",
    "\n",
    "- **Cholesky / Conjugate Gradients (CG):**\n",
    "    - Iteratively compute the **total derivative** (i.e., the matrix inverse).\n",
    "    - Aim to find the **exact solution** to the linear system in a finite number of steps (at most $N$ for CG, exactly $N$ for Cholesky).\n",
    "    - Provide both the **full inverse** $C_i$ and the **exact solution** $\\alpha_i$ (up to numerical precision).\n",
    "    - Directly enable **uncertainty quantification** (posterior covariance).\n",
    "\n",
    "- **Gradient Descent (GD):**\n",
    "    - Computes a **partial derivative** (the gradient) at each step and moves in that direction.\n",
    "    - Typically does **not converge in finite time**; requires many iterations.\n",
    "    - Only provides a **point estimate** (the mean), without direct access to uncertainty (the inverse covariance).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Convergence and Computational Cost**\n",
    "\n",
    "- **Cholesky/CG:**\n",
    "    - **Converge in finite time** (at most $N$ steps for CG).\n",
    "    - Each step is more computationally expensive ($\\mathcal{O}(N^2)$ per iteration), but the total number of steps is bounded.\n",
    "    - Suitable for problems where **exact solutions and uncertainty estimates** are required.\n",
    "\n",
    "- **Gradient Descent:**\n",
    "    - **Requires many iterations** to converge, especially if the problem is ill-conditioned.\n",
    "    - Each step is computationally cheap ($\\mathcal{O}(N)$ per iteration for vector operations).\n",
    "    - Does **not provide uncertainty**; only gives a point estimate.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Uncertainty Quantification**\n",
    "\n",
    "- **Cholesky/CG:**  \n",
    "    - By constructing the inverse (or an approximation), these methods provide **direct access to the posterior covariance**â€”a key feature of GPs.\n",
    "\n",
    "- **Gradient Descent:**  \n",
    "    - Only tracks the mean; **uncertainty information is lost** unless additional (often expensive) computations are performed.\n",
    "\n",
    "---\n",
    "\n",
    "## Implications and Trade-offs\n",
    "\n",
    "- **Cholesky/CG** methods are powerful for **exact inference** and **uncertainty quantification** in GPs, but are more computationally intensive per iteration.\n",
    "- **Gradient Descent** is scalable and simple, but sacrifices **exactness** and **uncertainty** for speed and memory efficiency.\n",
    "\n",
    "> **Summary Table**\n",
    "\n",
    "| Method            | Convergence      | Per-Step Cost | Uncertainty | Exact Solution | Typical Use Case                |\n",
    "|-------------------|------------------|---------------|-------------|---------------|---------------------------------|\n",
    "| Cholesky          | Finite ($N$)     | $\\mathcal{O}(N^2)$ | Yes         | Yes           | Small/medium GPs, exact stats   |\n",
    "| Conjugate Gradient| Finite ($\\leq N$)| $\\mathcal{O}(N^2)$ | Yes         | Yes           | Large GPs, iterative inference  |\n",
    "| Gradient Descent  | Infinite         | $\\mathcal{O}(N)$   | No          | No            | Large-scale, point estimation   |\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Summary\n",
    "\n",
    "- **Cholesky/CG:**  \n",
    "    ![Cholesky/CG: Exact solution in finite steps, with uncertainty](https://raw.githubusercontent.com/your-repo/cholesky-cg-visual.png)\n",
    "- **Gradient Descent:**  \n",
    "    ![GD: Gradual convergence, no uncertainty](https://raw.githubusercontent.com/your-repo/gd-visual.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Takeaway\n",
    "\n",
    "The choice between these methods reflects a **trade-off between computational efficiency, convergence guarantees, and the richness of information** (point estimate vs. full posterior) provided by the algorithm. For Gaussian Processes, **iterative linear solvers like CG bridge the gap**â€”offering both scalability and uncertainty quantification, which are central to probabilistic machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5ffdc",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Final Takeaways: Computation and Learning in Gaussian Processes\n",
    "\n",
    "This lecture has illuminated the profound connections between **computation** and **inference** in probabilistic machine learning, with a special focus on **Gaussian Processes (GPs)**. Here are the key insights:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® \"Training\" is Linear Algebra\n",
    "\n",
    "- In **least-squares regression** (and thus in GP posterior mean estimation), \"training\" is fundamentally a **linear algebra problem**.\n",
    "- Specifically, it reduces to **solving a linear system**:\n",
    "    $$\n",
    "    \\alpha^\\star = (K_{XX} + \\sigma^2 I)^{-1}(y - \\mu_X)\n",
    "    $$\n",
    "- This means that the process of learning from data is, at its core, a computational task rooted in matrix operations.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Numerical Methods as Data Loaders\n",
    "\n",
    "- **Numerical algorithms** such as **Cholesky decomposition** and **Conjugate Gradients (CG)** can be viewed as **smart data loaders**.\n",
    "- These methods **iteratively load projections of the data** (e.g., columns of the kernel matrix or linear combinations thereof) in a specific, efficient order.\n",
    "- Each iteration refines our estimates of both the **solution** (the mean) and the **uncertainty** (the covariance), making the process a form of learning.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ No Separation of Computing and Learning\n",
    "\n",
    "- For Gaussian Processes, there is **no fundamental separation** between \"computing\" and \"learning.\"\n",
    "- **Numerical algorithms are learning machines**:  \n",
    "        - They process information from the data step by step.\n",
    "        - They iteratively refine both the solution and our uncertainty about it.\n",
    "- This perspective unifies the concepts of computation and inference, showing that **learning is computation, and computation is learning**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Why Does This Matter?\n",
    "\n",
    "- **Scalability:**  \n",
    "        Understanding this connection is crucial for scaling probabilistic models to large datasets.\n",
    "- **Algorithm Design:**  \n",
    "        It inspires the development of new, efficient algorithms that bridge the gap between **exact Bayesian inference** and the **practical demands of big data**.\n",
    "- **Uncertainty Quantification:**  \n",
    "        Unlike many optimization methods, these algorithms provide not just point estimates, but also **uncertainty quantification**â€”a hallmark of probabilistic machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "> **In summary:**  \n",
    "> The act of \"training\" a Gaussian Process is not just about fitting a modelâ€”it's about performing computation in a way that is inherently probabilistic and iterative. By embracing this perspective, we can design algorithms that are both efficient and principled, bringing together the best of computation and learning.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
