{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Probabilistic Machine Learning: Lecture 25 - A Historical Perspective\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Welcome to the final lecture of Probabilistic Machine Learning! Having explored the theoretical foundations, key models, and powerful algorithms of probabilistic machine learning, we now take a step back to appreciate the **historical context** that shaped this fascinating field. This lecture will trace some of the intellectual lineages and pivotal moments, highlighting how core ideas emerged, evolved, and converged across different scientific disciplines and challenging historical periods.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. The Probabilistic Machine Learning Toolbox (Recap)\n",
    "\n",
    "Before diving into history, let's briefly revisit the comprehensive **toolbox** we've assembled throughout this course (Slide 3):\n",
    "\n",
    "* **Framework**: The bedrock of probabilistic reasoning – sum rule, product rule, and Bayes' Theorem – enabling us to describe all inference tasks by assigning probabilities.\n",
    "* **Modeling**: Diverse tools for constructing models:\n",
    "    * **Directed Graphical Models**: For representing conditional independencies and joint distributions.\n",
    "    * **Exponential Families**: A class of distributions allowing for tractable inference and conjugate priors.\n",
    "    * **Gaussian Distributions**: The workhorse of linear algebra-based inference.\n",
    "    * **Kernels**: For abstracting inner products and enabling nonparametric models like Gaussian Processes.\n",
    "    * **Markov Chains**: For modeling time series with local memory.\n",
    "    * **Deep Networks**: Flexible function approximators, which we've learned can be viewed through a probabilistic lens (e.g., as GPs via Laplace approximation).\n",
    "* **Computation**: Algorithms for performing inference and learning:\n",
    "    * **Autodiff**: For efficient gradient computation in optimization.\n",
    "    * **MAP with Laplace approximations**: For approximating intractable posteriors with Gaussians.\n",
    "    * **Linear algebra as a computational primitive**: The cornerstone of Gaussian inference.\n",
    "    * **Variational Inference**: For approximating intractable posteriors by optimizing a lower bound (ELBO).\n",
    "    * **Monte Carlo**: For approximating integrals and expectations through sampling.\n",
    "\n",
    "These tools, developed over decades by brilliant minds, form the foundation of modern probabilistic machine learning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. A Historical Story: Lemberg / Lwów / Львів (Slides 4-9)\n",
    "\n",
    "The history of mathematics and science is often intertwined with geopolitical events. One poignant story comes from **Lemberg (Lwów / Львів)**, a city with a rich intellectual heritage that endured immense suffering in the 20th century. This city was home to the **Lwów School of Mathematics**, a vibrant community of Polish mathematicians in the interwar period.\n",
    "\n",
    "Central to this school was the **Scottish Cafe**, where mathematicians like **Stefan Banach, Stanisław Ulam, and Hugo Steinhaus** would meet, discuss problems, and even write them down in a notebook known as the **Scottish Book**. These informal gatherings fostered an environment of intense collaboration and groundbreaking discoveries.\n",
    "\n",
    "**Hugo Steinhaus (1887-1972)**, a key figure of the Lwów School and a PhD student of David Hilbert, is particularly relevant to our course. He was a pioneer in functional analysis and probability theory. During the \"Third Reich\" and the horrors of World War II, many Jewish scientists, including some from Lwów, were forced into hiding or fled. Tragically, the **Lemberger Professorenmorde (Massacre of Lwów Professors)** in July 1941 saw the execution of many Polish intellectuals, including mathematicians, by Nazi forces.\n",
    "\n",
    "Despite these dark times, some, like **John von Neumann (1903-1957)** and **Stanisław Ulam (1909-1984)**, made it out alive, emigrating to the United States in the 1930s. Their contributions, particularly at institutions like Los Alamos, would profoundly impact science and technology, including the development of the atomic bomb and the birth of modern computing.\n",
    "\n",
    "This historical backdrop reminds us that scientific progress is a human endeavor, often pursued amidst challenging circumstances, and that the abstract ideas we study have deep roots in the lives and experiences of their creators."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. k-Means Clustering: An Early Method and its Convergence (Slides 10, 13-14)\n",
    "\n",
    "One of the oldest and most widely used clustering methods, **k-Means**, was proposed by **Hugo Steinhaus** in 1957. It's a simple, iterative algorithm for partitioning $N$ data points into $K$ clusters.\n",
    "\n",
    "**Algorithm:**\n",
    "1.  **Initialization**: Randomly choose $K$ initial cluster means (centroids) $\\{m_k\\}_{k=1}^K$.\n",
    "2.  **Assignment (E-step-like)**: Assign each data point $x_i$ to the cluster whose mean $m_k$ is closest to it (e.g., using Euclidean distance). This can be represented by binary responsibilities $r_{ik} = 1$ if $x_i$ is assigned to cluster $k$, and $0$ otherwise.\n",
    "    $$k_i = \\arg \\min_k ||x_i - m_k||^2$$\n",
    "3.  **Update (M-step-like)**: Update each cluster mean $m_k$ to be the sample mean of all data points assigned to that cluster.\n",
    "    $$m_k \\leftarrow \\frac{\\sum_i r_{ik} x_i}{\\sum_i r_{ik}}$$\n",
    "4.  **Repeat**: Iterate steps 2 and 3 until the cluster assignments no longer change.\n",
    "\n",
    "A remarkable property of k-Means is that it **always converges**. This is due to the existence of a **Lyapunov Function** (Slide 13). A Lyapunov function $J$ for an iterative algorithm is a positive function of the algorithm's state variables that decreases in each step. The existence of such a function guarantees convergence to a local (not necessarily global) minimum of $J$.\n",
    "\n",
    "For k-Means, the objective function is the sum of squared distances of each point to its assigned cluster mean:\n",
    "$$J(r, m) := \\sum_n \\sum_k r_{nk} \\frac{1}{2} ||x_n - m_k||^2$$\n",
    "Both the assignment step (by definition, each point is assigned to its *nearest* mean) and the update step (by setting the mean to the sample mean, which minimizes the sum of squared errors for a fixed set of points) guarantee that $J(r, m)$ is non-increasing. Since $J$ is bounded below (it's a sum of squared distances), it must converge.\n",
    "\n",
    "Let's implement a simple k-Means algorithm using JAX."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from jax.scipy.stats import multivariate_normal as mvn\n",
    "from jax.scipy.special import digamma, gammaln # For Dirichlet and Wishart expectations\n",
    "\n",
    "# Set JAX to use 64-bit floats for numerical stability\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# --- Utility Functions (from previous lectures, adapted) ---\n",
    "def plot_gmm_plotly(X, means, covariances, responsibilities=None, title=\"Gaussian Mixture Model\", colors=['red', 'blue', 'green', 'purple', 'orange']):\n",
    "    \"\"\"Plots 2D data, GMM components, and optionally responsibilities.\"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot data points, optionally colored by responsibility\n",
    "    if responsibilities is not None:\n",
    "        dominant_component = jnp.argmax(responsibilities, axis=1)\n",
    "        for k in range(means.shape[0]):\n",
    "            mask = dominant_component == k\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X[mask, 0],\n",
    "                y=X[mask, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(color=colors[k % len(colors)], size=5, opacity=0.7),\n",
    "                name=f'Data (Component {k+1})',\n",
    "                showlegend=True\n",
    "            ))\n",
    "    else:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X[:, 0],\n",
    "            y=X[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(color='gray', size=5, opacity=0.7),\n",
    "            name='Data Points',\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "    # Plot Gaussian components (mean and covariance ellipses)\n",
    "    for k in range(means.shape[0]):\n",
    "        mean = means[k]\n",
    "        cov = covariances[k]\n",
    "        \n",
    "        # Draw ellipse representing 2-sigma contour\n",
    "        vals, vecs = jnp.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        vals = vals[order]\n",
    "        vecs = vecs[:, order]\n",
    "        \n",
    "        theta = jnp.degrees(jnp.arctan2(*vecs[:, 0][::-1]))\n",
    "        width, height = 2 * jnp.sqrt(5.991 * vals) # 5.991 for 95% confidence for 2D Chi-squared with 2 DOF\n",
    "\n",
    "        fig.add_shape(\n",
    "            type='circle',\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            x0=mean[0] - width / 2,\n",
    "            y0=mean[1] - height / 2,\n",
    "            x1=mean[0] + width / 2,\n",
    "            y1=mean[1] + height / 2,\n",
    "            line=dict(color=colors[k % len(colors)], width=2),\n",
    "            opacity=0.8,\n",
    "            layer='below',\n",
    "            name=f'Component {k+1}'\n",
    "        )\n",
    "        # Add mean point\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[mean[0]],\n",
    "            y=[mean[1]],\n",
    "            mode='markers',\n",
    "            marker=dict(symbol='x', size=10, color=colors[k % len(colors)], line=dict(width=2, color='black')),\n",
    "            name=f'Mean {k+1}',\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(title_text=title, title_x=0.5,\n",
    "                      xaxis_title='Feature 1',\n",
    "                      yaxis_title='Feature 2',\n",
    "                      autosize=False, width=700, height=600)\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1) # Keep aspect ratio square\n",
    "    fig.show()\n",
    "\n",
    "def generate_gmm_data(num_samples=300, num_components=3, random_seed=42):\n",
    "    \"\"\"Generates synthetic data from a Gaussian Mixture Model.\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # True parameters for 3 components\n",
    "    true_weights = np.array([0.3, 0.4, 0.3])\n",
    "    true_means = np.array([\n",
    "        [0, 0],\n",
    "        [3, 3],\n",
    "        [0, 4]\n",
    "    ])\n",
    "    true_covariances = np.array([\n",
    "        [[0.5, 0.2], [0.2, 0.5]],\n",
    "        [[0.8, -0.1], [-0.1, 0.8]],\n",
    "        [[0.6, 0.3], [0.3, 0.6]]\n",
    "    ])\n",
    "\n",
    "    X = []\n",
    "    for _ in range(num_samples):\n",
    "        # Choose a component based on weights\n",
    "        k = np.random.choice(num_components, p=true_weights)\n",
    "        # Sample from the chosen Gaussian\n",
    "        sample = np.random.multivariate_normal(true_means[k], true_covariances[k])\n",
    "        X.append(sample)\n",
    "    \n",
    "    return jnp.array(X), true_weights, true_means, true_covariances\n",
    "\n",
    "# --- K-Means Implementation ---\n",
    "@jax.jit\n",
    "def assign_to_clusters(X, means):\n",
    "    \"\"\"Assigns each data point to the closest mean.\"\"\"\n",
    "    # Compute squared Euclidean distance from each point to each mean\n",
    "    diff = X[:, jnp.newaxis, :] - means[jnp.newaxis, :, :]\n",
    "    distances_sq = jnp.sum(diff**2, axis=-1)\n",
    "    # Get the index of the closest mean for each point\n",
    "    assignments = jnp.argmin(distances_sq, axis=1)\n",
    "    return assignments\n",
    "\n",
    "@jax.jit\n",
    "def update_means(X, assignments, num_components):\n",
    "    \"\"\"Updates the means based on current assignments.\"\"\"\n",
    "    new_means = jnp.zeros((num_components, X.shape[1]))\n",
    "    for k in range(num_components):\n",
    "        # Get points assigned to cluster k\n",
    "        cluster_points = X[assignments == k]\n",
    "        if cluster_points.shape[0] > 0:\n",
    "            new_means = new_means.at[k].set(jnp.mean(cluster_points, axis=0))\n",
    "        else:\n",
    "            # If a cluster becomes empty, keep its mean or re-initialize (here, keep)\n",
    "            new_means = new_means.at[k].set(new_means[k]) # No change\n",
    "    return new_means\n",
    "\n",
    "def run_kmeans(X, num_components, max_iter=100, tol=1e-4, key=None):\n",
    "    \"\"\"Runs the K-Means clustering algorithm.\"\"\"\n",
    "    if key is None:\n",
    "        key = jax.random.PRNGKey(0)\n",
    "\n",
    "    num_samples, data_dim = X.shape\n",
    "\n",
    "    # Initialize means randomly from data points\n",
    "    key, subkey_init = jax.random.split(key)\n",
    "    random_indices = jax.random.choice(subkey_init, num_samples, (num_components,), replace=False)\n",
    "    means = X[random_indices]\n",
    "\n",
    "    prev_assignments = None\n",
    "    history_means = [means]\n",
    "\n",
    "    print(f\"Starting K-Means with {num_components} clusters...\")\n",
    "    for i in range(max_iter):\n",
    "        # E-step-like: Assign points to clusters\n",
    "        assignments = assign_to_clusters(X, means)\n",
    "\n",
    "        # M-step-like: Update cluster means\n",
    "        new_means = update_means(X, assignments, num_components)\n",
    "\n",
    "        # Check for convergence\n",
    "        if prev_assignments is not None and jnp.all(assignments == prev_assignments):\n",
    "            print(f\"K-Means converged in {i+1} iterations.\")\n",
    "            break\n",
    "\n",
    "        means = new_means\n",
    "        prev_assignments = assignments\n",
    "        history_means.append(means)\n",
    "    else:\n",
    "        print(f\"K-Means did not converge after {max_iter} iterations.\")\n",
    "\n",
    "    # Compute final responsibilities for plotting\n",
    "    final_assignments = assign_to_clusters(X, means)\n",
    "    final_responsibilities = jax.nn.one_hot(final_assignments, num_classes=num_components)\n",
    "\n",
    "    return means, final_responsibilities, history_means\n"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example: Running k-Means on Synthetic Data\n",
    "\n",
    "Let's generate some data from a GMM and then apply k-Means to it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Main Execution: K-Means Example ---\n",
    "\n",
    "# 1. Generate synthetic GMM data (similar to previous lectures)\n",
    "num_samples_kmeans = 500\n",
    "num_components_kmeans = 3\n",
    "X_kmeans, _, _, _ = generate_gmm_data(num_samples_kmeans, num_components_kmeans, random_seed=42)\n",
    "\n",
    "print(\"Generated Data Shape for K-Means:\", X_kmeans.shape)\n",
    "\n",
    "# 2. Run the K-Means algorithm\n",
    "kmeans_key = jax.random.PRNGKey(100)\n",
    "estimated_means_kmeans, final_responsibilities_kmeans, history_means_kmeans = \\\n",
    "    run_kmeans(X_kmeans, num_components_kmeans, max_iter=100, key=kmeans_key)\n",
    "\n",
    "print(\"\\nFinal K-Means Estimated Means:\\n\", estimated_means_kmeans)\n",
    "\n",
    "# 3. Plot the final K-Means fit\n",
    "# For plotting with plot_gmm_plotly, we need 'covariances'. K-Means implicitly assumes spherical/identity covariance.\n",
    "# We can approximate this by computing the sample covariance of each cluster.\n",
    "estimated_covariances_kmeans = jnp.zeros((num_components_kmeans, X_kmeans.shape[1], X_kmeans.shape[1]))\n",
    "for k in range(num_components_kmeans):\n",
    "    cluster_points = X_kmeans[jnp.argmax(final_responsibilities_kmeans, axis=1) == k]\n",
    "    if cluster_points.shape[0] > 1:\n",
    "        estimated_covariances_kmeans = estimated_covariances_kmeans.at[k].set(jnp.cov(cluster_points, rowvar=False) + jnp.eye(X_kmeans.shape[1]) * 1e-6)\n",
    "    else:\n",
    "        estimated_covariances_kmeans = estimated_covariances_kmeans.at[k].set(jnp.eye(X_kmeans.shape[1]) * 0.1)\n",
    "\n",
    "plot_gmm_plotly(\n",
    "    X_kmeans,\n",
    "    estimated_means_kmeans,\n",
    "    estimated_covariances_kmeans,\n",
    "    responsibilities=final_responsibilities_kmeans,\n",
    "    title='K-Means Clustering Result'\n",
    ")\n",
    "\n",
    "# Optional: Visualize mean movement over iterations\n",
    "fig_means_path = go.Figure()\n",
    "fig_means_path.add_trace(go.Scatter(\n",
    "    x=X_kmeans[:, 0],\n",
    "    y=X_kmeans[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(color='gray', size=3, opacity=0.5),\n",
    "    name='Data Points'\n",
    "))\n",
    "for k in range(num_components_kmeans):\n",
    "    means_path = jnp.array([m[k] for m in history_means])\n",
    "    fig_means_path.add_trace(go.Scatter(\n",
    "        x=means_path[:, 0],\n",
    "        y=means_path[:, 1],\n",
    "        mode='lines+markers',\n",
    "        name=f'Mean {k+1} Path',\n",
    "        line=dict(color=plot_gmm_plotly.colors[k % len(plot_gmm_plotly.colors)], width=2),\n",
    "        marker=dict(size=8, symbol='circle')\n",
    "    ))\n",
    "fig_means_path.update_layout(title_text='K-Means: Path of Cluster Means', title_x=0.5,\n",
    "                             xaxis_title='Feature 1',\n",
    "                             yaxis_title='Feature 2',\n",
    "                             autosize=False, width=700, height=600)\n",
    "fig_means_path.show()\n"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. k-Means as a \"Hard\" EM Algorithm (Slide 15)\n",
    "\n",
    "Interestingly, k-Means can be viewed as a special case of the EM algorithm for a Gaussian Mixture Model where:\n",
    "1.  The covariance matrices for all components are fixed to be identity matrices (or spherical, $I\\sigma^2$).\n",
    "2.  The mixing coefficients are uniform and fixed.\n",
    "3.  Crucially, the E-step performs a **\"hard\" assignment** of data points to clusters (i.e., each point belongs *exclusively* to one cluster, $r_{nk} \\in \\{0, 1\\}$), rather than the \"soft\" probabilistic assignments ($r_{nk} \\in [0, 1]$) of the standard GMM EM.\n",
    "\n",
    "This \"hard\" assignment simplifies the likelihood to:\n",
    "$$p(x | \\mu) = \\prod_{n=1}^N \\sum_{k=1}^K z_{nk} \\mathcal{N}(x_n; \\mu_k, I)$$\n",
    "\n",
    "By iterating the assignment (E-step) and mean update (M-step), k-Means maximizes this specific likelihood. This connection highlights the unifying power of the EM framework, showing how even simple algorithms can be understood as optimizing a probabilistic objective."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. Free Energy: The Connection to Physics (Slides 16-17)\n",
    "\n",
    "The concept of the **Evidence Lower Bound (ELBO)**, which we maximized in Variational Inference, has a profound connection to **Free Energy** in statistical physics. This is not a mere analogy; it's a direct mathematical equivalence.\n",
    "\n",
    "Recall the decomposition of the log evidence:\n",
    "$$\\log p(x) = \\mathcal{L}(q(z)) + D_{KL}(q(z) || p(z | x))$$ \n",
    "\n",
    "where $\\mathcal{L}(q(z)) = \\int q(z) \\log \\left( \\frac{p(x, z)}{q(z)} \\right) dz$.\n",
    "\n",
    "In statistical physics, probability distributions are often expressed as **Gibbs measures**: $p(x, z) = \\exp(-E(x, z))$, where $E(x, z)$ is the energy of a state $(x, z)$. If we substitute this into the ELBO, we find:\n",
    "$$\\mathcal{L}(q) = \\int q(z) (\\log p(x, z) - \\log q(z)) dz = \\int q(z) (-E(x, z) - \\log q(z)) dz$$ \n",
    "$$\\mathcal{L}(q) = -\\mathbb{E}_q[E(x, z)] - \\mathbb{E}_q[\\log q(z)] = -\\mathbb{E}_q[E(x, z)] + H(q)$$ \n",
    "\n",
    "where $H(q) = -\\mathbb{E}_q[\\log q(z)]$ is the **entropy** of the variational distribution $q(z)$.\n",
    "\n",
    "The negative ELBO, $-\\mathcal{L}(q)$, is precisely the **Variational Free Energy** (often denoted $F$ or $A$ in physics, related to the Helmholtz free energy): \n",
    "$$F = \\mathbb{E}_q[E(x, z)] - H(q)$$ \n",
    "This has the familiar form of internal energy minus temperature times entropy ($F = U - TS$, where $U$ is internal energy and $T$ is temperature). In the context of VI, $\\mathbb{E}_q[E(x, z)]$ can be seen as an average energy, and $H(q)$ as an entropy term. Maximizing the ELBO is equivalent to minimizing this variational free energy.\n",
    "\n",
    "This deep connection highlights how concepts from physics, particularly statistical mechanics, have provided fundamental insights and mathematical tools for machine learning. Pioneers like **Hermann von Helmholtz, Josiah Willard Gibbs, and Ludwig Boltzmann** laid the groundwork for these ideas, which are now being applied to modern machine learning by researchers like **David M. Blei**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6. The Calculus of Variations (Slide 18)\n",
    "\n",
    "At the heart of Variational Inference lies the **Calculus of Variations**, a branch of mathematics concerned with finding functions that optimize certain functionals (integrals that depend on functions). While not typically taught in introductory calculus, it's a powerful idea that underpins many advanced scientific and engineering disciplines.\n",
    "\n",
    "Pioneers like **Leonhard Euler** and **Joseph-Louis Lagrange** developed the foundational concepts of the calculus of variations in the 18th century. Later, physicists like **Richard Feynman** (Nobel Prize 1965) used variational principles extensively in quantum mechanics (e.g., Feynman path integrals).\n",
    "\n",
    "In VI, we are essentially using the principles of the calculus of variations to find the optimal distribution $q(z)$ that maximizes the ELBO functional. The mean-field updates we derived are direct consequences of applying variational calculus to the ELBO under the factorization assumption."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7. Randomized Computations: Monte Carlo (Slides 19-24)\n",
    "\n",
    "Another critical computational paradigm in probabilistic machine learning is **Monte Carlo methods**. These techniques use random sampling to approximate integrals or expectations that are otherwise intractable. While the core idea of using randomness for computation has ancient roots, its modern application gained prominence during the **Manhattan Project** in the 1940s.\n",
    "\n",
    "**John von Neumann** and **Stanisław Ulam**, both with ties to the Lwów School, were instrumental in developing and popularizing Monte Carlo methods. Ulam, while recovering from an illness, conceived the idea of using random sampling to solve complex physics problems, such as neutron diffusion in fissionable material, which were too difficult for deterministic calculations. Von Neumann then formalized the approach and gave it the name \"Monte Carlo\" (after Ulam's uncle's gambling habits).\n",
    "\n",
    "These methods were crucial for the design of nuclear weapons, including the **Teller-Ulam design** for thermonuclear weapons, which involved complex simulations that could only be tackled with probabilistic sampling. The development of Monte Carlo methods at Los Alamos, often involving physical simulations with random numbers, marked a pivotal moment in the history of scientific computing and laid the groundwork for modern sampling-based inference techniques like Markov Chain Monte Carlo (MCMC)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8. And then the Machines Changed Everything. Everything? (Slides 25, 29-30)\n",
    "\n",
    "The advent of **digital computers** fundamentally transformed what could be computed and modeled. The history of AI and machine learning has seen a fascinating oscillation between advances driven by sheer **computing power** (brute force) and those driven by **algorithmic improvements** (efficiency).\n",
    "\n",
    "Key eras and developments include (Slide 25):\n",
    "* **Symbolic Logic (1930s onwards)**: Pioneered by Alan Turing, focusing on logical reasoning and rule-based systems.\n",
    "* **Connectionism (1950s onwards)**: The rise of neural networks, exemplified by **Frank Rosenblatt's Perceptron**, which sought to model intelligence through interconnected nodes.\n",
    "* **Operations Research (1970s onwards)**: Development of powerful optimization algorithms like BFGS.\n",
    "* **Kernels, Probability, Graphs (1990s)**: A resurgence of probabilistic methods, graphical models (Judea Pearl), and kernel methods (Vladimir Vapnik), emphasizing statistical rigor and structured representations.\n",
    "* **GPUs and Deep Learning (2010s)**: The explosion of deep learning, heavily reliant on massive datasets and parallel computation on GPUs, leading to unprecedented performance in areas like image recognition and natural language processing.\n",
    "\n",
    "Today, we stand at another inflection point. While computing power allows us to be \"lazy\" with algorithms, training contemporary large models is incredibly expensive, both computationally and environmentally. This suggests that **algorithmic and modeling knowledge will remain crucial** (Slide 30). Efficient algorithms can make the difference between a workstation and an edge device, between training on millions and billions of data points, and ultimately, between profitability and financial drain.\n",
    "\n",
    "The future of machine learning likely lies in a synergistic combination of powerful hardware, vast datasets, and ever more sophisticated and efficient probabilistic algorithms."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9. Stochastic Variational Inference: Leveraging Deep Networks (Slides 26-28)\n",
    "\n",
    "Bridging the gap between the algorithmic elegance of Variational Inference and the power of deep learning is **Stochastic Variational Inference (SVI)**, notably popularized by **Kingma & Welling's \"Auto-Encoding Variational Bayes\" (2013)**. SVI allows us to perform VI on very large datasets and with complex, non-conjugate models by leveraging deep neural networks and Monte Carlo sampling.\n",
    "\n",
    "Instead of carefully deriving analytic updates for each variational factor (which can be tedious or impossible for complex models), SVI proposes to:\n",
    "1.  **Parameterize the variational distribution $q(z|\\phi, x)$ (encoder) and the likelihood $p(x, z|\\theta)$ (decoder) using deep neural networks.** The parameters of these networks are $\\phi$ and $\\theta$, respectively.\n",
    "2.  **Optimize the ELBO by using stochastic gradient descent.** The challenge is computing gradients of expectations with respect to the parameters of the distribution defining the expectation. This is where the **reparameterization trick** comes in.\n",
    "\n",
    "**The Reparameterization Trick** (Slide 28):\n",
    "For a variational distribution like $q(z | \\phi, x) = \\mathcal{N}(z; \\mu_\\phi(x), \\Sigma_\\phi(x))$, we can reparameterize the random variable $z$ as a deterministic function of a simpler random variable $u$ (e.g., $u \\sim \\mathcal{N}(0, I)$) and the variational parameters $\\phi$:\n",
    "$$z = \\mu_\\phi(x) + L_\\phi(x) u$$ \n",
    "where $\\Sigma_\\phi(x) = L_\\phi(x)L_\\phi(x)^T$ (Cholesky decomposition).\n",
    "\n",
    "This allows us to move the expectation outside the gradient, making the gradient computable via backpropagation:\n",
    "$$\\nabla_\\phi \\mathbb{E}_{q_\\phi(z|x)} [\\dots] = \\mathbb{E}_{u \\sim \\mathcal{N}(0, I)} [\\nabla_\\phi (\\dots)]$$ \n",
    "\n",
    "The expectation can then be approximated by Monte Carlo sampling (taking a few samples of $u$). This combination of deep networks, variational inference, and the reparameterization trick forms the basis of powerful generative models like **Variational Autoencoders (VAEs)**, allowing for efficient inference and generation in high-dimensional spaces.\n",
    "\n",
    "SVI represents a significant step towards scalable and flexible probabilistic modeling, enabling us to combine the strengths of deep learning with the principled framework of Bayesian inference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Conclusion\n",
    "\n",
    "This course has taken you on a journey through the fascinating world of Probabilistic Machine Learning. We've seen how fundamental ideas from probability theory, statistics, and even physics have converged to create powerful tools for understanding data and making informed decisions under uncertainty. From the rigorous foundations of Bayes' theorem to the practicalities of EM and Variational Inference, and the modern advancements in deep probabilistic models, you now possess a robust framework for tackling complex machine learning problems.\n",
    "\n",
    "The historical perspective reminds us that scientific progress is often iterative, collaborative, and deeply human. The challenges that motivated past generations of researchers continue to inspire new innovations, and the pursuit of efficient, interpretable, and robust algorithms remains a vital frontier in machine learning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercises\n",
    "\n",
    "**Exercise 1: K-Means Initialization Sensitivity**\n",
    "Similar to EM, k-Means is sensitive to initialization. In the `run_kmeans` function, try different `key` values (e.g., 100, 200, 300). Observe how the final cluster assignments and means change. How might you address this sensitivity in a real-world application?\n",
    "\n",
    "**Exercise 2: K-Means vs. GMM EM**\n",
    "Compare the clustering results of k-Means (from this lecture) with the GMM EM algorithm (from Lecture 23) on the same `generate_gmm_data` dataset. Discuss the visual differences in the cluster shapes and assignments. Explain why GMM EM provides a \"softer\" and more flexible clustering compared to k-Means, relating it to their underlying probabilistic assumptions.\n",
    "\n",
    "**Exercise 3: The ELBO and Free Energy (Conceptual)**\n",
    "Revisit the ELBO decomposition: $\\log p(x) = \\mathcal{L}(q(z)) + D_{KL}(q(z) || p(z | x))$. Explain, in your own words, how minimizing the KL divergence is equivalent to maximizing the ELBO. Then, explain how the term $-\\mathcal{L}(q(z))$ relates to the concept of Free Energy in statistical physics. What does this connection imply about the nature of variational inference?\n",
    "\n",
    "**Exercise 4 (Advanced): Reparameterization Trick (Conceptual)**\n",
    "For a simple 1D Gaussian $q(z | \\mu, \\sigma^2) = \\mathcal{N}(z; \\mu, \\sigma^2)$, write down the reparameterization trick for $z$. If you wanted to compute $\\nabla_\\mu \\mathbb{E}_{q(z)}[f(z)]$ for some function $f(z)$, show how the reparameterization trick allows you to move the gradient inside the expectation, making it amenable to Monte Carlo estimation and automatic differentiation. Discuss why this is a crucial innovation for training VAEs and other deep generative models."
   ],
   "metadata": {}
  }
 ]
}
