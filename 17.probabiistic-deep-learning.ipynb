{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c0dca3",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning: Lecture 17 - Probabilistic Deep Learning\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Welcome to Lecture 17 of Probabilistic Machine Learning! In this lecture, we bridge the gap between deep neural networks and Gaussian Processes, exploring how we can imbue deep learning models with probabilistic capabilities. We will focus on the powerful technique of using Laplace approximations to transform a trained deep network into an approximate Gaussian Process, allowing for uncertainty quantification and a deeper probabilistic understanding.\n",
    "\n",
    "This notebook will provide detailed explanations and practical code illustrations using **JAX** for efficient numerical computations and **Plotly** for interactive visualizations, building upon the foundations laid in previous lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3666b5",
   "metadata": {},
   "source": [
    "#### 1. Recap: Deep Networks and Empirical Risk Minimization\n",
    "\n",
    "As we recapped in Lecture 16 (Slide 2), for our purposes, a deep neural network is a function $f(x, \\theta): \\mathbb{X} \\times \\mathbb{R}^D \\to \\mathbb{R}^F$, parametrized by parameters $\\theta \\in \\mathbb{R}^D$ and mapping inputs $x \\in \\mathbb{X}$ to outputs $f(x, \\theta) \\in \\mathbb{R}^F$. These networks are typically trained by **Empirical Risk Minimization (ERM)** to find parameters $\\theta_*$ on a training set $\\mathcal{D}=[(x_i,y_i)]_{i=1,...,N}$:\n",
    "\n",
    "$$\\theta_* = \\arg \\min_{\\theta} \\mathcal{L}(\\theta) = \\arg \\min_{\\theta} \\left( \\frac{1}{N} \\sum_{i=1}^N \\ell(y_i, f(x_i, \\theta)) + r(\\theta) \\right)$$\n",
    "\n",
    "We also established that this ERM objective is equivalent to finding the **Maximum A Posteriori (MAP) estimate** of the parameters:\n",
    "\n",
    "$$\\theta_* = \\arg \\max_{\\theta \\in \\mathbb{R}^D} p(\\theta | \\mathcal{D})$$\n",
    "\n",
    "This probabilistic interpretation is key to understanding how we can turn a deep network into a Gaussian Process. Let's set up our necessary imports and utility functions, including a simple MLP model from Lecture 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4520664a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from jax.flatten_util import ravel_pytree  # Utility to flatten/unflatten JAX pytrees\n",
    "\n",
    "# Set JAX to use 64-bit floats for numerical stability\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def sigmoid(f):\n",
    "    \"\"\"Logistic sigmoid function.\"\"\"\n",
    "    return 1 / (1 + jnp.exp(-f))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "\n",
    "def rbf_kernel(X1, X2, length_scale=1.0):\n",
    "    \"\"\"Radial Basis Function (RBF) kernel.\"\"\"\n",
    "    sqdist = jnp.sum(X1**2, 1)[:, None] + jnp.sum(X2**2, 1) - 2 * jnp.dot(X1, X2.T)\n",
    "    return jnp.exp(-0.5 * (1 / length_scale**2) * sqdist)\n",
    "\n",
    "\n",
    "def generate_data(type=\"sin_wave\", n_samples=50, noise_std=0.1):\n",
    "    \"\"\"Generates synthetic 1D regression data.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    if type == \"sin_wave\":\n",
    "        X = np.linspace(-3, 3, n_samples).reshape(-1, 1)\n",
    "        y = np.sin(X * 2) + np.cos(X * 3) + noise_std * np.random.randn(n_samples, 1)\n",
    "    elif type == \"quadratic\":\n",
    "        X = np.linspace(-2, 2, n_samples).reshape(-1, 1)\n",
    "        y = 0.5 * X**2 - 1.0 * X + 0.3 + noise_std * np.random.randn(n_samples, 1)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_regression_plotly(\n",
    "    X, y, predictions=None, std_dev=None, title=\"\", fig=None, row=None, col=None\n",
    "):\n",
    "    \"\"\"Plots 1D regression data and predictions using Plotly.\"\"\"\n",
    "    if fig is None:\n",
    "        fig = go.Figure()\n",
    "\n",
    "    X_np = np.asarray(X)\n",
    "    y_np = np.asarray(y)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_np.flatten(),\n",
    "            y=y_np.flatten(),\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=\"blue\", size=6),\n",
    "            name=\"Training Data\",\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    if predictions is not None:\n",
    "        predictions_np = np.asarray(predictions).flatten()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_np.flatten(),\n",
    "                y=predictions_np,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"red\", width=2),\n",
    "                name=\"Mean Prediction\",\n",
    "                showlegend=True,\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        if std_dev is not None:\n",
    "            std_dev_np = np.asarray(std_dev).flatten()\n",
    "            upper_bound = predictions_np + 2 * std_dev_np\n",
    "            lower_bound = predictions_np - 2 * std_dev_np\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=np.concatenate([X_np.flatten(), X_np.flatten()[::-1]]),\n",
    "                    y=np.concatenate([upper_bound, lower_bound[::-1]]),\n",
    "                    fill=\"toself\",\n",
    "                    fillcolor=\"rgba(255,0,0,0.1)\",\n",
    "                    line_color=\"rgba(255,255,255,0)\",\n",
    "                    name=\"2 Std Dev Uncertainty\",\n",
    "                    showlegend=True,\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(title_text=title, title_x=0.5)\n",
    "    fig.update_xaxes(title_text=\"X\", row=row, col=col)\n",
    "    fig.update_yaxes(title_text=\"Y\", row=row, col=col)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# --- Simple MLP Implementation in JAX ---\n",
    "def init_mlp_params(key, layer_sizes):\n",
    "    \"\"\"Initializes parameters for a simple MLP.\"\"\"\n",
    "    params = []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        in_dim = layer_sizes[i]\n",
    "        out_dim = layer_sizes[i + 1]\n",
    "        # Glorot initialization for weights\n",
    "        limit = jnp.sqrt(6 / (in_dim + out_dim))\n",
    "        weights = jax.random.uniform(\n",
    "            subkey, (in_dim, out_dim), minval=-limit, maxval=limit\n",
    "        )\n",
    "        biases = jnp.zeros(out_dim)\n",
    "        params.append({\"weights\": weights, \"biases\": biases})\n",
    "    return params\n",
    "\n",
    "\n",
    "def mlp_forward(params, x):\n",
    "    \"\"\"Forward pass through the MLP.\"\"\"\n",
    "    hidden_layers = params[:-1]\n",
    "    output_layer = params[-1]\n",
    "\n",
    "    h = x\n",
    "    for layer in hidden_layers:\n",
    "        h = jnp.dot(h, layer[\"weights\"]) + layer[\"biases\"]\n",
    "        h = relu(h)  # Using ReLU as nonlinearity\n",
    "\n",
    "    # Output layer (no activation for regression, as it's a linear output)\n",
    "    output = jnp.dot(h, output_layer[\"weights\"]) + output_layer[\"biases\"]\n",
    "    return output\n",
    "\n",
    "\n",
    "# --- Loss and Regularization Functions ---\n",
    "def mse_loss(predictions, targets):\n",
    "    \"\"\"Mean Squared Error loss.\"\"\"\n",
    "    return jnp.mean(jnp.square(predictions - targets))\n",
    "\n",
    "\n",
    "def l2_regularization(params, lambda_reg):\n",
    "    \"\"\"L2 regularization (weight decay).\"\"\"\n",
    "    l2_norm = 0.0\n",
    "    for layer in params:\n",
    "        l2_norm += jnp.sum(jnp.square(layer[\"weights\"]))\n",
    "    return 0.5 * lambda_reg * l2_norm\n",
    "\n",
    "\n",
    "def neg_log_posterior(params, X, y, lambda_reg, noise_variance):\n",
    "    \"\"\"\n",
    "    Negative log-posterior for MLP parameters (equivalent to ERM objective).\n",
    "    Assumes Gaussian likelihood and Gaussian prior on weights (mean zero, precision lambda_reg/sigma_noise^2).\n",
    "    \"\"\"\n",
    "    predictions = mlp_forward(params, X)\n",
    "    # Negative log-likelihood (Gaussian likelihood)\n",
    "    neg_log_likelihood = 0.5 * jnp.sum(jnp.square(y - predictions)) / noise_variance\n",
    "    # Negative log-prior (Gaussian prior on weights)\n",
    "    neg_log_prior = l2_regularization(params, lambda_reg)\n",
    "    return neg_log_likelihood + neg_log_prior\n",
    "\n",
    "\n",
    "# --- Newton's Method for MAP estimate (from Lecture 15, adapted) ---\n",
    "@jax.jit\n",
    "def newton_step_mlp(flat_params, unflatten_fn, X, y, lambda_reg, noise_variance):\n",
    "    \"\"\"Performs one Newton update step for maximizing the log posterior of MLP params.\"\"\"\n",
    "    params = unflatten_fn(flat_params)\n",
    "\n",
    "    # Compute gradient and Hessian of the negative log posterior\n",
    "    grad_fn = jax.grad(neg_log_posterior)\n",
    "    hess_fn = jax.hessian(neg_log_posterior)\n",
    "\n",
    "    grad_val = grad_fn(params, X, y, lambda_reg, noise_variance)\n",
    "    hess_val = hess_fn(params, X, y, lambda_reg, noise_variance)\n",
    "\n",
    "    # Flatten grad and hess for linear algebra\n",
    "    flat_grad, _ = ravel_pytree(grad_val)\n",
    "    flat_hess, _ = ravel_pytree(hess_val)\n",
    "\n",
    "    # Newton update: flat_params_new = flat_params_old - H_inv @ grad\n",
    "    delta_flat_params = jnp.linalg.solve(flat_hess, flat_grad)\n",
    "    flat_params_new = flat_params - delta_flat_params\n",
    "    return flat_params_new\n",
    "\n",
    "\n",
    "def find_map_mlp_params(\n",
    "    key,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    layer_sizes,\n",
    "    lambda_reg,\n",
    "    noise_variance,\n",
    "    max_iter=100,\n",
    "    tol=1e-5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds the MAP estimate (theta_star) for MLP parameters using Newton's method.\n",
    "    \"\"\"\n",
    "    initial_params = init_mlp_params(key, layer_sizes)\n",
    "    flat_params, unflatten_fn = ravel_pytree(initial_params)\n",
    "\n",
    "    print(\"Starting Newton's method for MLP MAP estimate...\")\n",
    "    for i in range(max_iter):\n",
    "        flat_params_new = newton_step_mlp(\n",
    "            flat_params, unflatten_fn, X_train, y_train, lambda_reg, noise_variance\n",
    "        )\n",
    "        change = jnp.linalg.norm(flat_params_new - flat_params)\n",
    "        if change < tol:\n",
    "            print(f\"Converged in {i + 1} iterations. Final change: {change:.6f}\")\n",
    "            break\n",
    "        flat_params = flat_params_new\n",
    "    else:\n",
    "        print(\n",
    "            f\"Newton's method did not converge after {max_iter} iterations. Final change: {change:.6f}\"\n",
    "        )\n",
    "\n",
    "    theta_star = unflatten_fn(flat_params)\n",
    "    return theta_star\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc08b4fb",
   "metadata": {},
   "source": [
    "#### 3. Deep Networks are GPs: The Four Easy Steps\n",
    "\n",
    "The core idea of turning a deep network into a Gaussian Process is elegantly summarized in four steps (as per Slide 4):\n",
    "\n",
    "1.  **Realize that the loss is a negative log-posterior**: As discussed, the ERM objective $\\mathcal{L}(\\theta)$ is equivalent to the negative log-posterior of the parameters, up to a constant:\n",
    "    $$\\mathcal{L}(\\theta) = \\left( \\frac{1}{N} \\sum_{i=1}^N \\ell(y_i, f(x_i, \\theta)) \\right) + r(\\theta) = -\\log p(\\mathcal{D}|\\theta) - \\log p(\\theta) = -\\log p(\\theta|\\mathcal{D}) + \\text{const.}$$\n",
    "\n",
    "2.  **Train the deep net as usual to find $\\theta_*$**: This is the standard deep learning training process, which finds the MAP estimate of the parameters:\n",
    "    $$\\theta_* = \\arg \\max_{\\theta \\in \\mathbb{R}^D} p(\\theta|\\mathcal{D})$$\n",
    "\n",
    "3.  **At $\\theta_*$, compute a Laplace approximation of the log-posterior**: We approximate the (typically non-Gaussian) posterior $p(\\theta|\\mathcal{D})$ with a Gaussian distribution centered at $\\theta_*$. This involves computing the Hessian matrix $\\Psi$ of the negative log-posterior at $\\theta_*$:\n",
    "    $$\\Psi := -\\nabla\\nabla^T \\log p(\\theta_*|\\mathcal{D})$$\n",
    "    The Gaussian approximation is then $\\mathcal{N}(\\theta; \\theta_*, -\\Psi^{-1})$.\n",
    "\n",
    "4.  **Linearize $f(x, \\theta)$ around $\\theta_*$**: We approximate the deep network's output $f(x, \\theta)$ with a first-order Taylor expansion around the MAP estimate $\\theta_*$:\n",
    "    $$f(x, \\theta) \\approx f(x, \\theta_*) + J(x, \\theta_*) (\\theta - \\theta_*)$$\n",
    "    where $J(x, \\theta_*)$ is the Jacobian matrix of $f(x, \\theta)$ with respect to $\\theta$, evaluated at $x$ and $\\theta_*$. Its elements are $[J(x)]_{ij} = \\frac{\\partial f_i(x, \\theta_*)}{\\partial \\theta_j}$.\n",
    "\n",
    "Combining these steps, the posterior distribution over the function output $f(\\bullet)$ given the data $\\mathcal{D}$ can be approximated as a Gaussian Process:\n",
    "\n",
    "$$p(f(\\bullet)|\\mathcal{D}) \\approx \\mathcal{GP}(f(\\bullet); f(\\bullet, \\theta_*), -J(\\bullet)\\Psi^{-1}J(\\circ)^T)$$\n",
    "\n",
    "Thus:\n",
    "* The **mean function** of this approximate GP is the trained deep network itself: $\\mathbb{E}(f(\\bullet)) = f(\\bullet, \\theta_*)$.\n",
    "* The **covariance function** is the **Laplace tangent kernel**: $\\text{cov}(f(\\bullet), f(\\circ)) = -J(\\bullet)\\Psi^{-1}J(\\circ)^T$.\n",
    "\n",
    "Let's put this into practice with a code example. We'll train a small MLP on a 1D regression task, find its MAP parameters, and then use the Laplace approximation to get predictive mean and uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf0b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Main Execution for Deep GP Approximation ---\n",
    "\n",
    "# 1. Generate 1D regression data\n",
    "X_train_np, y_train_np = generate_data(type=\"sin_wave\", n_samples=50, noise_std=0.2)\n",
    "X_train_jax = jnp.array(X_train_np)\n",
    "y_train_jax = jnp.array(y_train_np)\n",
    "\n",
    "# Define MLP architecture\n",
    "input_dim = 1\n",
    "hidden_dim = 20  # A bit wider to allow for more expressiveness\n",
    "output_dim = 1\n",
    "layer_sizes = [input_dim, hidden_dim, hidden_dim, output_dim]  # Two hidden layers\n",
    "\n",
    "# Hyperparameters for training and Laplace approximation\n",
    "lambda_reg = 0.01  # L2 regularization strength\n",
    "noise_variance = 0.1**2  # Assumed noise variance for likelihood\n",
    "\n",
    "# 2. Find the MAP estimate (theta_star) using Newton's method\n",
    "key = jax.random.PRNGKey(10)\n",
    "theta_star = find_map_mlp_params(\n",
    "    key,\n",
    "    X_train_jax,\n",
    "    y_train_jax,\n",
    "    layer_sizes,\n",
    "    lambda_reg,\n",
    "    noise_variance,\n",
    "    max_iter=1000,\n",
    "    tol=1e-7,\n",
    ")\n",
    "\n",
    "print(\"\\nMAP estimate (theta_star) of MLP parameters found.\")\n",
    "\n",
    "# 3. Compute the Hessian (Psi) of the negative log-posterior at theta_star\n",
    "flat_theta_star, unflatten_theta = ravel_pytree(theta_star)\n",
    "\n",
    "\n",
    "# Define a function that computes the negative log-posterior for flattened parameters\n",
    "def flat_neg_log_posterior(flat_params, X, y, lambda_reg, noise_variance):\n",
    "    params = unflatten_theta(flat_params)\n",
    "    return neg_log_posterior(params, X, y, lambda_reg, noise_variance)\n",
    "\n",
    "\n",
    "hess_neg_log_post_at_theta_star = jax.hessian(flat_neg_log_posterior)(\n",
    "    flat_theta_star, X_train_jax, y_train_jax, lambda_reg, noise_variance\n",
    ")\n",
    "\n",
    "# Psi is the negative of this Hessian\n",
    "Psi = -hess_neg_log_post_at_theta_star\n",
    "\n",
    "# For the covariance, we need -Psi_inv, which is Psi_inv if Psi is already negative of Hessian\n",
    "Sigma_theta_star = jnp.linalg.inv(-Psi)  # Covariance of theta_star approx\n",
    "\n",
    "print(f\"Shape of Psi (Hessian): {Psi.shape}\")\n",
    "print(f\"Shape of Sigma_theta_star: {Sigma_theta_star.shape}\")\n",
    "\n",
    "# 4. Linearize f(x, theta) around theta_star and compute predictions\n",
    "X_test_np = np.linspace(-4, 4, 200).reshape(-1, 1)\n",
    "X_test_jax = jnp.array(X_test_np)\n",
    "\n",
    "\n",
    "# Define a function to get the Jacobian of mlp_forward with respect to parameters\n",
    "def get_jacobian_fn(params, x_val):\n",
    "    # The Jacobian of mlp_forward(params, x_val) w.r.t. params\n",
    "    # We need to wrap mlp_forward to take flat_params as first arg\n",
    "    def f_flat_params(flat_p):\n",
    "        p = unflatten_theta(flat_p)\n",
    "        return mlp_forward(p, x_val)\n",
    "\n",
    "    return jax.jacobian(f_flat_params)(flat_theta_star)\n",
    "\n",
    "\n",
    "predictive_means = []\n",
    "predictive_variances = []\n",
    "\n",
    "for x_t in X_test_jax:\n",
    "    # Compute J(x_t, theta_star)\n",
    "    J_xt = get_jacobian_fn(\n",
    "        theta_star, x_t.reshape(1, -1)\n",
    "    )  # Reshape x_t to (1, input_dim)\n",
    "\n",
    "    # Ensure J_xt is 2D (num_outputs, num_flat_params)\n",
    "    if (\n",
    "        J_xt.ndim == 1\n",
    "    ):  # For 1D output, jax.jacobian might return 1D array if input is 1D\n",
    "        J_xt = J_xt.reshape(1, -1)\n",
    "\n",
    "    # Predictive Mean: E[f(x)] = f(x, theta_star)\n",
    "    mean_pred = mlp_forward(theta_star, x_t.reshape(1, -1)).flatten()[\n",
    "        0\n",
    "    ]  # Ensure scalar output\n",
    "\n",
    "    # Predictive Covariance: cov(f(x), f(x')) = J(x) @ Sigma_theta_star @ J(x').T\n",
    "    # For variance at a single point x, it's J(x) @ Sigma_theta_star @ J(x).T\n",
    "    # Plus noise variance for the observed data (if predicting observed y, otherwise latent f)\n",
    "    variance_pred_latent = (J_xt @ Sigma_theta_star @ J_xt.T).flatten()[0]\n",
    "    variance_pred_observed = (\n",
    "        variance_pred_latent + noise_variance\n",
    "    )  # Add noise for observed predictions\n",
    "\n",
    "    predictive_means.append(mean_pred)\n",
    "    predictive_variances.append(variance_pred_observed)\n",
    "\n",
    "predictive_means = jnp.array(predictive_means)\n",
    "predictive_std_devs = jnp.sqrt(jnp.array(predictive_variances))\n",
    "\n",
    "print(\"\\nPredictions computed using Laplace Approximation (Deep GP)...\")\n",
    "\n",
    "# Plotting the results\n",
    "fig = plot_regression_plotly(\n",
    "    X_test_np,\n",
    "    predictive_means,\n",
    "    predictions=predictive_means,\n",
    "    std_dev=predictive_std_devs,\n",
    "    title=\"Probabilistic Deep Learning: MLP as a GP (Laplace Approx.)\",\n",
    ")\n",
    "\n",
    "# Add training data to the plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_train_np.flatten(),\n",
    "        y=y_train_np.flatten(),\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"black\", size=8, symbol=\"x\"),\n",
    "        name=\"Training Data (Observed)\",\n",
    "        showlegend=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, width=800)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e41f2",
   "metadata": {},
   "source": [
    "#### 4. What's to Like? (Advantages of Laplace Approximations for Deep Learning)\n",
    "\n",
    "This approach offers several compelling advantages for deep learning practitioners (as per Slide 7):\n",
    "\n",
    "* **You get to keep your beloved point estimate**: The mean function of the resulting GP is simply the prediction from the trained deep network $f(\\bullet, \\theta_*)$. This means all the performance benefits of the original deep model are retained.\n",
    "* **You get to keep your beloved training procedure**: The Laplace approximation is constructed *post-hoc*, after the standard deep learning training (e.g., SGD, Adam) is completed. This applies even to pre-trained networks downloaded from the internet, provided the model architecture, training data, and loss function are available.\n",
    "* **Only auto-diff and numerical linear algebra are needed**: Unlike other probabilistic deep learning methods (e.g., MCMC, variational inference), Laplace approximation avoids sampling, stochasticity, or complex ensemble training. JAX's automatic differentiation makes computing the Hessian and Jacobian straightforward.\n",
    "* **The result is a GP, with all the trimmings**: By approximating the deep net as a GP, we gain access to all the benefits of Gaussian Processes, including:\n",
    "    * **Evidence estimation**: For hyperparameter tuning.\n",
    "    * **Sampling from the posterior**: To understand the range of plausible functions.\n",
    "    * **Uncertainty quantification**: Providing principled confidence intervals for predictions, which is crucial for safety-critical applications or active learning.\n",
    "    * **Sparse decompositions**: For scaling to larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d19b9e",
   "metadata": {},
   "source": [
    "#### 5. Challenges of Laplace Approximations for Deep Learning\n",
    "\n",
    "Despite its advantages, the Laplace approximation for deep learning also presents challenges (as per Slide 8):\n",
    "\n",
    "* **Hessian decomposition is $\\mathcal{O}(D^3)$**: The computation and inversion of the Hessian matrix $\\Psi \\in \\mathbb{R}^{D \\times D}$ (where $D$ is the number of parameters in the deep network) can be computationally very expensive for large networks. However, active research focuses on approximations (e.g., K-FAC, low-rank approximations) to make this more tractable.\n",
    "* **Laplace approximations are local**: They are based on a Taylor expansion around a single mode ($\\theta_*$) of the posterior. This means they can be \"arbitrarily wrong\" compared to the full, true posterior, especially if the posterior is multi-modal or highly non-Gaussian. However, they are still generally better than a simple point estimate.\n",
    "* **Loss functions not designed for generative models**: The loss functions used in deep learning (e.g., cross-entropy, MSE) are primarily designed for predictive performance, not necessarily for accurately capturing the underlying generative process or full uncertainty. This can limit the quality of the probabilistic interpretation derived from the Laplace approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f0f16",
   "metadata": {},
   "source": [
    "#### 6. The Laplace Tangent Kernel: Some Context\n",
    "\n",
    "The covariance function we derived, $-J(\\bullet)\\Psi^{-1}J(\\circ)^T$, is known as the **Laplace Tangent Kernel** (as per Slide 9).\n",
    "\n",
    "It's important to differentiate this from the **Neural Tangent Kernel (NTK)**, introduced by Jacot, Gabriel, and Hongler (NeurIPS 2019). The NTK is defined as:\n",
    "\n",
    "$$k_{\\text{NTK}}(\\bullet, \\circ) = J_{\\theta_0}(\\bullet) J_{\\theta_0}^T(\\circ) = \\sum_{d=1}^D \\frac{\\partial f(\\bullet, \\theta_0)}{\\partial [\\theta_0]_d} \\frac{\\partial f(\\circ, \\theta_0)}{\\partial [\\theta_0]_d}$$\n",
    "\n",
    "The NTK is typically evaluated at a random initialization $\\theta_0$ and remains fixed during training. It is a theoretical tool for analyzing the behavior of gradient descent in infinitely wide networks and does not directly yield meaningful uncertainty quantification in the same way the Laplace approximation does. The Laplace Tangent Kernel, in contrast, uses the *trained* parameters $\\theta_*$ and incorporates the curvature of the posterior (via $\\Psi^{-1}$), providing a direct measure of uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3552e4f",
   "metadata": {},
   "source": [
    "#### 7. Summary\n",
    "\n",
    "In summary (as per Slide 10):\n",
    "\n",
    "* **Laplace approximations turn (nearly) any deep neural network into a Gaussian process.** This is a powerful way to bridge the gap between deterministic deep learning and probabilistic modeling.\n",
    "* They involve **only auto-differentiation and linear algebra**, both of which are robust and scalable operations within modern deep learning frameworks like JAX.\n",
    "* **Deep nets thus approximately inherit probabilistic functionality**, gaining the ability to quantify uncertainty in their predictions.\n",
    "* For large-scale deep networks, **care must be taken to find approximate solutions to the Hessian decomposition** to manage computational complexity.\n",
    "\n",
    "This lecture demonstrates a practical method for adding probabilistic capabilities to deep learning models, offering a path towards more robust and interpretable AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb5316",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "**Exercise 1: Impact of Regularization and Noise Variance**\n",
    "Experiment with different values for `lambda_reg` (L2 regularization strength) and `noise_variance` in the `main` execution block. How do these hyperparameters affect the predictive mean and, more importantly, the predictive uncertainty (the shaded region)? Explain your observations in terms of their probabilistic interpretation (prior strength and likelihood noise).\n",
    "\n",
    "**Exercise 2: Effect of Network Depth and Width**\n",
    "Modify the `layer_sizes` in the MLP to create a shallower (e.g., `[input_dim, output_dim]`) or deeper (e.g., `[input_dim, 50, 50, 50, output_dim]`) network. How does this change the training process and the resulting predictive mean and uncertainty? (Note: Deeper networks might require more `max_iter` or careful initialization).\n",
    "\n",
    "**Exercise 3: Comparing to a Pure GP**\n",
    "Implement a standard Gaussian Process regression model (using `rbf_kernel` and the exact GP regression formulas) on the same `sin_wave` dataset. Compare its predictive mean and uncertainty to the Laplace-approximated Deep GP. Discuss similarities and differences.\n",
    "\n",
    "**Exercise 4 (Advanced): Hessian Approximation**\n",
    "For very large networks, computing the full Hessian is infeasible. Research one method for approximating the Hessian (e.g., K-FAC, diagonal approximation, block-diagonal approximation). Briefly describe how it works and its trade-offs in terms of accuracy and computational cost."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
