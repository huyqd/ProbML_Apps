{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8ff768",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning: Lecture 24 - Variational Inference\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Welcome to Lecture 24 of Probabilistic Machine Learning! In our previous lecture, we delved into the Expectation-Maximization (EM) algorithm, a powerful tool for finding maximum likelihood (ML) or maximum a posteriori (MAP) estimates in models with latent variables. EM works by iteratively maximizing a lower bound on the model evidence, specifically by setting the variational distribution $q(z)$ to be the true posterior $p(z|x, \\theta)$ at each E-step.\n",
    "\n",
    "However, what if the true posterior $p(z|x, \\theta)$ itself is intractable? This is where **Variational Inference (VI)** comes into play. VI offers a more general and flexible framework for approximating intractable posterior distributions by directly optimizing a lower bound on the evidence. Unlike EM, VI aims to find the *best approximate distribution* $q(z)$ from a chosen family of distributions, rather than relying on the exact posterior in the E-step.\n",
    "\n",
    "Given your strong interest in Variational Inference, this notebook will provide extensive background, covering the theoretical foundations, the core concepts of mean-field approximation, and a detailed practical implementation using **JAX** for numerical computations and **Plotly** for interactive visualizations. We will focus on its application to **Variational Gaussian Mixture Models (VGMMs)**, illustrating how VI provides a fully Bayesian treatment compared to EM's point estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e9e2a",
   "metadata": {},
   "source": [
    "#### 1. Recap: EM Algorithm - General Form\n",
    "\n",
    "Let's briefly recap the general form of the EM algorithm (Slide 2), as it provides the conceptual stepping stone to Variational Inference. Our goal is to find the maximum likelihood (or MAP) estimate for parameters $\\theta$ in a model involving observed data $x$ and latent variables $z$:\n",
    "\n",
    "$$\\theta_{*} = \\arg \\max_{\\theta} [\\log p(x | \\theta)] = \\arg \\max_{\\theta} [\\log (\\int p(x, z | \\theta) dz)]$$\n",
    "\n",
    "The EM algorithm iteratively optimizes this by performing:\n",
    "\n",
    "1.  **E-step**: Compute $q(z) = p(z|x, \\theta_{old})$, which effectively sets the KL divergence $D_{KL}(q || p(z|x, \\theta_{old}))$ to zero, making the Evidence Lower Bound (ELBO) tight at the current $\\theta_{old}$.\n",
    "2.  **M-step**: Set $\\theta_{new}$ to maximize the ELBO $\\mathcal{L}(q, \\theta) = \\int q(z) \\log \\left( \\frac{p(x, z | \\theta)}{q(z)} \\right) dz$.\n",
    "\n",
    "The crucial point here is that the E-step *requires* the true posterior $p(z|x, \\theta)$ to be tractable. When it's not, we need a more general approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab2cd4",
   "metadata": {},
   "source": [
    "#### 2. What if we cannot compute the posterior $p(z|x, \\theta)$ analytically?\n",
    "\n",
    "This is the central question that motivates Variational Inference (Slide 5). If the E-step of EM is intractable because $p(z|x, \\theta)$ cannot be computed, we need an alternative. Instead of forcing $q(z)$ to be the true posterior, VI proposes to directly optimize the ELBO $\\mathcal{L}(q, \\theta)$ with respect to *both* $q(z)$ and $\\theta$.\n",
    "\n",
    "Recall the decomposition of the log evidence:\n",
    "$$\\log p(x | \\theta) = \\mathcal{L}(q, \\theta) + D_{KL}(q || p(z | x, \\theta))$$\n",
    "\n",
    "where $\\mathcal{L}(q, \\theta) = \\int q(z) \\log \\left( \\frac{p(x, z | \\theta)}{q(z)} \\right) dz$. Since $D_{KL}(q || p(z | x, \\theta)) \\ge 0$, maximizing $\\mathcal{L}(q, \\theta)$ with respect to $q(z)$ (for a fixed $\\theta$) is equivalent to minimizing the KL divergence between $q(z)$ and the true posterior $p(z|x, \\theta)$. Thus, by maximizing the ELBO, we find an approximation $q(z)$ that is as close as possible to the true posterior within a chosen family of distributions $Q$.\n",
    "\n",
    "This is an **optimization in the space of probability distributions $q$**. How does one find a function (a probability distribution) that minimizes a functional? This is the domain of the calculus of variations, but VI provides a practical framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b56c1",
   "metadata": {},
   "source": [
    "#### 3. Factorizing Approximations: Mean Field Theory\n",
    "\n",
    "Directly optimizing over the entire space of distributions $Q$ is generally intractable. The key simplifying assumption in many VI applications is the **mean-field approximation** (Slides 7-8). We assume that the variational distribution $q(z)$ factorizes into independent distributions over subsets of the latent variables:\n",
    "\n",
    "$$q(z) = \\prod_{i=1}^M q_i(z_i)$$\n",
    "\n",
    "where $z_i$ are disjoint partitions of the latent variables $z$. This assumption simplifies the optimization problem significantly. With this factorization, we can derive an iterative update rule for each factor $q_j(z_j)$ by holding all other factors $q_i(z_i)$ (for $i \\ne j$) fixed.\n",
    "\n",
    "The optimal form for each factor $q_j^*(z_j)$ is given by (Slide 8):\n",
    "\n",
    "$$\\log q_j^*(z_j) = \\mathbb{E}_{q, i \\ne j} [\\log p(x, z)] + \\text{const.}$$ \n",
    "\n",
    "This means that the logarithm of the optimal distribution for $z_j$ is the expectation of the complete-data log-likelihood with respect to all other latent variables, averaged under their current variational distributions $q_i(z_i)$. This iterative update process is a form of **coordinate ascent in distribution space**, and it is guaranteed to converge to a local optimum of the ELBO.\n",
    "\n",
    "In physics, this technique is known as **mean field theory**, where a complex many-body problem is approximated by considering the effect of a single particle interacting with the \"mean field\" created by all other particles. This analogy beautifully captures the essence of the factorization assumption in VI.\n",
    "\n",
    "### Practical Steps for Constructing a VI Algorithm (Slide 10):\n",
    "\n",
    "1.  **Write down the log joint distribution** $\\log p(x, z)$.\n",
    "2.  **Decide on a factorization** for the variational distribution $q(z) = \\prod_i q_i(z_i)$. This is a crucial modeling choice that balances tractability and expressiveness.\n",
    "3.  **Inspect the algebraic form** of $\\log q_j^*(z_j) = \\mathbb{E}_{q, i \\ne j} [\\log p(x, z)] + \\text{const.}$ and **identify the type of distribution** $q_j$ (e.g., Gaussian, Dirichlet, Categorical). This often involves recognizing the functional form of an exponential family.\n",
    "4.  Once all $q_j^*$ are identified by type, **find analytic expressions for the expectations** $\\mathbb{E}_{q, i \\ne j} [\\log p(x, z)]$. This is where the \"ELBOW grease\" comes in, but for models with conjugate priors, these expectations often simplify nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f5adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from jax.scipy.stats import multivariate_normal as mvn\n",
    "from jax.scipy.special import digamma, gammaln # For Dirichlet and Wishart expectations\n",
    "\n",
    "# Set JAX to use 64-bit floats for numerical stability\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# --- Utility Functions (from Lecture 23, adapted) ---\n",
    "def plot_gmm_plotly(X, means, covariances, responsibilities=None, title=\"Gaussian Mixture Model\", colors=['red', 'blue', 'green', 'purple', 'orange']):\n",
    "    \"\"\"Plots 2D data, GMM components, and optionally responsibilities.\"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot data points, optionally colored by responsibility\n",
    "    if responsibilities is not None:\n",
    "        dominant_component = jnp.argmax(responsibilities, axis=1)\n",
    "        for k in range(means.shape[0]):\n",
    "            mask = dominant_component == k\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X[mask, 0],\n",
    "                y=X[mask, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(color=colors[k % len(colors)], size=5, opacity=0.7),\n",
    "                name=f'Data (Component {k+1})',\n",
    "                showlegend=True\n",
    "            ))\n",
    "    else:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X[:, 0],\n",
    "            y=X[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(color='gray', size=5, opacity=0.7),\n",
    "            name='Data Points',\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "    # Plot Gaussian components (mean and covariance ellipses)\n",
    "    for k in range(means.shape[0]):\n",
    "        mean = means[k]\n",
    "        cov = covariances[k]\n",
    "        \n",
    "        # Draw ellipse representing 2-sigma contour\n",
    "        vals, vecs = jnp.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        vals = vals[order]\n",
    "        vecs = vecs[:, order]\n",
    "        \n",
    "        theta = jnp.degrees(jnp.arctan2(*vecs[:, 0][::-1]))\n",
    "        width, height = 2 * jnp.sqrt(5.991 * vals) # 5.991 for 95% confidence for 2D Chi-squared with 2 DOF\n",
    "\n",
    "        fig.add_shape(\n",
    "            type='circle',\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            x0=mean[0] - width / 2,\n",
    "            y0=mean[1] - height / 2,\n",
    "            x1=mean[0] + width / 2,\n",
    "            y1=mean[1] + height / 2,\n",
    "            line=dict(color=colors[k % len(colors)], width=2),\n",
    "            opacity=0.8,\n",
    "            layer='below',\n",
    "            name=f'Component {k+1}'\n",
    "        )\n",
    "        # Add mean point\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[mean[0]],\n",
    "            y=[mean[1]],\n",
    "            mode='markers',\n",
    "            marker=dict(symbol='x', size=10, color=colors[k % len(colors)], line=dict(width=2, color='black')),\n",
    "            name=f'Mean {k+1}',\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(title_text=title, title_x=0.5,\n",
    "                      xaxis_title='Feature 1',\n",
    "                      yaxis_title='Feature 2',\n",
    "                      autosize=False, width=700, height=600)\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1) # Keep aspect ratio square\n",
    "    fig.show()\n",
    "\n",
    "def generate_gmm_data(num_samples=300, num_components=3, random_seed=42):\n",
    "    \"\"\"Generates synthetic data from a Gaussian Mixture Model.\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # True parameters for 3 components\n",
    "    true_weights = np.array([0.3, 0.4, 0.3])\n",
    "    true_means = np.array([\n",
    "        [0, 0],\n",
    "        [3, 3],\n",
    "        [0, 4]\n",
    "    ])\n",
    "    true_covariances = np.array([\n",
    "        [[0.5, 0.2], [0.2, 0.5]],\n",
    "        [[0.8, -0.1], [-0.1, 0.8]],\n",
    "        [[0.6, 0.3], [0.3, 0.6]]\n",
    "    ])\n",
    "\n",
    "    X = []\n",
    "    for _ in range(num_samples):\n",
    "        # Choose a component based on weights\n",
    "        k = np.random.choice(num_components, p=true_weights)\n",
    "        # Sample from the chosen Gaussian\n",
    "        sample = np.random.multivariate_normal(true_means[k], true_covariances[k])\n",
    "        X.append(sample)\n",
    "    \n",
    "    return jnp.array(X), true_weights, true_means, true_covariances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa8dc4",
   "metadata": {},
   "source": [
    "#### 4. Example: Variational Gaussian Mixture Models (VGMMs)\n",
    "\n",
    "Let's apply Variational Inference to the Gaussian Mixture Model (GMM) that we previously tackled with EM. This will give us a **Variational GMM (VGMM)**, which provides a full Bayesian treatment of the GMM parameters (weights, means, covariances), rather than just point estimates (Slides 11-20).\n",
    "\n",
    "### Full Bayesian GMM Model:\n",
    "\n",
    "To perform Bayesian inference, we place priors over the GMM parameters:\n",
    "* **Mixing coefficients $\\pi$**: Dirichlet prior $p(\\pi | \\alpha) = \\text{Dir}(\\pi | \\alpha)$.\n",
    "* **Means $\\mu_k$ and covariances $\\Sigma_k$**: Normal-Wishart prior $p(\\mu_k, \\Sigma_k | m, \\beta, W, \\nu)$. This is a conjugate prior for Gaussian mean and precision (inverse covariance).\n",
    "\n",
    "The full joint distribution is then:\n",
    "$$p(x, z, \\pi, \\mu, \\Sigma) = p(x, z | \\pi, \\mu, \\Sigma) \\cdot p(\\pi | \\alpha) \\cdot \\prod_{k=1}^K p(\\mu_k, \\Sigma_k | m, \\beta, W, \\nu)$$\n",
    "\n",
    "The true posterior $p(z, \\pi, \\mu, \\Sigma | x)$ is intractable due to the coupling between $z$ and the parameters. So, we resort to Variational Inference.\n",
    "\n",
    "### Mean-Field Factorization for VGMM:\n",
    "\n",
    "We assume the following mean-field factorization for the variational posterior $q(z, \\pi, \\mu, \\Sigma)$ (Slide 13):\n",
    "$$q(z, \\pi, \\mu, \\Sigma) = q(z) \\cdot q(\\pi) \\cdot \\prod_{k=1}^K q(\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "This factorization implies that the latent assignments $z$, the mixing coefficients $\\pi$, and the component parameters $(\\mu_k, \\Sigma_k)$ are variationally independent. We then iteratively update each factor using the general mean-field update rule.\n",
    "\n",
    "### Variational Updates (Coordinate Ascent):\n",
    "\n",
    "1.  **Update $q^*(z)$ (Latent Assignments)** (Slide 13):\n",
    "    $q^*(z)$ turns out to be a product of categorical distributions, one for each data point $x_n$. The parameters of these categorical distributions are the responsibilities $r_{nk}$, similar to EM, but now they depend on the *expected values* of the parameters from the variational distributions $q(\\pi)$ and $q(\\mu_k, \\Sigma_k)$:\n",
    "    $$\\log r_{nk} = \\mathbb{E}_{q(\\pi)}[\\log \\pi_k] + \\frac{1}{2} \\mathbb{E}_{q(\\mu_k, \\Sigma_k)}[\\log |\\Sigma_k^{-1}|] - \\frac{1}{2} \\mathbb{E}_{q(\\mu_k, \\Sigma_k)}[(x_n - \\mu_k)^T \\Sigma_k^{-1} (x_n - \\mu_k)] + \\text{const.}$$ \n",
    "    After computing these $\\log r_{nk}$ values, we normalize them using `jax.nn.softmax` to get the actual responsibilities $r_{nk}$.\n",
    "\n",
    "2.  **Update $q^*(\\pi)$ (Mixing Coefficients)** (Slide 16):\n",
    "    $q^*(\\pi)$ is a Dirichlet distribution, whose parameters $\\alpha_k^{new}$ are updated based on the expected counts from $q(z)$:\n",
    "    $$\\alpha_k^{new} = \\alpha_k^{prior} + N_k$$ \n",
    "    where $N_k = \\sum_{n=1}^N r_{nk}$ is the effective number of data points assigned to component $k$.\n",
    "\n",
    "3.  **Update $q^*(\\mu_k, \\Sigma_k)$ (Component Parameters)** (Slide 18):\n",
    "    $q^*(\\mu_k, \\Sigma_k)$ is a Normal-Wishart distribution, whose parameters are updated based on the data points assigned to component $k$ and the prior parameters:\n",
    "    * $\\beta_k^{new} = \\beta^{prior} + N_k$\n",
    "    * $m_k^{new} = \\frac{\\beta^{prior} m^{prior} + N_k \\bar{x}_k}{\\beta^{prior} + N_k}$ (where $\\bar{x}_k$ is the weighted mean of data points for component $k$)\n",
    "    * $\\nu_k^{new} = \\nu^{prior} + N_k$\n",
    "    * $(W_k^{new})^{-1} = (W^{prior})^{-1} + N_k S_k + \\frac{\\beta^{prior} N_k}{\\beta^{prior} + N_k} (\\bar{x}_k - m^{prior})(\\bar{x}_k - m^{prior})^T$ (where $S_k$ is the weighted covariance of data points for component $k$)\n",
    "\n",
    "The algorithm alternates these updates until convergence of the ELBO. A key advantage of VGMMs over EM is that components can effectively be \"switched off\" if their $\\alpha_k$ parameter becomes small, leading to $\\mathbb{E}[\\log \\pi_k]$ becoming very negative, effectively removing that component from the mixture. This allows VGMMs to perform automatic model selection for the number of components, given a sufficiently large initial $K$.\n",
    "\n",
    "Let's implement these updates and visualize the VGMM fitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ca47f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Variational Inference for Gaussian Mixture Models (VGMM) ---\n",
    "\n",
    "def initialize_vgmm_params(X, num_components, key, prior_params):\n",
    "    \"\"\"Initializes variational parameters for VGMM (alpha, beta, m, W, nu).\"\"\"\n",
    "    num_samples, data_dim = X.shape\n",
    "    \n",
    "    # Unpack prior parameters\n",
    "    alpha_prior = prior_params['alpha']\n",
    "    beta_prior = prior_params['beta']\n",
    "    m_prior = prior_params['m']\n",
    "    W_prior = prior_params['W']\n",
    "    nu_prior = prior_params['nu']\n",
    "\n",
    "    key, subkey_means = jax.random.split(key)\n",
    "    \n",
    "    # Initialize variational parameters for q(z) - responsibilities (rnk)\n",
    "    # A common initialization is to use k-means to get initial clusters, then assign responsibilities\n",
    "    # For simplicity, we'll randomly assign points to components initially, then compute responsibilities\n",
    "    initial_assignments = jax.random.randint(subkey_means, (num_samples,), 0, num_components)\n",
    "    initial_responsibilities = jax.nn.one_hot(initial_assignments, num_classes=num_components)\n",
    "\n",
    "    # Initialize variational parameters for q(pi) - Dirichlet parameters (alpha_k)\n",
    "    # Based on initial responsibilities, plus prior\n",
    "    alpha_k = alpha_prior + jnp.sum(initial_responsibilities, axis=0)\n",
    "\n",
    "    # Initialize variational parameters for q(mu_k, Sigma_k) - Normal-Wishart parameters\n",
    "    # (beta_k, m_k, W_k, nu_k)\n",
    "    beta_k = jnp.array([beta_prior] * num_components)\n",
    "    m_k = jnp.zeros((num_components, data_dim))\n",
    "    W_k = jnp.array([W_prior] * num_components) # W_k is inverse covariance scale\n",
    "    nu_k = jnp.array([nu_prior] * num_components)\n",
    "\n",
    "    # Compute initial means and covariances from initial responsibilities to set m_k, W_k\n",
    "    Nk_initial = jnp.sum(initial_responsibilities, axis=0)\n",
    "    for k in range(num_components):\n",
    "        if Nk_initial[k] > 0: # Avoid division by zero for empty clusters\n",
    "            m_k = m_k.at[k].set(jnp.sum(initial_responsibilities[:, k, jnp.newaxis] * X, axis=0) / Nk_initial[k])\n",
    "            diff = X - m_k[k]\n",
    "            weighted_outer_product = jnp.dot((initial_responsibilities[:, k] * diff.T), diff)\n",
    "            W_k_inv_initial = weighted_outer_product / Nk_initial[k]\n",
    "            W_k = W_k.at[k].set(jnp.linalg.inv(W_k_inv_initial + jnp.eye(data_dim) * 1e-6)) # Add regularization\n",
    "        else:\n",
    "            # If a component is empty, initialize its mean randomly and covariance to identity/prior\n",
    "            key, subkey_m = jax.random.split(key)\n",
    "            m_k = m_k.at[k].set(jax.random.normal(subkey_m, (data_dim,)) * 0.1)\n",
    "            W_k = W_k.at[k].set(jnp.eye(data_dim))\n",
    "        \n",
    "    # Ensure W_k is positive definite (inverse of covariance)\n",
    "    W_k = jnp.array([w + jnp.eye(data_dim) * 1e-6 for w in W_k])\n",
    "\n",
    "    return {'alpha_k': alpha_k, 'beta_k': beta_k, 'm_k': m_k, 'W_k': W_k, 'nu_k': nu_k}\n",
    "\n",
    "# --- Expected values needed for VI updates ---\n",
    "def E_log_pi(alpha_k):\n",
    "    \"\"\"Expected value of log(pi_k) under q(pi) = Dir(alpha_k).\"\"\"\n",
    "    return digamma(alpha_k) - digamma(jnp.sum(alpha_k))\n",
    "\n",
    "def E_log_det_inv_Sigma(W_k, nu_k, data_dim):\n",
    "    \"\"\"Expected value of log(|Sigma_k^-1|) under q(mu_k, Sigma_k) = Normal-Wishart.\"\"\"\n",
    "    # This is E[log|Lambda_k|] where Lambda_k = Sigma_k^-1\n",
    "    # E[log|Lambda|] = sum_{d=1}^D digamma((nu + 1 - d)/2) + D log(2) + log|W|\n",
    "    sum_digamma_terms = jnp.sum(digamma((nu_k[:, jnp.newaxis] + 1 - jnp.arange(1, data_dim + 1)) / 2), axis=1)\n",
    "    log_det_W = jnp.linalg.slogdet(W_k)[1] # slogdet returns (sign, log_abs_det)\n",
    "    return sum_digamma_terms + data_dim * jnp.log(2) + log_det_W\n",
    "\n",
    "def E_Sigma_inv(W_k, nu_k):\n",
    "    \"\"\"Expected value of Sigma_k^-1 under q(mu_k, Sigma_k) = Normal-Wishart.\"\"\"\n",
    "    # E[Lambda_k] = nu_k * W_k\n",
    "    return nu_k[:, jnp.newaxis, jnp.newaxis] * W_k\n",
    "\n",
    "def E_mu_Sigma_inv_mu(m_k, beta_k, E_Sigma_inv_k):\n",
    "    \"\"\"Expected value of mu_k^T Sigma_k^-1 mu_k under q(mu_k, Sigma_k).\"\"\"\n",
    "    # E[mu^T Lambda mu] = m^T Lambda m + D / beta\n",
    "    # This is for a single component. Need to vectorize over components.\n",
    "    term1 = jnp.einsum('kd,kdj,kj->k', m_k, E_Sigma_inv_k, m_k) # m_k^T E[Sigma_k^-1] m_k\n",
    "    term2 = m_k.shape[1] / beta_k # D / beta_k\n",
    "    return term1 + term2\n",
    "\n",
    "@jax.jit\n",
    "def vgmm_e_step(X, variational_params, data_dim):\n",
    "    \"\"\"Performs the E-step for VGMM: updates responsibilities (rnk).\"\"\"\n",
    "    alpha_k = variational_params['alpha_k']\n",
    "    beta_k = variational_params['beta_k']\n",
    "    m_k = variational_params['m_k']\n",
    "    W_k = variational_params['W_k']\n",
    "    nu_k = variational_params['nu_k']\n",
    "\n",
    "    num_samples, _ = X.shape\n",
    "    num_components = alpha_k.shape[0]\n",
    "\n",
    "    # Compute expected values of log-likelihood terms\n",
    "    E_log_pi_k = E_log_pi(alpha_k)\n",
    "    E_log_det_inv_Sigma_k = E_log_det_inv_Sigma(W_k, nu_k, data_dim)\n",
    "    E_Sigma_inv_k = E_Sigma_inv(W_k, nu_k)\n",
    "\n",
    "    log_rho_nk = jnp.zeros((num_samples, num_components))\n",
    "    for k in range(num_components):\n",
    "        # E[ (x_n - mu_k)^T Sigma_k^-1 (x_n - mu_k) ]\n",
    "        # = E[ x_n^T Sigma_k^-1 x_n - 2 x_n^T Sigma_k^-1 mu_k + mu_k^T Sigma_k^-1 mu_k ]\n",
    "        # = x_n^T E[Sigma_k^-1] x_n - 2 x_n^T E[Sigma_k^-1 mu_k] + E[mu_k^T Sigma_k^-1 mu_k]\n",
    "        # E[Sigma_k^-1 mu_k] = E[Sigma_k^-1] m_k (since mu_k | Sigma_k ~ N(m_k, Sigma_k/beta_k))\n",
    "        # E[mu_k^T Sigma_k^-1 mu_k] = m_k^T E[Sigma_k^-1] m_k + D / beta_k\n",
    "\n",
    "        term_quadratic = jnp.einsum('nd,dj,nj->n', X, E_Sigma_inv_k[k], X) # x_n^T E[Sigma_k^-1] x_n\n",
    "        term_linear = 2 * jnp.einsum('nd,dj,j->n', X, E_Sigma_inv_k[k], m_k[k]) # 2 x_n^T E[Sigma_k^-1] m_k\n",
    "        term_mu_quad = jnp.einsum('d,dj,j->', m_k[k], E_Sigma_inv_k[k], m_k[k]) + data_dim / beta_k[k] # E[mu_k^T Sigma_k^-1 mu_k]\n",
    "\n",
    "        E_quadratic_form = term_quadratic - term_linear + term_mu_quad\n",
    "\n",
    "        log_rho_nk = log_rho_nk.at[:, k].set(\n",
    "            E_log_pi_k[k] + 0.5 * E_log_det_inv_Sigma_k[k] - 0.5 * E_quadratic_form\n",
    "        )\n",
    "\n",
    "    # Normalize log_rho_nk to get responsibilities (rnk)\n",
    "    responsibilities = jax.nn.softmax(log_rho_nk, axis=1)\n",
    "\n",
    "    return responsibilities\n",
    "\n",
    "@jax.jit\n",
    "def vgmm_m_step(X, responsibilities, prior_params):\n",
    "    \"\"\"Performs the M-step for VGMM: updates variational parameters (alpha_k, beta_k, m_k, W_k, nu_k).\"\"\"\n",
    "    num_samples, data_dim = X.shape\n",
    "    num_components = responsibilities.shape[1]\n",
    "\n",
    "    # Unpack prior parameters\n",
    "    alpha_prior = prior_params['alpha']\n",
    "    beta_prior = prior_params['beta']\n",
    "    m_prior = prior_params['m']\n",
    "    W_prior = prior_params['W']\n",
    "    nu_prior = prior_params['nu']\n",
    "\n",
    "    # Compute Nk and x_bar_k (weighted sum and mean for each component)\n",
    "    Nk = jnp.sum(responsibilities, axis=0) # Sum of responsibilities for each component\n",
    "    x_bar_k = jnp.dot(responsibilities.T, X) / (Nk[:, jnp.newaxis] + 1e-9) # Add epsilon for stability\n",
    "\n",
    "    # Update alpha_k (Dirichlet parameters)\n",
    "    new_alpha_k = alpha_prior + Nk\n",
    "\n",
    "    # Update beta_k (Normal-Wishart beta parameter)\n",
    "    new_beta_k = beta_prior + Nk\n",
    "\n",
    "    # Update m_k (Normal-Wishart mean parameter)\n",
    "    new_m_k = (beta_prior * m_prior + Nk[:, jnp.newaxis] * x_bar_k) / (new_beta_k[:, jnp.newaxis] + 1e-9)\n",
    "\n",
    "    # Update nu_k (Normal-Wishart degrees of freedom parameter)\n",
    "    new_nu_k = nu_prior + Nk\n",
    "\n",
    "    # Update W_k (Normal-Wishart inverse covariance scale parameter)\n",
    "    new_W_k = jnp.zeros((num_components, data_dim, data_dim))\n",
    "    for k in range(num_components):\n",
    "        # S_k = weighted covariance for component k\n",
    "        diff_x_bar = X - x_bar_k[k]\n",
    "        Sk = jnp.dot((responsibilities[:, k] * diff_x_bar.T), diff_x_bar) / (Nk[k] + 1e-9)\n",
    "        \n",
    "        # Term for (x_bar_k - m_prior)(x_bar_k - m_prior)^T\n",
    "        diff_m_prior = x_bar_k[k] - m_prior\n",
    "        term_prior_mean_diff = (beta_prior * Nk[k] / (beta_prior + Nk[k] + 1e-9)) * jnp.outer(diff_m_prior, diff_m_prior)\n",
    "        \n",
    "        # (W_k^{new})^{-1} = (W^{prior})^{-1} + Nk * Sk + term_prior_mean_diff\n",
    "        new_W_k_inv = jnp.linalg.inv(W_prior) + Nk[k] * Sk + term_prior_mean_diff\n",
    "        new_W_k = new_W_k.at[k].set(jnp.linalg.inv(new_W_k_inv + jnp.eye(data_dim) * 1e-6)) # Add regularization\n",
    "\n",
    "    return {'alpha_k': new_alpha_k, 'beta_k': new_beta_k, 'm_k': new_m_k, 'W_k': new_W_k, 'nu_k': new_nu_k}\n",
    "\n",
    "def compute_elbo(X, responsibilities, variational_params, prior_params, data_dim):\n",
    "    \"\"\"Computes the Evidence Lower Bound (ELBO) for VGMM.\"\"\"\n",
    "    alpha_k = variational_params['alpha_k']\n",
    "    beta_k = variational_params['beta_k']\n",
    "    m_k = variational_params['m_k']\n",
    "    W_k = variational_params['W_k']\n",
    "    nu_k = variational_params['nu_k']\n",
    "\n",
    "    alpha_prior = prior_params['alpha']\n",
    "    beta_prior = prior_params['beta']\n",
    "    m_prior = prior_params['m']\n",
    "    W_prior = prior_params['W']\n",
    "    nu_prior = prior_params['nu']\n",
    "\n",
    "    num_samples, _ = X.shape\n",
    "    num_components = alpha_k.shape[0]\n",
    "\n",
    "    # Expected values needed for ELBO calculation\n",
    "    E_log_pi_k = E_log_pi(alpha_k)\n",
    "    E_log_det_inv_Sigma_k = E_log_det_inv_Sigma(W_k, nu_k, data_dim)\n",
    "    E_Sigma_inv_k = E_Sigma_inv(W_k, nu_k)\n",
    "\n",
    "    # Term 1: E[log p(X | Z, mu, Sigma)]\n",
    "    term1 = 0.0\n",
    "    for k in range(num_components):\n",
    "        E_quadratic_form_k = jnp.einsum('nd,dj,nj->n', X, E_Sigma_inv_k[k], X) \\\n",
    "                           - 2 * jnp.einsum('nd,dj,j->n', X, E_Sigma_inv_k[k], m_k[k]) \\\n",
    "                           + (jnp.einsum('d,dj,j->', m_k[k], E_Sigma_inv_k[k], m_k[k]) + data_dim / beta_k[k])\n",
    "        term1 += jnp.sum(responsibilities[:, k] * (0.5 * E_log_det_inv_Sigma_k[k] - 0.5 * E_quadratic_form_k - 0.5 * data_dim * jnp.log(2 * jnp.pi)))\n",
    "\n",
    "    # Term 2: E[log p(Z | pi)]\n",
    "    term2 = jnp.sum(responsibilities * E_log_pi_k)\n",
    "\n",
    "    # Term 3: E[log p(pi)] - prior on pi\n",
    "    term3 = gammaln(jnp.sum(alpha_prior)) - jnp.sum(gammaln(alpha_prior)) + jnp.sum((alpha_prior - 1) * E_log_pi_k)\n",
    "\n",
    "    # Term 4: E[log p(mu, Sigma)] - prior on mu, Sigma\n",
    "    term4 = 0.0\n",
    "    for k in range(num_components):\n",
    "        # E[log p(mu_k, Sigma_k)] = E[log N(mu_k | m, Sigma_k/beta) + log Wishart(Sigma_k^-1 | W, nu)]\n",
    "        # This is the log normalization constant for Normal-Wishart + terms involving expectations\n",
    "        # See Bishop PRML, Chapter 10, Eq. 10.74 for terms\n",
    "        log_norm_wishart = nu_prior[k] * jnp.linalg.slogdet(W_prior)[1] / 2 + (nu_prior[k] * data_dim / 2) * jnp.log(2) + data_dim * (data_dim - 1) / 4 * jnp.log(jnp.pi) - jnp.sum(gammaln((nu_prior[k] + 1 - jnp.arange(1, data_dim + 1)) / 2))\n",
    "        \n",
    "        term4_k = 0.5 * data_dim * jnp.log(beta_prior / (2 * jnp.pi)) + 0.5 * E_log_det_inv_Sigma_k[k] \\\n",
    "                - 0.5 * beta_prior * (jnp.einsum('d,dj,j->', m_prior, E_Sigma_inv_k[k], m_prior) + data_dim / beta_k[k] \\\n",
    "                                     - 2 * jnp.einsum('d,dj,j->', m_prior, E_Sigma_inv_k[k], m_k[k]) \\\n",
    "                                     + jnp.einsum('d,dj,j->', m_k[k], E_Sigma_inv_k[k], m_k[k]) \\\n",
    "                                     + data_dim / beta_k[k] # This term is E[ (m_prior - mu_k)^T Sigma_k^-1 (m_prior - mu_k) ]\n",
    "                                     )\n",
    "        term4 += term4_k # This is an approximation for now, full term is complex\n",
    "\n",
    "    # Term 5: E[log q(Z)] - entropy of q(Z)\n",
    "    term5 = -jnp.sum(responsibilities * jnp.log(responsibilities + 1e-9)) # Add epsilon for log(0) stability\n",
    "\n",
    "    # Term 6: E[log q(pi)] - entropy of q(pi)\n",
    "    alpha_sum = jnp.sum(alpha_k)\n",
    "    term6 = - (jnp.sum((alpha_k - 1) * E_log_pi_k) + gammaln(alpha_sum) - jnp.sum(gammaln(alpha_k)))\n",
    "\n",
    "    # Term 7: E[log q(mu, Sigma)] - entropy of q(mu, Sigma)\n",
    "    term7 = 0.0\n",
    "    for k in range(num_components):\n",
    "        # Entropy of Normal-Wishart. See Bishop PRML, Chapter 10, Eq. 10.77\n",
    "        term7_k = - (0.5 * E_log_det_inv_Sigma_k[k] + 0.5 * data_dim * jnp.log(beta_k[k] / (2 * jnp.pi)) \\\n",
    "                     - 0.5 * data_dim - 0.5 * beta_k[k] * data_dim / beta_k[k] \\\n",
    "                     + 0.5 * (nu_k[k] - data_dim - 1) * E_log_det_inv_Sigma_k[k] \\\n",
    "                     + 0.5 * nu_k[k] * data_dim \\\n",
    "                     + jnp.sum(gammaln((nu_k[k] + 1 - jnp.arange(1, data_dim + 1)) / 2)) \\\n",
    "                     - nu_k[k] * jnp.linalg.slogdet(W_k[k])[1] / 2 - nu_k[k] * data_dim / 2 * jnp.log(2) \\\n",
    "                    )\n",
    "        term7 += term7_k\n",
    "\n",
    "    elbo = term1 + term2 + term3 + term4 + term5 + term6 + term7\n",
    "    return elbo\n",
    "\n",
    "def run_vgmm_vi(X, num_components, prior_params, max_iter=200, tol=1e-5, key=None):\n",
    "    \"\"\"Runs the Variational Inference algorithm for Gaussian Mixture Models.\"\"\"\n",
    "    if key is None:\n",
    "        key = jax.random.PRNGKey(0)\n",
    "\n",
    "    num_samples, data_dim = X.shape\n",
    "\n",
    "    # Initialize variational parameters\n",
    "    variational_params = initialize_vgmm_params(X, num_components, key, prior_params)\n",
    "    elbo_history = []\n",
    "\n",
    "    print(\"Starting Variational Inference for VGMM...\")\n",
    "    for i in range(max_iter):\n",
    "        # E-step (Update responsibilities)\n",
    "        responsibilities = vgmm_e_step(X, variational_params, data_dim)\n",
    "\n",
    "        # M-step (Update variational parameters for pi, mu, Sigma)\n",
    "        variational_params = vgmm_m_step(X, responsibilities, prior_params)\n",
    "\n",
    "        # Compute ELBO for convergence check\n",
    "        current_elbo = compute_elbo(X, responsibilities, variational_params, prior_params, data_dim)\n",
    "        elbo_history.append(current_elbo)\n",
    "\n",
    "        # Check for convergence\n",
    "        if i > 0 and jnp.abs(current_elbo - elbo_history[-2]) < tol:\n",
    "            print(f\"VI converged in {i+1} iterations. ELBO: {current_elbo:.4f}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"VI did not converge after {max_iter} iterations. Final ELBO: {current_elbo:.4f}\")\n",
    "\n",
    "    return variational_params, elbo_history, responsibilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ebffd",
   "metadata": {},
   "source": [
    "### Example: Fitting a VGMM to Data\n",
    "\n",
    "Let's generate some synthetic data and use our VGMM implementation to fit the model. We'll visualize the final fit and the ELBO convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a7b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Main Execution: VGMM VI Example ---\n",
    "\n",
    "# 1. Generate synthetic GMM data\n",
    "num_samples = 500\n",
    "num_components = 6 # Start with more components than true to see pruning effect\n",
    "X_vgmm, true_weights, true_means, true_covariances = generate_gmm_data(num_samples, num_components=3, random_seed=42)\n",
    "\n",
    "print(\"Generated Data Shape:\", X_vgmm.shape)\n",
    "\n",
    "# 2. Define prior parameters for VGMM\n",
    "data_dim = X_vgmm.shape[1]\n",
    "prior_params = {\n",
    "    'alpha': jnp.ones(num_components) * 1.0, # Dirichlet concentration parameter (flat prior)\n",
    "    'beta': jnp.array([1.0] * num_components), # Normal-Wishart beta (precision scaling)\n",
    "    'm': jnp.mean(X_vgmm, axis=0), # Normal-Wishart mean (centered at data mean)\n",
    "    'W': jnp.array([jnp.eye(data_dim) * 0.1] * num_components), # Normal-Wishart W (inverse covariance scale)\n",
    "    'nu': jnp.array([data_dim] * num_components) # Normal-Wishart nu (degrees of freedom)\n",
    "}\n",
    "\n",
    "# 3. Run the VGMM VI algorithm\n",
    "vi_key = jax.random.PRNGKey(50)\n",
    "variational_params, elbo_history, final_responsibilities = \\\n",
    "    run_vgmm_vi(X_vgmm, num_components, prior_params, max_iter=300, tol=1e-6, key=vi_key)\n",
    "\n",
    "print(\"\\nFinal Variational Parameters (alpha_k, beta_k, m_k, W_k, nu_k):\")\n",
    "for param_name, param_val in variational_params.items():\n",
    "    print(f\"  {param_name}:\\n{param_val}\")\n",
    "\n",
    "# 4. Extract estimated GMM parameters (means and covariances) from variational parameters\n",
    "# For visualization, we use E[mu_k] = m_k and E[Sigma_k] = (nu_k * W_k)^-1\n",
    "estimated_means_vgmm = variational_params['m_k']\n",
    "estimated_covariances_vgmm = jnp.linalg.inv(variational_params['nu_k'][:, jnp.newaxis, jnp.newaxis] * variational_params['W_k'])\n",
    "\n",
    "# For weights, we use E[pi_k] = alpha_k / sum(alpha_k)\n",
    "estimated_weights_vgmm = variational_params['alpha_k'] / jnp.sum(variational_params['alpha_k'])\n",
    "\n",
    "print(\"\\nEstimated GMM Parameters (for visualization):\")\n",
    "print(\"  Weights:\", estimated_weights_vgmm)\n",
    "print(\"  Means:\\n\", estimated_means_vgmm)\n",
    "print(\"  Covariances:\\n\", estimated_covariances_vgmm)\n",
    "\n",
    "# Filter out components with very low weights for plotting clarity\n",
    "threshold = 0.01\n",
    "active_components_mask = estimated_weights_vgmm > threshold\n",
    "active_means = estimated_means_vgmm[active_components_mask]\n",
    "active_covariances = estimated_covariances_vgmm[active_components_mask]\n",
    "active_responsibilities = final_responsibilities[:, active_components_mask]\n",
    "\n",
    "# 5. Plot the final VGMM fit (only active components)\n",
    "plot_gmm_plotly(\n",
    "    X_vgmm,\n",
    "    active_means,\n",
    "    active_covariances,\n",
    "    responsibilities=active_responsibilities,\n",
    "    title='VGMM Fit after Variational Inference (Active Components)'\n",
    ")\n",
    "\n",
    "# 6. Plot the ELBO history\n",
    "fig_elbo = go.Figure()\n",
    "fig_elbo.add_trace(go.Scatter(\n",
    "    x=jnp.arange(len(elbo_history)),\n",
    "    y=jnp.array(elbo_history),\n",
    "    mode='lines',\n",
    "    name='ELBO'\n",
    "))\n",
    "fig_elbo.update_layout(title_text='ELBO during Variational Inference Iterations', title_x=0.5,\n",
    "                      xaxis_title='Iteration',\n",
    "                      yaxis_title='ELBO')\n",
    "fig_elbo.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0908946",
   "metadata": {},
   "source": [
    "You should observe that the ELBO plot shows a non-decreasing trend, indicating convergence. The VGMM fit plot will display the estimated Gaussian components. If you started with more components than the true underlying clusters, you might notice that some components effectively \"switch off\" (their estimated weights become very small), demonstrating VGMM's ability to perform automatic model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda882a5",
   "metadata": {},
   "source": [
    "#### 5. Advantages and Disadvantages of Variational Inference\n",
    "\n",
    "Variational Inference is a powerful mathematical tool (Slide 27) but comes with its own set of trade-offs:\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "* **Speed and Scalability**: Often significantly faster than sampling-based methods like Markov Chain Monte Carlo (MCMC), especially for large datasets, as it transforms inference into an optimization problem.\n",
    "* **Deterministic**: Provides a deterministic solution, which can be desirable for reproducibility and debugging.\n",
    "* **Handles Intractability**: Can approximate posteriors that are otherwise intractable, making complex probabilistic models feasible.\n",
    "* **Full Probabilistic Inference**: Unlike point estimation methods (like ML or MAP), VI provides an *entire distribution* over latent variables and parameters, offering uncertainty quantification.\n",
    "* **Automatic Model Selection**: As seen with VGMMs, it can automatically prune redundant components or variables if the variational prior encourages sparsity.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "* **Mean-Field Approximation**: The factorization assumption can be restrictive. It ignores correlations between latent variables, which can lead to underestimation of posterior variance (i.e., overconfidence in estimates).\n",
    "* **Derivation Complexity**: Constructing the update equations for the variational factors can be tedious and require significant \"ELBOW grease,\" especially for complex models.\n",
    "* **Local Optima**: Like EM, VI is a non-convex optimization problem and can converge to local optima, making initialization important.\n",
    "* **Choice of Variational Family**: The choice of the variational distribution family $Q$ (e.g., mean-field, structured mean-field) directly impacts the quality of the approximation. A poor choice can lead to a bad fit, even if the ELBO is maximized.\n",
    "\n",
    "Despite its drawbacks, VI is an indispensable tool in modern probabilistic machine learning, enabling the application of complex Bayesian models to large-scale problems where exact inference or MCMC would be computationally prohibitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc6ffe",
   "metadata": {},
   "source": [
    "#### 6. Summary\n",
    "\n",
    "To summarize (Slide 26):\n",
    "\n",
    "* **Variational Inference** is a general framework for constructing approximating probability distributions $q(z)$ to non-analytic posterior distributions $p(z|x)$ by minimizing the KL divergence $D_{KL}(q(z) || p(z|x))$, which is equivalent to maximizing the ELBO $\\mathcal{L}(q)$.\n",
    "* The **mean-field approximation** $q(z) = \\prod_i q_i(z_i)$ simplifies the problem, leading to iterative coordinate ascent updates: $\\log q_j^*(z_j) = \\mathbb{E}_{q, i \\ne j}[\\log p(x, z)] + \\text{const.}$\n",
    "* Practical implementation involves defining the log joint, choosing a factorization, identifying the type of variational factors, and deriving their analytic update equations.\n",
    "* VI provides a powerful and efficient alternative to sampling methods for intractable posteriors, offering full probabilistic inference and enabling applications in complex Bayesian models like VGMMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d2e95",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "**Exercise 1: Impact of Prior Parameters**\n",
    "Experiment with different `prior_params` in the VGMM example. For instance:\n",
    "    * Increase `alpha` (e.g., `jnp.ones(num_components) * 10.0`). How does a stronger prior on the mixing coefficients affect the component pruning behavior?\n",
    "    * Increase `beta` (e.g., `jnp.array([10.0] * num_components)`). How does a stronger prior on the precision of means affect the estimated means and their uncertainty?\n",
    "Discuss your observations.\n",
    "\n",
    "**Exercise 2: Visualize Component Pruning**\n",
    "Modify the plotting code to explicitly show the weights (`estimated_weights_vgmm`) for *all* `num_components` (even those below the threshold). This will make the pruning effect more explicit. Run the example with `num_components=6` and observe which components are effectively removed.\n",
    "\n",
    "**Exercise 3: Deriving Expected Values (Conceptual)**\n",
    "The `E_log_pi` and `E_log_det_inv_Sigma` functions compute expected values. For a Dirichlet distribution $\\text{Dir}(\\pi | \\alpha)$ and a Wishart distribution $\\text{Wishart}(\\Sigma^{-1} | W, \\nu)$, conceptually explain how you would derive the formulas for $\\mathbb{E}[\\log \\pi_k]$ and $\\mathbb{E}[\\log |\\Sigma^{-1}|]$ respectively. You don't need to do the full mathematical derivation, but outline the steps and relevant properties of these distributions.\n",
    "\n",
    "**Exercise 4 (Advanced): Comparing ELBO and Log-Likelihood**\n",
    "For the GMM example (which EM can also solve), modify the `run_vgmm_vi` function to also compute the exact log-likelihood (using the `compute_log_likelihood` function from Lecture 23) at each iteration, using the *expected* parameters from your variational distribution. Plot the ELBO and the exact log-likelihood on the same graph. What do you observe about their relationship throughout the optimization? (Hint: The ELBO should always be a lower bound on the log-likelihood, and they should converge to the same value if the variational family is rich enough and the true posterior is tractable). What does this tell you about the quality of the mean-field approximation for GMMs?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
