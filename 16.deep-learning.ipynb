{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning: Lecture 16 - Deep Learning\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Welcome to Lecture 16 of Probabilistic Machine Learning, where we delve into the world of **Deep Learning**! This lecture connects the concepts we've learned about probabilistic models, Gaussian Processes, and inference to the highly influential field of deep neural networks. We'll explore the fundamental definitions, training paradigms, and compare the strengths and weaknesses of deep learning against our established probabilistic frameworks.\n",
    "\n",
    "This notebook will guide you through the key ideas from the lecture slides, providing explanations and setting the stage for understanding deep learning from a probabilistic perspective. We'll maintain our use of **JAX** for any underlying numerical concepts and **Plotly** for visualizations, ensuring consistency with previous lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Recap: The Course So Far\n",
    "\n",
    "Before diving into deep learning, let's quickly recap the core themes and models we've covered in this course (as per Slide 2):\n",
    "\n",
    "* **Learning is Inference**: We've consistently framed learning as a process of re-weighing a space of hypotheses using likelihood functions, fundamentally rooted in Bayes' theorem. Probability theory provides the mathematical framework for tracking volume/measure correctly.\n",
    "\n",
    "* **Exponential Families**: These are parametric probability distributions that allow for tractable inference. They simplify calculations and provide a foundation for many statistical models.\n",
    "\n",
    "* **Gaussian Distributions**: A particularly important exponential family where inference often reduces to linear algebra. They are central to:\n",
    "    * **Linear Regression**: Learning general linear functions $f: X \\to \\mathbb{R}$, $f(x) = \\phi(x)^T w$.\n",
    "    * **Gaussian Process (GP) Regression**: Abstracting linear functions to a functional form that doesn't require explicit features, using inner products (kernels, covariance functions) $k(\\bullet,\\circ) = \\phi(\\bullet)^T \\Sigma \\phi(\\circ)$. This leads to a nonparametric model.\n",
    "\n",
    "* **Classification with Sigmoid Likelihood**: For classification problems (functions $f: X \\to [0,1]^C$), we adapted this framework by considering a sigmoid likelihood (logistic regression). This necessitated approximate inference, often realized through **Laplace approximations**, as exact posteriors are non-Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Deep Learning: A Vague Definition\n",
    "\n",
    "Deep learning, while widely used, can be defined in various ways. For our purposes in this lecture, we will consider a deep neural network as (as per Slide 3):\n",
    "\n",
    "A function $f(x, \\theta): X \\times \\mathbb{R}^D \\to \\mathbb{R}^F$, parametrized by parameters $\\theta \\in \\mathbb{R}^D$ and mapping inputs $x \\in \\mathbb{X}$ to outputs $f(x, \\theta) \\in \\mathbb{R}^F$.\n",
    "\n",
    "Such functions are often realized in a **hierarchical fashion**:\n",
    "\n",
    "$$f(x, \\theta) = b_L + w_L \\sigma(b_{L-1} + w_{L-1} \\sigma(\\cdot\\cdot\\cdot\\sigma(b_0 + w_0 x)))$$\n",
    "\n",
    "This structure is parametrized by weights and biases $\\theta = [b_i, w_i]_{i=0,...,L}$ and uses nonlinearities $\\sigma$ (e.g., ReLU, tanh, sigmoid). We will ignore the intricate details of specific architectures here, as they are typically covered in dedicated deep learning courses.\n",
    "\n",
    "The core idea is that deep networks learn complex, hierarchical representations of data through multiple layers of non-linear transformations.\n",
    "\n",
    "Let's illustrate a simple multi-layer perceptron (MLP) using JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set JAX to use 64-bit floats for numerical stability\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "# --- Utility Functions (re-defined for clarity and JAX compatibility) ---\n",
    "def sigmoid(f):\n",
    "    \"\"\"Logistic sigmoid function.\"\"\"\n",
    "    return 1 / (1 + jnp.exp(-f))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "\n",
    "def rbf_kernel(X1, X2, length_scale=1.0):\n",
    "    \"\"\"Radial Basis Function (RBF) kernel.\"\"\"\n",
    "    sqdist = jnp.sum(X1**2, 1)[:, None] + jnp.sum(X2**2, 1) - 2 * jnp.dot(X1, X2.T)\n",
    "    return jnp.exp(-0.5 * (1 / length_scale**2) * sqdist)\n",
    "\n",
    "\n",
    "def generate_data(type=\"separable\", n_samples=100):\n",
    "    \"\"\"Generates synthetic 2D classification data (using numpy for random generation).\"\"\"\n",
    "    np.random.seed(42)  # Ensure reproducibility for data generation\n",
    "    if type == \"separable\":\n",
    "        mean1 = [-1, 0.5]\n",
    "        cov1 = [[0.5, 0.2], [0.2, 0.5]]\n",
    "        data1 = np.random.multivariate_normal(mean1, cov1, n_samples // 2)\n",
    "        labels1 = np.ones(n_samples // 2) * -1\n",
    "        mean2 = [1, -0.5]\n",
    "        cov2 = [[0.5, 0.2], [0.2, 0.5]]\n",
    "        data2 = np.random.multivariate_normal(mean2, cov2, n_samples // 2)\n",
    "        labels2 = np.ones(n_samples // 2)\n",
    "    elif type == \"overlapping\":\n",
    "        mean1 = [-0.5, 0.5]\n",
    "        cov1 = [[1.0, 0.5], [0.5, 1.0]]\n",
    "        data1 = np.random.multivariate_normal(mean1, cov1, n_samples // 2)\n",
    "        labels1 = np.ones(n_samples // 2) * -1\n",
    "        mean2 = [0.5, -0.5]\n",
    "        cov2 = [[1.0, 0.5], [0.5, 1.0]]\n",
    "        data2 = np.random.multivariate_normal(mean2, cov2, n_samples // 2)\n",
    "        labels2 = np.ones(n_samples // 2)\n",
    "    elif type == \"intermingled\":\n",
    "        r1 = np.random.rand(n_samples // 2) * 2\n",
    "        theta1 = np.random.rand(n_samples // 2) * 2 * np.pi\n",
    "        data1 = np.array(\n",
    "            [\n",
    "                r1 * np.cos(theta1) + np.random.randn(n_samples // 2) * 0.2,\n",
    "                r1 * np.sin(theta1) + np.random.randn(n_samples // 2) * 0.2,\n",
    "            ]\n",
    "        ).T\n",
    "        labels1 = np.ones(n_samples // 2) * -1\n",
    "        r2 = np.random.rand(n_samples // 2) * 2 + 1.5\n",
    "        theta2 = np.random.rand(n_samples // 2) * 2 * np.pi\n",
    "        data2 = np.array(\n",
    "            [\n",
    "                r2 * np.cos(theta2) + np.random.randn(n_samples // 2) * 0.2,\n",
    "                r2 * np.sin(theta2) + np.random.randn(n_samples // 2) * 0.2,\n",
    "            ]\n",
    "        ).T\n",
    "        labels2 = np.ones(n_samples // 2)\n",
    "        all_data = np.vstack((data1, data2))\n",
    "        all_labels = np.hstack((labels1, labels2))\n",
    "        perm = np.random.permutation(n_samples)\n",
    "        data1 = all_data[all_labels == -1]\n",
    "        data2 = all_data[all_labels == 1]\n",
    "    X = np.vstack((data1, data2))\n",
    "    y = np.hstack((labels1, labels2))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_data_plotly(X, y, title=\"\", fig=None, row=None, col=None):\n",
    "    \"\"\"Plots 2D classification data using Plotly.\"\"\"\n",
    "    if fig is None:\n",
    "        fig = go.Figure()\n",
    "\n",
    "    # Convert JAX arrays to NumPy for Plotly\n",
    "    X_np = np.asarray(X)\n",
    "    y_np = np.asarray(y)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_np[y_np == -1, 0],\n",
    "            y=X_np[y_np == -1, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=\"maroon\", symbol=\"circle\"),\n",
    "            name=\"Class -1\",\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_np[y_np == 1, 0],\n",
    "            y=X_np[y_np == 1, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                color=\"skyblue\", symbol=\"circle\", line=dict(width=1, color=\"skyblue\")\n",
    "            ),\n",
    "            name=\"Class +1\",\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title_text=title, title_x=0.5)\n",
    "    fig.update_xaxes(title_text=\"$x_1$\", range=[-4, 4], row=row, col=col)\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"$x_2$\",\n",
    "        range=[-4, 4],\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1,\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# --- Simple MLP Implementation in JAX ---\n",
    "def init_mlp_params(key, layer_sizes):\n",
    "    \"\"\"Initializes parameters for a simple MLP.\"\"\"\n",
    "    params = []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        in_dim = layer_sizes[i]\n",
    "        out_dim = layer_sizes[i + 1]\n",
    "        # Glorot initialization for weights\n",
    "        limit = jnp.sqrt(6 / (in_dim + out_dim))\n",
    "        weights = jax.random.uniform(\n",
    "            subkey, (in_dim, out_dim), minval=-limit, maxval=limit\n",
    "        )\n",
    "        biases = jnp.zeros(out_dim)\n",
    "        params.append({\"weights\": weights, \"biases\": biases})\n",
    "    return params\n",
    "\n",
    "\n",
    "def mlp_forward(params, x):\n",
    "    \"\"\"Forward pass through the MLP.\"\"\"\n",
    "    hidden_layers = params[:-1]\n",
    "    output_layer = params[-1]\n",
    "\n",
    "    h = x\n",
    "    for layer in hidden_layers:\n",
    "        h = jnp.dot(h, layer[\"weights\"]) + layer[\"biases\"]\n",
    "        h = relu(h)  # Using ReLU as nonlinearity\n",
    "\n",
    "    # Output layer (no activation for regression, or sigmoid/softmax for classification)\n",
    "    output = jnp.dot(h, output_layer[\"weights\"]) + output_layer[\"biases\"]\n",
    "    return output\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "key = jax.random.PRNGKey(0)\n",
    "input_dim = 2\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "layer_sizes = [input_dim, hidden_dim, output_dim]\n",
    "mlp_params = init_mlp_params(key, layer_sizes)\n",
    "\n",
    "dummy_input = jnp.array([[1.0, 2.0], [0.5, -1.0]])\n",
    "dummy_output = mlp_forward(mlp_params, dummy_input)\n",
    "\n",
    "print(\"\\n--- Simple MLP Example ---\")\n",
    "print(\"MLP Parameters (first layer weights and biases):\\n\", mlp_params[0])\n",
    "print(\"Dummy Input:\\n\", dummy_input)\n",
    "print(\"Dummy Output (before final activation if any):\\n\", dummy_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. How are Deep Architectures Trained? Empirical Risk Minimization (ERM)\n",
    "\n",
    "Deep learning models are typically trained using **Empirical Risk Minimization (ERM)**. The goal is to find parameters $\\theta_*$ that minimize a loss function on a given training dataset $\\mathcal{D} = [(x_i, y_i)]_{i=1,...,N}$ (as per Slide 5):\n",
    "\n",
    "$$\\theta_* = \\arg \\min_{\\theta} \\mathcal{L}(\\theta) = \\arg \\min_{\\theta} \\left( \\frac{1}{N} \\sum_{i=1}^N \\ell(y_i, f(x_i, \\theta)) + r(\\theta) \\right)$$\n",
    "\n",
    "Here:\n",
    "* $\\ell(y_i, f(x_i, \\theta))$ is the **loss function** for a single data point, measuring how well the model's output $f(x_i, \\theta)$ matches the true label $y_i$.\n",
    "* $r(\\theta)$ is a **regularization term**, which penalizes complex models to prevent overfitting.\n",
    "\n",
    "Typical choices for loss functions include:\n",
    "* **Cross-entropy loss (aka. log loss)** for classification:\n",
    "    * Binary: $\\ell_{\\text{logistic}}(y_i, \\hat{y}_i) = -y_i \\log \\hat{y}_i - (1-y_i) \\log(1-\\hat{y}_i)$ (for $y_i \\in \\{0,1\\}$)\n",
    "    * Multi-class: $\\ell_{\\text{CE}}(y_i, \\hat{y}_i) = -\\sum_{c=1}^C \\mathbb{I}_{y_i=c} \\log(\\hat{y}_{ic})$\n",
    "* **Mean Squared Error (MSE)** for regression:\n",
    "    * $\\ell_{\\text{MSE}}(y_i, \\hat{y}_i) = \\frac{1}{2}||y_i - \\hat{y}_i||^2$\n",
    "\n",
    "A typical choice of regularizer $r(\\theta)$ is **weight decay (L2-regularization)** (as per Slide 6):\n",
    "\n",
    "$$r_{L2}(\\theta) = \\frac{\\lambda}{2} \\sum_{j=1}^D \\theta_j^2 = \\frac{\\lambda}{2} ||\\theta||_2^2$$\n",
    "\n",
    "### ERM as Maximum A Posteriori (MAP) Estimation\n",
    "\n",
    "Crucially, the ERM objective can be directly related to a **Maximum A Posteriori (MAP) estimate** (as per Slide 7):\n",
    "\n",
    "$$\\theta_* = \\arg \\min_{\\theta \\in \\mathbb{R}^D} \\mathcal{L}(\\theta) = \\arg \\min_{\\theta \\in \\mathbb{R}^D} -\\log p(\\mathcal{D} | \\theta) - \\log p(\\theta)$$\n",
    "$$= \\arg \\max_{\\theta \\in \\mathbb{R}^D} \\underbrace{\\log p(\\mathcal{D} | \\theta)}_{\\text{likelihood}} + \\underbrace{\\log p(\\theta)}_{\\text{prior}} = \\arg \\max_{\\theta \\in \\mathbb{R}^D} p(\\theta | \\mathcal{D})$$\n",
    "\n",
    "This shows that training a deep neural network via ERM is equivalent to finding the mode of the posterior distribution over the parameters, given the data, under certain probabilistic assumptions on the likelihood and prior.\n",
    "\n",
    "For example:\n",
    "* **MSE loss** corresponds to a Gaussian likelihood $p(y_i | \\hat{y}_i) = \\mathcal{N}(y_i; \\hat{y}_i, I)$.\n",
    "* **Cross-entropy loss** corresponds to a Bernoulli (for binary) or Categorical (for multi-class) likelihood.\n",
    "* **L2-regularization** corresponds to a Gaussian prior on the parameters $p(\\theta) = \\mathcal{N}(\\theta; 0, \\lambda^{-1}I)$.\n",
    "\n",
    "This probabilistic interpretation of deep learning is fundamental to connecting it with the GP framework.\n",
    "\n",
    "Let's define some common loss and regularization functions in JAX and see how gradients can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Loss and Regularization Functions in JAX ---\n",
    "\n",
    "\n",
    "def mse_loss(predictions, targets):\n",
    "    \"\"\"Mean Squared Error loss.\"\"\"\n",
    "    return jnp.mean(jnp.square(predictions - targets))\n",
    "\n",
    "\n",
    "def l2_regularization(params, lambda_reg):\n",
    "    \"\"\"L2 regularization (weight decay).\"\"\"\n",
    "    l2_norm = 0.0\n",
    "    for layer in params:\n",
    "        l2_norm += jnp.sum(jnp.square(layer[\"weights\"]))\n",
    "        # Biases are typically not regularized, but can be added if desired\n",
    "    return 0.5 * lambda_reg * l2_norm\n",
    "\n",
    "\n",
    "def total_loss(params, x_batch, y_batch, lambda_reg):\n",
    "    \"\"\"Combines forward pass, MSE loss, and L2 regularization.\"\"\"\n",
    "    predictions = mlp_forward(params, x_batch)\n",
    "    data_loss = mse_loss(predictions, y_batch)\n",
    "    reg_loss = l2_regularization(params, lambda_reg)\n",
    "    return data_loss + reg_loss\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "dummy_x_batch = jnp.array([[1.0, 2.0], [0.5, -1.0]])\n",
    "dummy_y_batch = jnp.array([[3.0], [0.0]])\n",
    "lambda_reg = 0.01\n",
    "\n",
    "# Compute the total loss\n",
    "current_loss = total_loss(mlp_params, dummy_x_batch, dummy_y_batch, lambda_reg)\n",
    "print(\"\\n--- Loss and Regularization Example ---\")\n",
    "print(f\"Current Total Loss: {current_loss:.4f}\")\n",
    "\n",
    "# Compute gradients of the total loss with respect to parameters\n",
    "grad_loss = jax.grad(total_loss)(mlp_params, dummy_x_batch, dummy_y_batch, lambda_reg)\n",
    "print(\"Gradients of parameters (first layer weights and biases):\\n\", grad_loss[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Context: Parametric Regression\n",
    "\n",
    "Let's connect deep learning to our previous understanding of **parametric regression** (as per Slide 9). For Gaussian / parametric / least-squares regression, we posited functions of the linear form:\n",
    "\n",
    "$$f(x, \\theta): \\mathbb{X} \\times \\mathbb{R}^D \\to \\mathbb{R}^F, \\quad f(x, \\theta) = \\phi(x)^T \\theta$$\n",
    "\n",
    "Here, $\\phi: \\mathbb{X} \\to \\mathbb{R}^D$ can be a feature map of almost any form (including discontinuities, point-masses, etc., provided numerical stability). $\\mathbb{X}$ can also be a diverse set, not just $\\mathbb{R}^M$. Examples include:\n",
    "* Strings (Natural Language Processing)\n",
    "* Graphs (molecules, genes, proteins)\n",
    "* Functions (operators, simulations)\n",
    "* Gödel numbers, etc.\n",
    "\n",
    "This flexibility arises because $\\phi(x)$ \"masks\" the input $x$, transforming it into a feature representation that the linear model then operates on.\n",
    "\n",
    "The diagram on Slide 9 illustrates this: input $X$ is transformed into features $\\phi_X$, which are then weighted by parameters $W$ (or $\\theta$) to produce the output $y$.\n",
    "\n",
    "### Probabilistic Inference in Parametric Regression\n",
    "\n",
    "For parametric regression with Gaussian generative models (as per Slide 10):\n",
    "\n",
    "Prior: $p(\\theta) = \\mathcal{N}(\\theta; \\mu, \\Sigma)$\n",
    "Likelihood: $p(y | f_X) = \\mathcal{N}(y; f_X, \\sigma^2 I)$, where $f_X = \\phi_X^T \\theta$\n",
    "\n",
    "The posterior $p(\\theta | y)$ is then also Gaussian:\n",
    "\n",
    "$$p(\\theta | y) = \\frac{p(y | \\theta) p(\\theta)}{p(y)} = \\mathcal{N}(\\theta; \\mu_{\\text{post}}, \\Sigma_{\\text{post}})$$\n",
    "\n",
    "with:\n",
    "$$\\mu_{\\text{post}} = (\\Sigma^{-1} + \\sigma^{-2} \\phi_X \\phi_X^T)^{-1}(\\Sigma^{-1}\\mu + \\sigma^{-2}\\phi_X y)$$\n",
    "$$\\Sigma_{\\text{post}} = (\\Sigma^{-1} + \\sigma^{-2} \\phi_X \\phi_X^T)^{-1}$$\n",
    "\n",
    "Observation: $\\mu_{\\text{post}}$ is the mode of $\\log p(\\theta | y)$, and $-\\Sigma_{\\text{post}}^{-1}$ is the Hessian of $\\log p(\\theta | y)$. This directly links back to our MAP estimation and Laplace Approximation concepts.\n",
    "\n",
    "As shown on Slide 11, this Gaussian inference can be described as L2-regularized empirical risk minimization:\n",
    "\n",
    "$$\\mu_{\\text{post}} = \\arg \\min_{\\theta \\in \\mathbb{R}^D} -\\log p(\\theta | y) = \\arg \\min_{\\theta \\in \\mathbb{R}^D} -\\log p(y | \\theta) - \\log p(\\theta)$$\n",
    "$$= \\arg \\min_{\\theta \\in \\mathbb{R}^D} \\frac{1}{2\\sigma^2} ||y - \\phi_X^T \\theta||^2 + \\frac{1}{2}(\\theta - \\mu)^T \\Sigma^{-1} (\\theta - \\mu)$$\n",
    "If we assume $\\mu=0$ and $\\Sigma = \\lambda^{-1}I$, this becomes:\n",
    "$$= \\arg \\min_{\\theta \\in \\mathbb{R}^D} \\frac{1}{2} \\sum_{i=1}^N (y_i - \\phi_{x_i}^T \\theta)^2 + \\frac{\\sigma^2 \\lambda}{2} ||\\theta||^2$$\n",
    "\n",
    "Let's illustrate a simple parametric regression model using a polynomial feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Parametric Regression Example ---\n",
    "\n",
    "\n",
    "def polynomial_features(x, degree):\n",
    "    \"\"\"Generates polynomial features for input x.\"\"\"\n",
    "    return jnp.power(x[:, None], jnp.arange(degree + 1))\n",
    "\n",
    "\n",
    "def linear_model_predict(features, weights):\n",
    "    \"\"\"Predicts output using a linear model: f(x) = phi(x)^T @ weights.\"\"\"\n",
    "    return features @ weights\n",
    "\n",
    "\n",
    "# Generate some dummy 1D data\n",
    "np.random.seed(1)\n",
    "X_1d_np = np.linspace(-2, 2, 50)\n",
    "true_weights = jnp.array([0.5, -1.0, 0.3])  # For a quadratic function\n",
    "true_features = polynomial_features(jnp.array(X_1d_np), degree=2)\n",
    "y_1d_np = np.asarray(\n",
    "    linear_model_predict(true_features, true_weights) + 0.2 * np.random.randn(50)\n",
    ")\n",
    "\n",
    "X_1d_jax = jnp.array(X_1d_np)\n",
    "y_1d_jax = jnp.array(y_1d_np)\n",
    "\n",
    "# Initialize random weights for the linear model\n",
    "key, subkey = jax.random.split(jax.random.PRNGKey(1))\n",
    "initial_weights = jax.random.normal(subkey, (3,))\n",
    "\n",
    "# Compute predictions with initial weights\n",
    "features_at_x = polynomial_features(X_1d_jax, degree=2)\n",
    "initial_predictions = linear_model_predict(features_at_x, initial_weights)\n",
    "\n",
    "print(\"\\n--- Parametric Regression Example ---\")\n",
    "print(\"Initial Weights:\\n\", initial_weights)\n",
    "print(\"First 5 True Y values:\\n\", y_1d_jax[:5])\n",
    "print(\"First 5 Initial Predictions:\\n\", initial_predictions[:5])\n",
    "\n",
    "# Plotting the initial fit\n",
    "fig_param_reg = go.Figure()\n",
    "fig_param_reg.add_trace(\n",
    "    go.Scatter(x=X_1d_np, y=y_1d_np, mode=\"markers\", name=\"True Data\")\n",
    ")\n",
    "fig_param_reg.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_1d_np,\n",
    "        y=np.asarray(initial_predictions),\n",
    "        mode=\"lines\",\n",
    "        name=\"Initial Model Fit\",\n",
    "    )\n",
    ")\n",
    "fig_param_reg.update_layout(\n",
    "    title_text=\"Parametric Regression with Polynomial Features (Initial Fit)\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"X\",\n",
    "    yaxis_title=\"Y\",\n",
    "    height=500,\n",
    "    width=700,\n",
    ")\n",
    "fig_param_reg.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Context: Nonparametric Regression (GPs)\n",
    "\n",
    "In contrast to parametric models, **nonparametric regression** with Gaussian Processes (GPs) operates without explicit weights in the traditional sense (as per Slide 12):\n",
    "\n",
    "Prior: $p(f) = \\mathcal{GP}(f; m, k)$\n",
    "Likelihood: $p(y | f_X) = \\mathcal{N}(y; f_X, \\sigma^2 I)$\n",
    "\n",
    "The posterior $p(f | y)$ is also a GP, with updated mean and covariance functions:\n",
    "\n",
    "$$m_{\\text{post}}(\\bullet) = m_{\\bullet} + K_{\\bullet X}(K_{XX} + \\sigma^2 I)^{-1}(f_X - m_X)$$\n",
    "$$k_{\\text{post}}(\\bullet, \\circ) = k_{\\bullet \\circ} - K_{\\bullet X}(K_{XX} + \\sigma^2 I)^{-1}K_{X \\circ}$$\n",
    "\n",
    "Although a GP model is nonparametric (\"there are no weights\"), there are interesting connections:\n",
    "\n",
    "* $m_{\\text{post}}(\\bullet)$ is the minimizer of the **ridge loss in the Reproducing Kernel Hilbert Space (RKHS)** $\\mathcal{H}_k$:\n",
    "    $$m_{\\text{post}}(\\bullet) = \\arg \\min_{f \\in \\mathcal{H}_k} \\left( \\frac{1}{2} \\sum_{i=1}^N (y_i - f(x_i))^2 + \\frac{\\sigma^2}{2} ||f||_{\\mathcal{H}_k}^2 \\right)$$\n",
    "* $k_{\\text{post}}(\\bullet, \\bullet)$ provides a worst-case error estimate for RKHS functions of bounded norm.\n",
    "\n",
    "Every RKHS function can be expanded in the (countably many) eigenfunctions $\\phi_i$ of the kernel:\n",
    "$$\\mathcal{H}_k = \\left\\{ f(x) := \\sum_{i \\in I} \\alpha_i \\lambda_i^{1/2} \\phi_i(x) \\text{ such that } ||f||_{\\mathcal{H}_k}^2 := \\sum_{i \\in I} \\alpha_i^2 < \\infty \\right\\}$$\n",
    "with inner product $\\langle f, g \\rangle_{\\mathcal{H}_k} := \\sum_{i \\in I} \\alpha_i \\beta_i$.\n",
    "\n",
    "This perspective highlights that even nonparametric models implicitly operate within a structured function space defined by the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Context: Logistic Regression / GP Classification\n",
    "\n",
    "To model classification problems, we change the likelihood function (as per Slide 13):\n",
    "\n",
    "$$p(y | f(x)) = \\sigma(yf(x)) \\quad \\text{with } \\sigma(a) = \\frac{1}{1 + \\exp(-a)}$$\n",
    "\n",
    "The negative log-likelihood (which becomes the loss function) is then:\n",
    "\n",
    "$$-\\log p(y | f(x)) = \\log(1 + \\exp(-yf(x)))$$\n",
    "\n",
    "For multi-class classification, the likelihood uses the softmax function:\n",
    "\n",
    "$$p(y | f(x)) = \\text{softmax}(f(x))_y \\quad \\text{with } \\text{softmax}(a)_i = \\frac{\\exp(a_i)}{\\sum_{j=1}^C \\exp(a_j)}$$\n",
    "\n",
    "The corresponding negative log-likelihood (cross-entropy loss) is:\n",
    "\n",
    "$$-\\log p(y | f(x)) = -f(x)_y + \\log \\sum_{j=1}^C \\exp(f(x)_j)$$\n",
    "\n",
    "Implementing multi-class GP classification requires **multi-output GPs**, which involve a joint covariance function between different outputs (as discussed on Slide 14):\n",
    "\n",
    "$$k(f_c(a), f_d(b)) = k((a, c), (b, d))$$\n",
    "\n",
    "While possible, this often leads to more complex covariance structures. Simple cases factorize covariance between inputs and outputs, leading to Kronecker structure, or assume independent outputs. For this course, we've focused on binary classification to keep the code structure manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Are Infinitely Many Weights Enough? (Universality of Kernels)\n",
    "\n",
    "A fascinating question arises: If GPs are nonparametric and can be seen as \"infinitely wide single-layer neural networks,\" does this mean they can learn anything? (as per Slide 15)\n",
    "\n",
    "For some kernels, known as **universal kernels**, the RKHS \"lies dense\" in the space of all continuous functions. A prime example is the **Square-Exponential / Gaussian / RBF kernel**:\n",
    "\n",
    "$$k(a, b) = \\exp\\left(-\\frac{1}{2}(a - b)^2\\right)$$\n",
    "\n",
    "When using such kernels for GP or kernel-ridge regression, for any continuous function $f$ and any $\\epsilon > 0$, there exists an RKHS element $\\hat{f} \\in \\mathcal{H}_k$ such that $||f - \\hat{f}|| < \\epsilon$ (where $||\\cdot||$ is the maximum norm on a compact subset of $X$).\n",
    "\n",
    "**This implies that, given enough data, the GP posterior mean can approximate any function arbitrarily well!** GPs are indeed \"infinitely flexible\" and can learn infinite-dimensional functions arbitrarily well, provided the true function is from a sufficiently smooth space and the RKHS covers that space well (Theorem by v.d. Vaart & v. Zanten, 2011, Slide 27).\n",
    "\n",
    "### The Bad News: If f is not in the RKHS\n",
    "\n",
    "However, this theoretical flexibility comes with a caveat: if the true function $f$ is **not well-covered by the RKHS**, the convergence rates can be severely impacted (as illustrated on Slides 16-24). The number of data points required to achieve a certain error $\\epsilon$ can become **exponential in $\\epsilon$**. Outside the observation range, there are no guarantees at all.\n",
    "\n",
    "This is analogous to representing an irrational number like $\\pi$ using rational numbers (Slide 26). While rational numbers are dense in real numbers, some sequences (like decimal expansion) converge quickly, while others (like Gregory-Leibniz series) converge very slowly, requiring many \"datapoints\" (terms) to achieve high precision.\n",
    "\n",
    "The takeaway is that while GPs are theoretically powerful, practical convergence can depend heavily on the alignment between the true function's properties and the chosen kernel's RKHS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Deep or Wide? An Assessment\n",
    "\n",
    "This brings us to a crucial question: why is deep learning so popular if nonparametric models like GPs offer such theoretical flexibility? (as per Slide 28)\n",
    "\n",
    "While nonparametric models provide a strong theoretical foundation for supervised learning, \"having infinite flexibility\" does not automatically translate to fast learning in practice. There are applications where carefully designing the RKHS or sample space matters (e.g., in simulation methods).\n",
    "\n",
    "The real reasons for deep learning's popularity are often practical, rather than purely theoretical. It's important to separate fact from misconception.\n",
    "\n",
    "### Deep Learning: An Assessment (Pros and Cons)\n",
    "\n",
    "**What people like about deep learning (as per Slide 29):**\n",
    "* **Efficient Training**: Training with optimizers like SGD, Adam, etc., is often considered \"O(1)\" per step because stochastic gradients can be computed on mini-batches, making it scalable to large datasets.\n",
    "    $$\\nabla \\mathcal{L}(w) = \\frac{1}{N} \\sum_{i=1}^N \\nabla \\ell(y_i, f(w, x_i)) + \\nabla r(w) \\approx \\frac{1}{B} \\sum_{j=1}^B \\nabla \\ell(y_{i(j)}, f(w, x_{i(j)})) + \\nabla r(w)$$\n",
    "* **Parallelization**: The parametric loss has an array structure, allowing for efficient sharding and parallelization across multiple processors or GPUs.\n",
    "* **Intuitive Metaphors**: Concepts like neural networks, skip connections, attention, pooling, and compression resonate well and provide useful abstractions.\n",
    "* **Model Deployment**: Once trained, the model (parameters) can be deployed independently of the training data, often hidden behind an API, which is convenient for commercial applications.\n",
    "\n",
    "**What people don't like about deep learning (as per Slide 30):**\n",
    "* **Fiddly Training**: Training a deep net is often an art, requiring many choices and hyperparameter tuning:\n",
    "    * Initialization strategy for weights.\n",
    "    * Learning rate and learning-rate schedules.\n",
    "    * Other optimizer parameters.\n",
    "    * Regularization techniques (dropout, batch normalization, weight decay).\n",
    "    * Deciding when to stop training and monitoring optimizer stability.\n",
    "    * Overall architecture design.\n",
    "* **Model Updates**: It's often unclear how to efficiently update a trained deep learning model when new data arrives (requiring retraining or complex fine-tuning).\n",
    "* **Conceptual Pathologies**: Deep learning models can exhibit certain brittleness, leading to issues like adversarial examples and poor generalization to out-of-distribution data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPs and Kernels: An Assessment (Pros and Cons)\n",
    "\n",
    "**What people like about GPs (as per Slide 31):**\n",
    "* **Automatic Training**: Training primarily involves linear algebra, with no explicit parameters to tune in the same way as deep networks.\n",
    "* **Full Probabilistic Model**: Provides a complete probabilistic framework, allowing for interpretability (e.g., drawing samples from the prior) and crucial uncertainty quantification.\n",
    "* **Easy Updates**: Updating the model with new data is straightforward using techniques like the Schur complement.\n",
    "* **Elegant Mathematical Theory**: Supported by a rich and elegant mathematical theory.\n",
    "\n",
    "**What people don't like about GPs (as per Slide 31):**\n",
    "* **Computational Cost**: Training is typically $O(N^3)$ with respect to the number of data points $N$, making it computationally expensive for large datasets.\n",
    "* **\"The data is the model\"**: Releasing a GP model often means releasing the entire training data, which can be a privacy or intellectual property concern.\n",
    "* **Limited Parallelization**: Because all data points interact directly (not via a compact weight-space), sharding and parallelization are not as straightforward as in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Summary\n",
    "\n",
    "To summarize (as per Slide 32):\n",
    "\n",
    "* **Deep Learning and GP regression/classification are closely related** from a probabilistic perspective, often representing different approaches to function approximation and inference.\n",
    "* The **central difference** lies in their structure: Deep models are hierarchical and inherently nonlinear, while basic GP models (with standard kernels) are often seen as shallow or linear in a high-dimensional feature space.\n",
    "* The **nonlinear nature of deep models** can be advantageous for performance but necessitates complex nonlinear optimization for training.\n",
    "* Both **shallow (GP) and deep models have distinct advantages and disadvantages**, making each suitable for different problem settings and priorities.\n",
    "\n",
    "This lecture serves as a bridge, highlighting the connections and contrasts between these two powerful paradigms in machine learning. The next lecture will delve into combining GPs and deep learning, exploring the best of both worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "Since this lecture is more conceptual, the exercises will focus on understanding the implications of the discussed concepts.\n",
    "\n",
    "**Exercise 1: ERM and MAP Equivalence**\n",
    "Consider a simple linear regression problem with a Gaussian likelihood and a Gaussian prior on the weights. Write down the explicit form of the ERM objective function and show how it maps directly to the negative log-posterior, thus demonstrating the MAP equivalence.\n",
    "\n",
    "**Exercise 2: Deep vs. Shallow Model Strengths**\n",
    "Based on the pros and cons discussed, identify a real-world problem where a GP model would likely be preferred over a deep learning model, and explain why. Then, identify a problem where a deep learning model would be more suitable, and explain your reasoning.\n",
    "\n",
    "**Exercise 3: The \"Data is the Model\" Implication**\n",
    "Discuss the practical and ethical implications of the statement \"The data is the model\" for GPs, especially in scenarios involving sensitive data or intellectual property. How do deep learning models mitigate (or exacerbate) these issues?\n",
    "\n",
    "**Exercise 4: Challenges in Deep Learning Training**\n",
    "Choose one of the \"fiddly\" aspects of deep learning training (e.g., learning rate schedules, regularization). Briefly research common strategies used to address this challenge and explain why it's a non-trivial problem."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
