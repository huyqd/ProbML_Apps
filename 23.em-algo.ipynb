{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c6852b",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning: Lecture 23 - EM Algorithm\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Welcome to Lecture 23 of Probabilistic Machine Learning! This lecture introduces the **Expectation-Maximization (EM) algorithm**, a powerful iterative method for finding maximum likelihood (ML) or maximum a posteriori (MAP) estimates of parameters in probabilistic models, especially when the model involves **latent variables**. We will delve into why EM works by understanding its connection to maximizing a lower bound on the model evidence.\n",
    "\n",
    "This notebook will provide detailed explanations and practical code illustrations using **JAX** for numerical computations and **Plotly** for interactive visualizations, focusing on the classic application of EM to **Gaussian Mixture Models (GMMs)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378d089",
   "metadata": {},
   "source": [
    "#### 1. The EM Algorithm: Maximizing Model Evidence\n",
    "\n",
    "As we discussed in Lecture 22 (and recap from Slide 2), the general recipe for hyperparameter inference involves maximizing the marginal (log-) likelihood, also known as the **model evidence**:\n",
    "\n",
    "$$\\log p(y | \\theta) = \\log \\left( \\int p(y, z | \\theta) dz \\right)$$\n",
    "\n",
    "where $y$ are the observed data, $\\theta$ are the model parameters, and $z$ are the latent variables. This integral is often intractable. While Laplace approximations offer one way to approximate this, in some cases, the **EM algorithm** provides an iterative solution.\n",
    "\n",
    "The core idea of EM (Slide 3) is to iteratively improve the parameter estimates by performing two steps:\n",
    "\n",
    "* **E-step (Expectation)**: Compute the expected complete-data log-likelihood, $q(\\theta, \\theta_t)$, given the current parameter estimates $\\theta_t$:\n",
    "    $$q(\\theta, \\theta_t) = \\int p(z | y, \\theta_t) \\log p(y, z | \\theta) dz$$\n",
    "\n",
    "* **M-step (Maximization)**: Set the new parameter estimates $\\theta_{t+1}$ to maximize this expected complete-data log-likelihood:\n",
    "    $$\\theta_{t+1} = \\arg \\max_{\\theta} q(\\theta, \\theta_{t+1})$$\n",
    "\n",
    "The algorithm iterates these two steps until convergence of either the log-likelihood or the parameters $\\theta$. Let's start by setting up our imports and a utility function for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d343e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from jax.scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "# Set JAX to use 64-bit floats for numerical stability\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "def plot_gmm_plotly(X, means, covariances, responsibilities=None, title=\"Gaussian Mixture Model\", colors=['red', 'blue', 'green', 'purple', 'orange']):\n",
    "    \"\"\"Plots 2D data, GMM components, and optionally responsibilities.\"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot data points, optionally colored by responsibility\n",
    "    if responsibilities is not None:\n",
    "        # For responsibilities, we can use a scatter plot with customdata for hover text\n",
    "        # and color based on the component with highest responsibility\n",
    "        dominant_component = jnp.argmax(responsibilities, axis=1)\n",
    "        for k in range(means.shape[0]):\n",
    "            mask = dominant_component == k\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X[mask, 0],\n",
    "                y=X[mask, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(color=colors[k % len(colors)], size=5, opacity=0.7),\n",
    "                name=f'Data (Component {k+1})',\n",
    "                showlegend=True\n",
    "            ))\n",
    "    else:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X[:, 0],\n",
    "            y=X[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(color='gray', size=5, opacity=0.7),\n",
    "            name='Data Points',\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "    # Plot Gaussian components (mean and covariance ellipses)\n",
    "    for k in range(means.shape[0]):\n",
    "        mean = means[k]\n",
    "        cov = covariances[k]\n",
    "        \n",
    "        # Draw ellipse representing 2-sigma contour\n",
    "        vals, vecs = jnp.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        vals = vals[order]\n",
    "        vecs = vecs[:, order]\n",
    "        \n",
    "        theta = jnp.degrees(jnp.arctan2(*vecs[:, 0][::-1]))\n",
    "        width, height = 2 * jnp.sqrt(5.991 * vals) # 5.991 for 95% confidence for 2D Chi-squared with 2 DOF\n",
    "\n",
    "        fig.add_shape(\n",
    "            type='circle',\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            x0=mean[0] - width / 2,\n",
    "            y0=mean[1] - height / 2,\n",
    "            x1=mean[0] + width / 2,\n",
    "            y1=mean[1] + height / 2,\n",
    "            line=dict(color=colors[k % len(colors)], width=2),\n",
    "            opacity=0.8,\n",
    "            layer='below',\n",
    "            name=f'Component {k+1}'\n",
    "        )\n",
    "        # Add mean point\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[mean[0]],\n",
    "            y=[mean[1]],\n",
    "            mode='markers',\n",
    "            marker=dict(symbol='x', size=10, color=colors[k % len(colors)], line=dict(width=2, color='black')),\n",
    "            name=f'Mean {k+1}',\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(title_text=title, title_x=0.5,\n",
    "                      xaxis_title='Feature 1',\n",
    "                      yaxis_title='Feature 2',\n",
    "                      autosize=False, width=700, height=600)\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1) # Keep aspect ratio square\n",
    "    fig.show()\n",
    "\n",
    "def generate_gmm_data(num_samples=300, num_components=3, random_seed=42):\n",
    "    \"\"\"Generates synthetic data from a Gaussian Mixture Model.\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # True parameters for 3 components\n",
    "    true_weights = np.array([0.3, 0.4, 0.3])\n",
    "    true_means = np.array([\n",
    "        [0, 0],\n",
    "        [3, 3],\n",
    "        [0, 4]\n",
    "    ])\n",
    "    true_covariances = np.array([\n",
    "        [[0.5, 0.2], [0.2, 0.5]],\n",
    "        [[0.8, -0.1], [-0.1, 0.8]],\n",
    "        [[0.6, 0.3], [0.3, 0.6]]\n",
    "    ])\n",
    "\n",
    "    X = []\n",
    "    for _ in range(num_samples):\n",
    "        # Choose a component based on weights\n",
    "        k = np.random.choice(num_components, p=true_weights)\n",
    "        # Sample from the chosen Gaussian\n",
    "        sample = np.random.multivariate_normal(true_means[k], true_covariances[k])\n",
    "        X.append(sample)\n",
    "    \n",
    "    return jnp.array(X), true_weights, true_means, true_covariances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65f15c",
   "metadata": {},
   "source": [
    "#### 2. Why EM Works: Maximizing a Lower Bound\n",
    "\n",
    "The EM algorithm works by iteratively maximizing a lower bound on the model evidence, known as the **Evidence Lower Bound (ELBO)** (Slides 4-5).\n",
    "\n",
    "For any arbitrary distribution $q(z)$ over the latent variables $z$, the log evidence can be decomposed as:\n",
    "\n",
    "$$\\log p(x | \\theta) = \\mathcal{L}(q, \\theta) + D_{KL}(q || p(z | x, \\theta))$$\n",
    "\n",
    "where:\n",
    "* $\\mathcal{L}(q, \\theta) = \\int q(z) \\log \\left( \\frac{p(x, z | \\theta)}{q(z)} \\right) dz$ is the **ELBO**.\n",
    "* $D_{KL}(q || p(z | x, \\theta)) = -\\int q(z) \\log \\left( \\frac{p(z | x, \\theta)}{q(z)} \\right) dz$ is the **Kullback-Leibler (KL) divergence** between $q(z)$ and the true posterior $p(z | x, \\theta)$.\n",
    "\n",
    "Since KL divergence is always non-negative ($D_{KL}(q || p) \\ge 0$), the ELBO $\\mathcal{L}(q, \\theta)$ is a lower bound on the log evidence: $\\log p(x | \\theta) \\ge \\mathcal{L}(q, \\theta)$.\n",
    "\n",
    "The EM algorithm's steps can be understood in terms of this decomposition (Slides 7-9):\n",
    "\n",
    "* **E-step**: With $\\theta_{old}$ fixed, we set $q(z)$ to be the true posterior $p(z | x, \\theta_{old})$. This makes the KL divergence term $D_{KL}(q || p(z | x, \\theta_{old})) = 0$, thus making the ELBO equal to the log evidence at $\\theta_{old}$: $\\mathcal{L}(q, \\theta_{old}) = \\log p(x | \\theta_{old})$. This step tightens the lower bound at the current parameter values.\n",
    "\n",
    "* **M-step**: With $q(z)$ fixed (from the E-step), we maximize the ELBO $\\mathcal{L}(q, \\theta)$ with respect to $\\theta$ to obtain $\\theta_{new}$. Since $q(z)$ is fixed, maximizing $\\mathcal{L}(q, \\theta)$ is equivalent to maximizing $\\int q(z) \\log p(x, z | \\theta) dz$, which is the expected complete-data log-likelihood. This step increases the ELBO, and since the KL divergence is non-negative, it also guarantees that the log evidence $\\log p(x | \\theta)$ either increases or stays the same.\n",
    "\n",
    "This iterative process ensures that the log evidence is non-decreasing with each step, eventually converging to a local maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d471ee87",
   "metadata": {},
   "source": [
    "#### 3. Example: EM for Gaussian Mixture Models (GMMs)\n",
    "\n",
    "A classic application of the EM algorithm is to learn the parameters of a **Gaussian Mixture Model (GMM)** (Slide 13). A GMM assumes that the data $x$ is generated from a mixture of $K$ Gaussian distributions:\n",
    "\n",
    "$$p(x | \\pi, \\mu, \\Sigma) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "where $\\pi_k$ are the mixing coefficients (weights), $\\mu_k$ are the means, and $\\Sigma_k$ are the covariances for each component $k$. The sum over components makes direct maximization of the likelihood difficult due to the logarithm of a sum.\n",
    "\n",
    "We introduce a latent variable $z_n$ for each data point $x_n$, where $z_{nk}=1$ if $x_n$ belongs to component $k$, and $z_{nk}=0$ otherwise. The \"complete data\" likelihood then factorizes (Slide 15):\n",
    "\n",
    "$$\\log p(x, z | \\pi, \\mu, \\Sigma) = \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} (\\log \\pi_k + \\log \\mathcal{N}(x_n | \\mu_k, \\Sigma_k))$$\n",
    "\n",
    "This form is much easier to optimize because the logarithm is inside the sum over components.\n",
    "\n",
    "### EM Algorithm for GMMs\n",
    "\n",
    "Let $\\theta = (\\pi, \\mu, \\Sigma)$ denote all the GMM parameters.\n",
    "\n",
    "**E-step (Compute Responsibilities)** (Slide 17):\n",
    "The E-step requires computing the posterior probability of the latent variable $z_n$ given the observed data $x_n$ and current parameters $\\theta_t$. This is the \"responsibility\" of component $k$ for data point $x_n$, denoted $r_{nk}$:\n",
    "\n",
    "$$r_{nk} = p(z_{nk}=1 | x_n, \\theta_t) = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "**M-step (Update Parameters)** (Slides 18-20):\n",
    "The M-step maximizes the expected complete-data log-likelihood with respect to $\\pi, \\mu, \\Sigma$. The expectations $\\mathbb{E}_{q(z)}[z_{nk}]$ are simply $r_{nk}$.\n",
    "\n",
    "* **Update Means ($\\mu_k$)**:\n",
    "    $$\\mu_k^{new} = \\frac{\\sum_{n=1}^N r_{nk} x_n}{\\sum_{n=1}^N r_{nk}}$$\n",
    "\n",
    "* **Update Covariances ($\\Sigma_k$)**:\n",
    "    $$\\Sigma_k^{new} = \\frac{\\sum_{n=1}^N r_{nk} (x_n - \\mu_k^{new})(x_n - \\mu_k^{new})^T}{\\sum_{n=1}^N r_{nk}}$$\n",
    "\n",
    "* **Update Weights ($\\pi_k$)**:\n",
    "    $$\\pi_k^{new} = \\frac{\\sum_{n=1}^N r_{nk}}{N}$$\n",
    "\n",
    "Let's implement the EM algorithm for GMMs and visualize its iterative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff966784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- EM Algorithm for Gaussian Mixture Models ---\n",
    "\n",
    "def initialize_gmm_params(X, num_components, key):\n",
    "    \"\"\"Randomly initializes GMM parameters (means, covariances, weights).\"\"\"\n",
    "    num_samples, data_dim = X.shape\n",
    "\n",
    "    key, subkey_means, subkey_covs, subkey_weights = jax.random.split(key, 4)\n",
    "\n",
    "    # Initialize means by randomly picking data points\n",
    "    random_indices = jax.random.choice(subkey_means, num_samples, (num_components,), replace=False)\n",
    "    means = X[random_indices]\n",
    "\n",
    "    # Initialize covariances to identity matrices (or small diagonal)\n",
    "    covariances = jnp.array([jnp.eye(data_dim) * 0.1 for _ in range(num_components)])\n",
    "\n",
    "    # Initialize weights uniformly\n",
    "    weights = jnp.ones(num_components) / num_components\n",
    "\n",
    "    return means, covariances, weights\n",
    "\n",
    "@jax.jit\n",
    "def e_step(X, means, covariances, weights):\n",
    "    \"\"\"Computes responsibilities (rnk) for the GMM.\"\"\"\n",
    "    num_samples, _ = X.shape\n",
    "    num_components = means.shape[0]\n",
    "\n",
    "    # Compute log-likelihood for each data point under each component\n",
    "    log_likelihoods = jnp.zeros((num_samples, num_components))\n",
    "    for k in range(num_components):\n",
    "        # Add a small regularization to covariance to prevent singularity\n",
    "        reg_cov = covariances[k] + jnp.eye(covariances[k].shape[0]) * 1e-6\n",
    "        log_likelihoods = log_likelihoods.at[:, k].set(mvn.logpdf(X, means[k], reg_cov))\n",
    "\n",
    "    # Compute log(p(x_n, z_n=k)) = log(pi_k) + log(N(x_n | mu_k, Sigma_k))\n",
    "    log_joint = log_likelihoods + jnp.log(weights)\n",
    "\n",
    "    # Compute log(sum_k p(x_n, z_n=k)) for normalization\n",
    "    log_sum_exp = jax.nn.logsumexp(log_joint, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute responsibilities (rnk) = p(z_n=k | x_n) = exp(log_joint - log_sum_exp)\n",
    "    responsibilities = jnp.exp(log_joint - log_sum_exp)\n",
    "\n",
    "    return responsibilities\n",
    "\n",
    "@jax.jit\n",
    "def m_step(X, responsibilities):\n",
    "    \"\"\"Updates GMM parameters (means, covariances, weights).\"\"\"\n",
    "    num_samples, data_dim = X.shape\n",
    "    num_components = responsibilities.shape[1]\n",
    "\n",
    "    # Update weights\n",
    "    Nk = jnp.sum(responsibilities, axis=0) # Sum of responsibilities for each component\n",
    "    new_weights = Nk / num_samples\n",
    "\n",
    "    # Update means\n",
    "    new_means = jnp.dot(responsibilities.T, X) / Nk[:, jnp.newaxis]\n",
    "\n",
    "    # Update covariances\n",
    "    new_covariances = jnp.zeros((num_components, data_dim, data_dim))\n",
    "    for k in range(num_components):\n",
    "        diff = X - new_means[k]\n",
    "        # Weighted outer product sum\n",
    "        weighted_outer_product = jnp.dot((responsibilities[:, k] * diff.T), diff)\n",
    "        new_covariances = new_covariances.at[k].set(weighted_outer_product / Nk[k])\n",
    "        # Add a small diagonal term for numerical stability, especially if a component gets very few points\n",
    "        new_covariances = new_covariances.at[k].set(new_covariances[k] + jnp.eye(data_dim) * 1e-6)\n",
    "\n",
    "    return new_means, new_covariances, new_weights\n",
    "\n",
    "def compute_log_likelihood(X, means, covariances, weights):\n",
    "    \"\"\"Computes the total log-likelihood of the data under the GMM.\"\"\"\n",
    "    num_samples, _ = X.shape\n",
    "    num_components = means.shape[0]\n",
    "\n",
    "    log_likelihoods_per_component = jnp.zeros((num_samples, num_components))\n",
    "    for k in range(num_components):\n",
    "        reg_cov = covariances[k] + jnp.eye(covariances[k].shape[0]) * 1e-6\n",
    "        log_likelihoods_per_component = log_likelihoods_per_component.at[:, k].set(mvn.logpdf(X, means[k], reg_cov))\n",
    "\n",
    "    # log(sum_k pi_k * N(x_n | mu_k, Sigma_k))\n",
    "    log_weighted_likelihoods = log_likelihoods_per_component + jnp.log(weights)\n",
    "    total_log_likelihood = jnp.sum(jax.nn.logsumexp(log_weighted_likelihoods, axis=1))\n",
    "    return total_log_likelihood\n",
    "\n",
    "def run_gmm_em(X, num_components, max_iter=100, tol=1e-4, key=None):\n",
    "    \"\"\"\n",
    "    Runs the EM algorithm for Gaussian Mixture Models.\n",
    "    Returns final parameters, log-likelihood history, and responsibilities.\n",
    "    \"\"\"\n",
    "    if key is None:\n",
    "        key = jax.random.PRNGKey(0)\n",
    "\n",
    "    means, covariances, weights = initialize_gmm_params(X, num_components, key)\n",
    "    log_likelihood_history = []\n",
    "\n",
    "    print(\"Starting EM for GMM...\")\n",
    "    for i in range(max_iter):\n",
    "        # E-step\n",
    "        responsibilities = e_step(X, means, covariances, weights)\n",
    "\n",
    "        # M-step\n",
    "        new_means, new_covariances, new_weights = m_step(X, responsibilities)\n",
    "\n",
    "        # Compute log-likelihood for convergence check\n",
    "        current_log_likelihood = compute_log_likelihood(X, new_means, new_covariances, new_weights)\n",
    "        log_likelihood_history.append(current_log_likelihood)\n",
    "\n",
    "        # Check for convergence\n",
    "        if i > 0 and jnp.abs(current_log_likelihood - log_likelihood_history[-2]) < tol:\n",
    "            print(f\"EM converged in {i+1} iterations. Log-likelihood: {current_log_likelihood:.4f}\")\n",
    "            break\n",
    "\n",
    "        means, covariances, weights = new_means, new_covariances, new_weights\n",
    "    else:\n",
    "        print(f\"EM did not converge after {max_iter} iterations. Final log-likelihood: {current_log_likelihood:.4f}\")\n",
    "\n",
    "    return means, covariances, weights, log_likelihood_history, responsibilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ecd9f",
   "metadata": {},
   "source": [
    "### Example: Fitting a GMM to Data\n",
    "\n",
    "Let's generate some synthetic data from a known GMM and then use our EM implementation to recover the parameters. We'll visualize the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f935e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Main Execution: GMM EM Example ---\n",
    "\n",
    "# 1. Generate synthetic GMM data\n",
    "num_samples = 500\n",
    "num_components = 3\n",
    "X_gmm, true_weights, true_means, true_covariances = generate_gmm_data(num_samples, num_components)\n",
    "\n",
    "print(\"True GMM Parameters:\")\n",
    "print(\"  Weights:\", true_weights)\n",
    "print(\"  Means:\\n\", true_means)\n",
    "print(\"  Covariances:\\n\", true_covariances)\n",
    "\n",
    "# 2. Run the EM algorithm\n",
    "em_key = jax.random.PRNGKey(10)\n",
    "estimated_means, estimated_covariances, estimated_weights, ll_history, final_responsibilities = \\\n",
    "    run_gmm_em(X_gmm, num_components, max_iter=200, tol=1e-5, key=em_key)\n",
    "\n",
    "print(\"\\nEstimated GMM Parameters after EM:\")\n",
    "print(\"  Weights:\", estimated_weights)\n",
    "print(\"  Means:\\n\", estimated_means)\n",
    "print(\"  Covariances:\\n\", estimated_covariances)\n",
    "\n",
    "# 3. Plot the final GMM fit\n",
    "plot_gmm_plotly(\n",
    "    X_gmm,\n",
    "    estimated_means,\n",
    "    estimated_covariances,\n",
    "    responsibilities=final_responsibilities,\n",
    "    title='GMM Fit after EM Algorithm'\n",
    ")\n",
    "\n",
    "# 4. Plot the log-likelihood history\n",
    "fig_ll = go.Figure()\n",
    "fig_ll.add_trace(go.Scatter(\n",
    "    x=jnp.arange(len(ll_history)),\n",
    "    y=jnp.array(ll_history),\n",
    "    mode='lines',\n",
    "    name='Log-Likelihood'\n",
    "))\n",
    "fig_ll.update_layout(title_text='Log-Likelihood during EM Iterations', title_x=0.5,\n",
    "                     xaxis_title='Iteration',\n",
    "                     yaxis_title='Log-Likelihood')\n",
    "fig_ll.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b4602",
   "metadata": {},
   "source": [
    "The plots should show the data points clustered according to the estimated Gaussian components, with ellipses representing their covariances. The log-likelihood plot should demonstrate a non-decreasing trend, indicating the EM algorithm's convergence towards a local maximum of the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e84d8c",
   "metadata": {},
   "source": [
    "#### 4. The EM Algorithm for MAP Estimation\n",
    "\n",
    "It is straightforward to extend the EM algorithm to find the **Maximum A Posteriori (MAP) estimate** instead of the maximum likelihood estimate (Slide 11). This involves simply adding a log-prior term for the parameters $\\theta$ to the objective function in the M-step:\n",
    "\n",
    "$$\\theta_{new} = \\arg \\max_{\\theta} \\left( \\int q(z) \\log \\frac{p(x, z | \\theta)}{q(z)} dz + \\log p(\\theta) \\right)$$\n",
    "\n",
    "This effectively maximizes $\\mathcal{L}(q, \\theta) + \\log p(\\theta)$, which is a lower bound on $\\log p(\\theta | x)$. The E-step remains the same, as it only depends on the likelihood $p(z|x, \\theta_{old})$. The M-step then optimizes the combination of the expected complete-data log-likelihood and the log-prior over parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd4167f",
   "metadata": {},
   "source": [
    "#### 5. Why is EM Useful? (Analytic Updates)\n",
    "\n",
    "One of the main reasons EM is so useful is that for many models, particularly those where the complete-data likelihood $p(x, z | \\theta)$ belongs to an exponential family, the M-step updates can be found **analytically** (Slide 12). This is because the expectation of the sufficient statistics is often all that's needed.\n",
    "\n",
    "For instance, if $p(x, z | \\theta) = \\exp(\\phi(x, z)^T \\theta - \\log Z(\\theta))$, then the ELBO is $\\mathbb{E}_{q(z)}[\\phi(x, z)]^T \\theta - \\log Z(\\theta)$. Maximizing this with respect to $\\theta$ often leads to closed-form solutions, as demonstrated with the GMM updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd66b182",
   "metadata": {},
   "source": [
    "#### 6. Summary\n",
    "\n",
    "In summary (Slide 26):\n",
    "\n",
    "* The **EM algorithm** is an iterative procedure to find maximum likelihood (or MAP) estimates for models involving latent variables.\n",
    "* It works by alternating between an **E-step** (computing the posterior over latent variables given current parameters) and an **M-step** (maximizing the expected complete-data log-likelihood with respect to parameters).\n",
    "* EM is guaranteed to converge to a local maximum of the model evidence (marginal likelihood) because each step increases a lower bound (ELBO) on the evidence.\n",
    "* For many models, especially those whose complete-data likelihood belongs to an exponential family, the M-step updates are **analytic**, making EM a computationally attractive solution.\n",
    "\n",
    "The EM algorithm is a cornerstone of many probabilistic models, including mixture models, Hidden Markov Models, and factor analysis, providing a robust framework for learning in the presence of unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e03405",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "**Exercise 1: Impact of Initialization**\n",
    "The EM algorithm is sensitive to initialization. In the `run_gmm_em` function, change the `em_key` to different random seeds (e.g., 10, 20, 30). Observe how the final estimated parameters and the log-likelihood history change. Plot the final GMM fits for a few different initializations. Discuss why initialization matters for EM.\n",
    "\n",
    "**Exercise 2: Varying Number of Components**\n",
    "Change `num_components` in the `main` execution block (e.g., to 2 or 4). How does the fit change? What happens if `num_components` is too low or too high compared to the true number of components in the data? (You might need to adjust `max_iter` or `tol` for convergence).\n",
    "\n",
    "**Exercise 3: Implementing Log-Likelihood for Convergence**\n",
    "The provided `compute_log_likelihood` function calculates the log-likelihood. Explain, in detail, why this specific formula correctly represents the log-likelihood of the observed data $X$ under the GMM, given the parameters $\\pi, \\mu, \\Sigma$. Relate it back to the definition of the GMM likelihood.\n",
    "\n",
    "**Exercise 4 (Advanced): EM for a Simple HMM (Conceptual)**\n",
    "Briefly research how the EM algorithm (specifically, the Baum-Welch algorithm) is applied to Hidden Markov Models (HMMs). Conceptually describe what the E-step and M-step would involve for an HMM with discrete states and Gaussian observations. How do these steps relate to the general EM framework?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
