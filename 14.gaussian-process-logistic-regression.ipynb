{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5f9421",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning: Lecture 14 - Logistic Regression with JAX and Plotly\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Welcome to Lecture 14 of Probabilistic Machine Learning, focusing on Logistic Regression! This notebook aims to translate the core concepts from the lecture slides into a clear, understandable format, augmented with Python code examples and exercises to reinforce your learning. We'll be using **JAX** for efficient numerical computations and **Plotly** for interactive visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf4544",
   "metadata": {},
   "source": [
    "#### 1. From Regression to Classification: A Conceptual Shift\n",
    "\n",
    "In previous lectures, we primarily focused on **regression** problems, where the goal is to predict a continuous output variable $Y$ given input data $X$. For instance, in least-squares regression, we aim to find a function $f: X \\to \\mathbb{R}^d$ such that $Y \\approx f(X)$.\n",
    "\n",
    "However, many real-world problems involve predicting discrete categories or labels. This is where **classification** comes in. In classification, given supervised data \n",
    "$$(X, Y) = (x_i, c_i)_{i=1,...,n}$$\n",
    " where $x_i \\in \\mathcal{X}$ and $c_i \\in \\{1, ..., d\\}$, our goal is to find a probability distribution \n",
    " $$\\pi: X \\to \\mathcal{U}^d$$\n",
    "  (where $\\mathcal{U}^d = \\{p \\in [0, 1]^d : \\sum_{i=1}^d p_i = 1\\}$) such that $\\pi$ \"models\" $y_i \\sim \\pi_{x_i}$.\n",
    "\n",
    "**Key Distinction**: Regression predicts a function, while classification predicts a probability.\n",
    "\n",
    "For simplicity, we will initially focus on **discriminative binary classification**, where the output $y$ can take one of two values, typically $y \\in \\{-1, +1\\}$. In this setting, we want to learn $\\pi(x) = p(y | x) \\in [0, 1]$. The probability distribution for $y$ given $x$ can be expressed as:\n",
    "\n",
    "$$p(y | x) = \\begin{cases} \\pi(x) & \\text{if } y = 1 \\\\ 1 - \\pi(x) & \\text{if } y = -1 \\end{cases}$$\n",
    "\n",
    "##### **Visualizing Classification Problems**\n",
    "\n",
    "The lecture slides provide several examples of classification problems with varying degrees of separability. Let's recreate these visualizations to understand what we're trying to achieve.\n",
    "\n",
    "* **Slide 6 & 7**: Clearly separable classes. A linear boundary works well.\n",
    "* **Slide 8 & 9**: Overlapping classes. A linear boundary still attempts to separate, but there will be misclassifications.\n",
    "* **Slide 10 & 11**: Highly intermingled classes. A simple linear boundary is insufficient.\n",
    "\n",
    "**Activity 1.1: Recreate Classification Problem Visualizations**\n",
    "\n",
    "Let's generate some synthetic 2D data to mimic the classification problems shown in the slides. We'll start by defining our necessary imports and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c1e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np  # Still using numpy for data generation\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set JAX to use 64-bit floats for numerical stability, matching numpy's default\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def sigmoid(f):\n",
    "    \"\"\"Logistic sigmoid function.\"\"\"\n",
    "    return 1 / (1 + jnp.exp(-f))\n",
    "\n",
    "\n",
    "def rbf_kernel(X1, X2, length_scale=1.0):\n",
    "    \"\"\"Radial Basis Function (RBF) kernel.\"\"\"\n",
    "    sqdist = jnp.sum(X1**2, 1)[:, None] + jnp.sum(X2**2, 1) - 2 * jnp.dot(X1, X2.T)\n",
    "    return jnp.exp(-0.5 * (1 / length_scale**2) * sqdist)\n",
    "\n",
    "\n",
    "def generate_data(type=\"separable\", n_samples=100):\n",
    "    \"\"\"Generates synthetic 2D classification data (using numpy for random generation).\"\"\"\n",
    "    np.random.seed(42)  # Ensure reproducibility for data generation\n",
    "    if type == \"separable\":\n",
    "        mean1 = [-1, 0.5]\n",
    "        cov1 = [[0.5, 0.2], [0.2, 0.5]]\n",
    "        data1 = np.random.multivariate_normal(mean1, cov1, n_samples // 2)\n",
    "        labels1 = np.ones(n_samples // 2) * -1\n",
    "        mean2 = [1, -0.5]\n",
    "        cov2 = [[0.5, 0.2], [0.2, 0.5]]\n",
    "        data2 = np.random.multivariate_normal(mean2, cov2, n_samples // 2)\n",
    "        labels2 = np.ones(n_samples // 2)\n",
    "    elif type == \"overlapping\":\n",
    "        mean1 = [-0.5, 0.5]\n",
    "        cov1 = [[1.0, 0.5], [0.5, 1.0]]\n",
    "        data1 = np.random.multivariate_normal(mean1, cov1, n_samples // 2)\n",
    "        labels1 = np.ones(n_samples // 2) * -1\n",
    "        mean2 = [0.5, -0.5]\n",
    "        cov2 = [[1.0, 0.5], [0.5, 1.0]]\n",
    "        data2 = np.random.multivariate_normal(mean2, cov2, n_samples // 2)\n",
    "        labels2 = np.ones(n_samples // 2)\n",
    "    elif type == \"intermingled\":\n",
    "        r1 = np.random.rand(n_samples // 2) * 2\n",
    "        theta1 = np.random.rand(n_samples // 2) * 2 * np.pi\n",
    "        data1 = np.array(\n",
    "            [\n",
    "                r1 * np.cos(theta1) + np.random.randn(n_samples // 2) * 0.2,\n",
    "                r1 * np.sin(theta1) + np.random.randn(n_samples // 2) * 0.2,\n",
    "            ]\n",
    "        ).T\n",
    "        labels1 = np.ones(n_samples // 2) * -1\n",
    "        r2 = np.random.rand(n_samples // 2) * 2 + 1.5\n",
    "        theta2 = np.random.rand(n_samples // 2) * 2 * np.pi\n",
    "        data2 = np.array(\n",
    "            [\n",
    "                r2 * np.cos(theta2) + np.random.randn(n_samples // 2) * 0.2,\n",
    "                r2 * np.sin(theta2) + np.random.randn(n_samples // 2) * 0.2,\n",
    "            ]\n",
    "        ).T\n",
    "        labels2 = np.ones(n_samples // 2)\n",
    "        all_data = np.vstack((data1, data2))\n",
    "        all_labels = np.hstack((labels1, labels2))\n",
    "        perm = np.random.permutation(n_samples)\n",
    "        data1 = all_data[all_labels == -1]\n",
    "        data2 = all_data[all_labels == 1]\n",
    "    X = np.vstack((data1, data2))\n",
    "    y = np.hstack((labels1, labels2))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_data_plotly(X, y, title=\"\", fig=None, row=None, col=None):\n",
    "    \"\"\"Plots 2D classification data using Plotly.\"\"\"\n",
    "    if fig is None:\n",
    "        fig = go.Figure()\n",
    "\n",
    "    # Convert JAX arrays to NumPy for Plotly\n",
    "    X_np = np.asarray(X)\n",
    "    y_np = np.asarray(y)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_np[y_np == -1, 0],\n",
    "            y=X_np[y_np == -1, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=\"maroon\", symbol=\"circle\"),\n",
    "            name=\"Class -1\",\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_np[y_np == 1, 0],\n",
    "            y=X_np[y_np == 1, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                color=\"skyblue\", symbol=\"circle\", line=dict(width=1, color=\"skyblue\")\n",
    "            ),\n",
    "            name=\"Class +1\",\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title_text=title, title_x=0.5)\n",
    "    fig.update_xaxes(title_text=\"$x_1$\", range=[-4, 4], row=row, col=col)\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"$x_2$\",\n",
    "        range=[-4, 4],\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1,\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e0c3bf",
   "metadata": {},
   "source": [
    "Now, let's generate and plot our synthetic classification datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4089a27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate different types of data for demonstration\n",
    "X_sep_np, y_sep_np = generate_data(type=\"separable\", n_samples=100)\n",
    "X_ovl_np, y_ovl_np = generate_data(type=\"overlapping\", n_samples=100)\n",
    "X_int_np, y_int_np = generate_data(type=\"intermingled\", n_samples=100)\n",
    "\n",
    "# Create subplots for data visualization\n",
    "fig_data = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=(\"Separable Classes\", \"Overlapping Classes\", \"Intermingled Classes\"),\n",
    ")\n",
    "\n",
    "plot_data_plotly(X_sep_np, y_sep_np, fig=fig_data, row=1, col=1)\n",
    "plot_data_plotly(X_ovl_np, y_ovl_np, fig=fig_data, row=1, col=2)\n",
    "plot_data_plotly(X_int_np, y_int_np, fig=fig_data, row=1, col=3)\n",
    "\n",
    "fig_data.update_layout(showlegend=False, height=500, width=1500)\n",
    "fig_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d5aa0",
   "metadata": {},
   "source": [
    "#### 2. Link Functions: Connecting Latent Functions to Probabilities\n",
    "\n",
    "Since classification predicts probabilities, and probabilities are restricted to the $[0, 1]$ interval, we cannot directly use a linear model $f(x) = w^T x + b$ (which can output any real value). We need a \"link function\" that maps the real-valued output of a latent function (like $f(x)$) to a probability.\n",
    "\n",
    "The most common link function in binary classification is the **logistic sigmoid function** (often just called the sigmoid function):\n",
    "\n",
    "$$\\sigma(f) = \\frac{1}{1 + \\exp(-f)}$$\n",
    "\n",
    "This function has several desirable properties:\n",
    "* It maps any real number $f$ to a value between 0 and 1.\n",
    "* It is monotonically increasing.\n",
    "* It is differentiable, which is crucial for optimization.\n",
    "\n",
    "The inverse of the sigmoid function, $f(\\pi) = \\ln \\frac{\\pi}{1-\\pi}$, is called the **logit function**. The derivative of the sigmoid function with respect to $f$ is given by $\\frac{d\\pi}{df} = \\pi(f)(1 - \\pi(f))$.\n",
    "\n",
    "**Activity 2.1: Plot the Sigmoid Function**\n",
    "\n",
    "Let's visualize the sigmoid function and its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf02d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a range of f values\n",
    "f_values = np.linspace(-5, 5, 100)\n",
    "sigma_f_values = sigmoid(jnp.array(f_values))  # Convert to JAX array for sigmoid\n",
    "\n",
    "fig_sigmoid = go.Figure()\n",
    "fig_sigmoid.add_trace(\n",
    "    go.Scatter(\n",
    "        x=f_values,\n",
    "        y=np.asarray(sigma_f_values),\n",
    "        mode=\"lines\",\n",
    "        name=\"$\\\\sigma(f) = 1 / (1 + \\\\exp(-f))$\",\n",
    "    )\n",
    ")\n",
    "fig_sigmoid.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=-5,\n",
    "    y0=0.5,\n",
    "    x1=5,\n",
    "    y1=0.5,\n",
    "    line=dict(color=\"gray\", dash=\"dot\", width=0.8),\n",
    ")\n",
    "fig_sigmoid.add_shape(\n",
    "    type=\"line\", x0=0, y0=0, x1=0, y1=1, line=dict(color=\"gray\", dash=\"dot\", width=0.8)\n",
    ")\n",
    "fig_sigmoid.add_annotation(\n",
    "    x=0.1, y=0.55, text=\"$\\\\sigma(0) = 0.5$\", showarrow=False, font=dict(size=10)\n",
    ")\n",
    "fig_sigmoid.update_layout(\n",
    "    title_text=\"Logistic Sigmoid Link Function\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"$f$\",\n",
    "    yaxis_title=\"$\\\\sigma(f)$\",\n",
    ")\n",
    "fig_sigmoid.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3589d2bd",
   "metadata": {},
   "source": [
    "#### 3. Gaussian Process Model for Classification: Logistic Regression\n",
    "\n",
    "In a probabilistic setting, we can define a **Gaussian Process (GP) model for classification**, often referred to as **Logistic Regression** when using a linear basis function or when the latent function $f$ is a Gaussian Process.\n",
    "\n",
    "The core idea is to place a Gaussian Process prior over the latent function $f(x)$:\n",
    "\n",
    "$$p(f) = \\mathcal{GP}(f; m, k)$$\n",
    "\n",
    "where $m$ is the mean function and $k$ is the covariance (kernel) function.\n",
    "\n",
    "Then, the likelihood of observing a label $y$ given the latent function output $f_x$ at point $x$ is defined using the sigmoid link function:\n",
    "\n",
    "$$p(y | f_x) = \\sigma(y f_x)$$\n",
    "\n",
    "This can be explicitly written as:\n",
    "\n",
    "$$p(y | f_x) = \\begin{cases} \\sigma(f_x) & \\text{if } y = 1 \\\\ 1 - \\sigma(f_x) & \\text{if } y = -1 \\end{cases}$$\n",
    "\n",
    "*(Self-reflection): Note that $\\sigma(f)$ and $1-\\sigma(f)$ are equivalent to $\\sigma(f)$ and $\\sigma(-f)$ respectively, due to the property $\\sigma(x) = 1 - \\sigma(-x)$. This simplification is useful for writing the likelihood compactly as $\\sigma(y f_x)$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13354504",
   "metadata": {},
   "source": [
    "#### 4. The Challenge: Non-Gaussian Posterior\n",
    "\n",
    "The beauty of Gaussian Processes in regression is that if the likelihood (noise model) is Gaussian, the posterior distribution over the latent function $f$ (and thus the predictions) remains Gaussian, making inference tractable.\n",
    "\n",
    "However, with the logistic sigmoid likelihood, the posterior distribution $p(f_X | Y)$ is **not Gaussian**. This makes exact inference intractable.\n",
    "\n",
    "Let's look at the log-posterior:\n",
    "\n",
    "$$\\log p(f_X | Y) = \\log p(Y | f_X) + \\log p(f_X) - \\log p(Y)$$\n",
    "\n",
    "Substituting the GP prior and the sigmoid likelihood:\n",
    "\n",
    "$$\\log p(f_X | Y) = \\sum_{i=1}^n \\log \\sigma(y_i f_{x_i}) - \\frac{1}{2}(f_X - m_X)^T K_{XX}^{-1} (f_X - m_X) + \\text{const.}$$\n",
    "\n",
    "where $\\log \\sigma(y_i f_{x_i}) = -\\log(1 + e^{-y_i f_{x_i}})$.\n",
    "\n",
    "The presence of the $\\log(1 + e^{-y_i f_{x_i}})$ term in the sum makes the posterior non-Gaussian. This means we cannot simply compute the posterior mean and covariance analytically as we would in GP regression.\n",
    "\n",
    "**Visualizing Intractability (Slide 21, 22, 23 of Lecture 14)**\n",
    "\n",
    "The slides illustrate this non-Gaussian posterior by showing a contour plot that is not perfectly elliptical (which would be the case for a Gaussian). This non-elliptical shape signifies the intractability.\n",
    "\n",
    "Let's define the `log_posterior` function that we will maximize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93f536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_posterior(f_X, K_XX, y_labels):\n",
    "    \"\"\"\n",
    "    Calculates the log posterior of f_X given observations y_labels.\n",
    "    Assumes mean_X = 0.\n",
    "    \"\"\"\n",
    "    log_likelihood_term = jnp.sum(-jnp.log(1 + jnp.exp(-y_labels * f_X)))\n",
    "    log_prior_term = -0.5 * f_X.T @ jnp.linalg.solve(K_XX, f_X)\n",
    "    return log_likelihood_term + log_prior_term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90271e",
   "metadata": {},
   "source": [
    "#### 5. Solution: The Laplace Approximation (Introduction)\n",
    "\n",
    "Since exact inference is intractable, we resort to approximation methods. One popular and computationally efficient method is the **Laplace Approximation**.\n",
    "\n",
    "The core idea of the Laplace Approximation is to approximate a non-Gaussian probability distribution $p(\\theta)$ with a local Gaussian distribution $q(\\theta)$. This approximation is built around a (local) maximum of $p(\\theta)$ (or equivalently, $\\log p(\\theta)$), which is known as the Maximum A Posteriori (MAP) estimate.\n",
    "\n",
    "Here are the steps:\n",
    "1.  **Find the MAP estimate**: Locate $\\hat{\\theta} = \\arg \\max_{\\theta} \\log p(\\theta)$. At this point, the gradient of the log-posterior is zero: $\\nabla \\log p(\\hat{\\theta}) = 0$.\n",
    "2.  **Perform a second-order Taylor expansion**: Expand $\\log p(\\theta)$ around $\\hat{\\theta}$:\n",
    "    $$\\log p(\\theta) \\approx \\log p(\\hat{\\theta}) + \\frac{1}{2}(\\theta - \\hat{\\theta})^T \\Psi (\\theta - \\hat{\\theta})$$\n",
    "    where $\\Psi = \\nabla\\nabla^T \\log p(\\hat{\\theta})$ is the Hessian matrix (matrix of second derivatives) evaluated at $\\hat{\\theta}$.\n",
    "3.  **Define the Gaussian approximation**: The Laplace approximation $q(\\theta)$ is then a Gaussian distribution with mean $\\hat{\\theta}$ and covariance matrix $-\\Psi^{-1}$:\n",
    "    $$q(\\theta) = \\mathcal{N}(\\theta; \\hat{\\theta}, -\\Psi^{-1})$$\n",
    "\n",
    "The Laplace approximation is a local approximation and can be arbitrarily wrong, but it tends to be computationally efficient and often works well for logistic regression because the log posterior is concave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd296f82",
   "metadata": {},
   "source": [
    "#### 6. Numerical Implementation: Mode-Finding with Newton's Method (JAX)\n",
    "\n",
    "To implement the mode-finding part (finding $\\hat{f}$), we use Newton's method. This iterative optimization algorithm requires the gradient and Hessian of the log posterior. With JAX, we can automatically compute these derivatives, simplifying our implementation.\n",
    "\n",
    "Let $L(f_X) = \\log p(f_X | Y)$. We aim to maximize $L(f_X)$, which is equivalent to minimizing $-L(f_X)$.\n",
    "\n",
    "The Newton-Raphson update rule is:\n",
    "$$f^{new} = f^{old} - \\mathbf{H}^{-1} \\mathbf{g}$$\n",
    "\n",
    "where $\\mathbf{g}$ is the gradient of $L(f_X)$ and $\\mathbf{H}$ is the Hessian of $L(f_X)$. Alternatively, if we work with the negative log posterior, the update is $f^{new} = f^{old} - (\\text{Hessian of } -L(f_X))^{-1} (\\text{Gradient of } -L(f_X))$.\n",
    "\n",
    "We will define the `newton_step` function using JAX's automatic differentiation capabilities and then the `find_map_f_newton_jax` function to perform the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cddf93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit  # JIT compile the optimization step for performance\n",
    "def newton_step(f_X, K_XX, y_labels):\n",
    "    \"\"\"Performs one Newton update step for maximizing the log posterior.\"\"\"\n",
    "    # We want to maximize log_posterior, so we minimize -log_posterior\n",
    "    neg_log_posterior = lambda f: -log_posterior(f, K_XX, y_labels)\n",
    "\n",
    "    # Compute gradient and Hessian of the *negative* log posterior\n",
    "    grad_neg_log_post = jax.grad(neg_log_posterior)(f_X)\n",
    "    hess_neg_log_post = jax.hessian(neg_log_posterior)(f_X)\n",
    "\n",
    "    # Newton update: f_new = f_old - H_inv * grad\n",
    "    delta_f = jnp.linalg.solve(hess_neg_log_post, grad_neg_log_post)\n",
    "    f_X_new = f_X - delta_f\n",
    "    return f_X_new\n",
    "\n",
    "\n",
    "def find_map_f_newton_jax(\n",
    "    X_train, y_train, kernel_func, length_scale, max_iter=100, tol=1e-5\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds the MAP estimate hat_f using Newton's method with JAX autodiff.\n",
    "    Returns hat_f and the kernel matrix K_XX.\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    f_X = jnp.zeros(n)  # Initialize f_X with zeros\n",
    "    K_XX = kernel_func(X_train, X_train, length_scale)\n",
    "    K_XX += 1e-6 * jnp.eye(n)  # Add jitter for numerical stability\n",
    "    y_labels_jax = jnp.array(y_train)\n",
    "\n",
    "    print(\"Starting Newton's method with JAX...\")\n",
    "    for i in range(max_iter):\n",
    "        f_X_new = newton_step(f_X, K_XX, y_labels_jax)\n",
    "        change = jnp.linalg.norm(f_X_new - f_X)\n",
    "        if change < tol:\n",
    "            print(f\"Converged in {i + 1} iterations. Final change: {change:.6f}\")\n",
    "            break\n",
    "        f_X = f_X_new\n",
    "    else:\n",
    "        print(\n",
    "            f\"Newton's method did not converge after {max_iter} iterations. Final change: {change:.6f}\"\n",
    "        )\n",
    "    return f_X, K_XX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc91546",
   "metadata": {},
   "source": [
    "Now, let's apply the Newton's method to find the MAP estimate for our separable dataset and visualize the resulting decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e37993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert training data to JAX arrays for use in JAX functions\n",
    "X_train_jax = jnp.array(X_ovl_np)  # Using the separable data generated earlier\n",
    "y_train_jax = jnp.array(y_ovl_np)\n",
    "\n",
    "# Find the MAP estimate hat_f\n",
    "hat_f_jax, K_XX_jax = find_map_f_newton_jax(\n",
    "    X_train_jax, y_train_jax, rbf_kernel, length_scale=1.0\n",
    ")\n",
    "\n",
    "print(\"\\nMAP estimate (hat_f) first 5 elements:\")\n",
    "print(hat_f_jax[:5])\n",
    "\n",
    "# Prepare a grid of test points for visualization\n",
    "x1_grid = np.linspace(-4, 4, 100)\n",
    "x2_grid = np.linspace(-4, 4, 100)\n",
    "X1_mesh, X2_mesh = np.meshgrid(x1_grid, x2_grid)\n",
    "X_grid_np = np.vstack([X1_mesh.ravel(), X2_mesh.ravel()]).T\n",
    "X_grid_jax = jnp.array(X_grid_np)\n",
    "\n",
    "# Compute mean of the latent function at grid points given hat_f\n",
    "# E[f_* | hat_f] = k_x*X @ K_XX^-1 @ hat_f (assuming m_X=0)\n",
    "K_x_X_grid = rbf_kernel(X_grid_jax, X_train_jax, length_scale=2.0)\n",
    "K_XX_inv = jnp.linalg.inv(K_XX_jax)\n",
    "mean_latent_predictions_jax = K_x_X_grid @ K_XX_inv @ hat_f_jax\n",
    "\n",
    "# Convert latent predictions to probabilities using sigmoid (plug-in approximation)\n",
    "prob_predictions_jax = sigmoid(mean_latent_predictions_jax)\n",
    "prob_predictions_mesh_np = np.array(prob_predictions_jax).reshape(X1_mesh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5381f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Predictive Probabilities with Plotly\n",
    "fig_prob = go.Figure()\n",
    "fig_prob.add_trace(\n",
    "    go.Contour(\n",
    "        z=prob_predictions_mesh_np,\n",
    "        x=x1_grid,\n",
    "        y=x2_grid,\n",
    "        colorscale=\"RdBu\",\n",
    "        opacity=0.6,\n",
    "        colorbar_title=\"Predicted Probability $P(y=1|x)$\",\n",
    "        line_smoothing=0.8,  # Smooth contours\n",
    "        contours_coloring=\"heatmap\",  # Color between contours\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add decision boundary (0.5 probability contour)\n",
    "fig_prob.add_trace(\n",
    "    go.Contour(\n",
    "        z=prob_predictions_mesh_np,\n",
    "        x=x1_grid,\n",
    "        y=x2_grid,\n",
    "        showscale=False,\n",
    "        contours=dict(\n",
    "            start=0.5,\n",
    "            end=0.5,\n",
    "            size=0,\n",
    "            coloring=\"lines\",\n",
    "            showlabels=True,\n",
    "            labelfont=dict(size=12, color=\"black\"),\n",
    "        ),\n",
    "        line_width=2,\n",
    "        name=\"Decision Boundary ($P(y=1|x)=0.5$)\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add training data points\n",
    "fig_prob = plot_data_plotly(X_train_jax, y_train_jax, fig=fig_prob)\n",
    "\n",
    "fig_prob.update_layout(\n",
    "    title_text=\"Logistic Regression: Predictive Probabilities (Plug-in) with JAX and Plotly\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"$x_1$\",\n",
    "    yaxis_title=\"$x_2$\",\n",
    "    height=600,\n",
    "    width=800,\n",
    "    template=\"plotly_white\",\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5,\n",
    "        font=dict(size=14),\n",
    "    ),\n",
    ")\n",
    "fig_prob.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff7dcb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### What Does the Contour Plot Show?\n",
    "\n",
    "The contour plot you see above visualizes the **predicted probability** of class $+1$ (i.e., $P(y=1|x)$) across the 2D input space for the logistic regression model trained on your data.\n",
    "\n",
    "#### Key Elements of the Plot\n",
    "\n",
    "- **Background Colors (Contours):**\n",
    "    - Each color represents a different predicted probability value for $P(y=1|x)$.\n",
    "        - Redder regions (or one end of the color scale) indicate higher probabilities of class $+1$.\n",
    "        - Bluer regions (or the other end) indicate higher probabilities of class $-1$.\n",
    "        - The color smoothly transitions, showing how the model's confidence changes across the input space.\n",
    "\n",
    "    - **Black Line (Decision Boundary):**\n",
    "        - The thick black contour corresponds to $P(y=1|x) = 0.5$.\n",
    "        - This line represents the set of points where the model is equally uncertain between class $+1$ and class $-1$—in other words, the model's \"best guess\" at the boundary between the two classes.\n",
    "\n",
    "    - **Data Points:**\n",
    "        - The scatter points overlaying the contours show the actual training data, colored by their true class.\n",
    "        - This helps you see how well the decision boundary separates the two classes and where the model might be uncertain.\n",
    "\n",
    "    #### How to Interpret the Plot\n",
    "\n",
    "    - Regions far from the decision boundary (deep red or blue) indicate high model confidence in predicting a particular class.\n",
    "    - Regions near the decision boundary (where the color is more neutral) indicate uncertainty, with predicted probabilities close to $0.5$.\n",
    "    - The shape of the decision boundary reflects the flexibility of the model and the influence of the kernel (RBF in this case).\n",
    "\n",
    "    This visualization is a powerful way to understand not just the model's predictions, but also its uncertainty and the effect of the chosen kernel and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e4bec1",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "### Why Does the Model Become Uncertain Far from the Data?\n",
    "\n",
    "Great observation! In the contour plot, you may notice that the model is **most confident** (probabilities close to 0 or 1) near the training data, especially away from the decision boundary. However, **far from the data**—even far from the boundary—the predicted probabilities tend to approach 0.5, indicating **high uncertainty**.\n",
    "\n",
    "#### **Why Does This Happen?**\n",
    "\n",
    "- **Gaussian Process Prior:**  \n",
    "    The GP prior assumes that, in regions where there are no data points, the latent function $f(x)$ reverts to its prior mean (often zero). The model has no evidence to push the prediction toward either class, so it defaults to being maximally uncertain.\n",
    "\n",
    "- **Sigmoid Link Function:**  \n",
    "    The sigmoid function $\\sigma(f(x))$ maps $f(x) = 0$ to a probability of 0.5. Thus, in regions where $f(x)$ is close to zero (i.e., where the model is unsure), the predicted probability is 0.5.\n",
    "\n",
    "- **Kernel Influence:**  \n",
    "    The RBF kernel causes the influence of each data point to decay rapidly with distance. Far from any data, the kernel values are near zero, so the posterior mean of $f(x)$ is close to the prior mean.\n",
    "\n",
    "#### **Summary Table**\n",
    "\n",
    "| Region                  | Model Confidence | Reason                                      |\n",
    "|-------------------------|------------------|---------------------------------------------|\n",
    "| Near data, away from boundary | High (close to 0 or 1) | Strong evidence from nearby labeled data    |\n",
    "| Near decision boundary  | Low (close to 0.5) | Model is uncertain between classes          |\n",
    "| Far from all data       | Low (close to 0.5) | No evidence; model reverts to prior         |\n",
    "\n",
    "#### **Key Takeaway**\n",
    "\n",
    "> **In Gaussian Process classification, the model is only confident where it has seen data. Far from the data, it expresses uncertainty, which is a desirable property in probabilistic modeling.**\n",
    "\n",
    "This behavior is a hallmark of Bayesian models: **uncertainty increases in regions with little or no data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90a434",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "**Exercise 1: Impact of Kernel Length Scale**\n",
    "Modify the `length_scale` parameter in the `rbf_kernel` and observe its effect on the learned decision boundary. How does a very small `length_scale` differ from a very large one? Explain why.\n",
    "\n",
    "**Exercise 2: Non-Separable Data**\n",
    "Change the `generate_data` type to `'overlapping'` and `'intermingled'`. Rerun the mode-finding and plotting. How does the decision boundary change? What are the limitations of a linear-like decision boundary (which is what we get with the RBF kernel and no explicit feature mapping)?\n",
    "\n",
    "**Exercise 3: Iteration Count and Convergence**\n",
    "Experiment with the `max_iter` and `tol` parameters in `find_map_f_newton_jax`. How do they affect the convergence speed and the quality of the solution? What happens if `max_iter` is too small?\n",
    "\n",
    "**Exercise 4: Exploring the Log Posterior**\n",
    "For a very simple 1D classification problem with two data points, plot the log posterior function. Can you visually verify its concavity (which allows Newton's method to work well)? *(Hint: This is an advanced exercise. You'll need to define a 1D problem and plot a 2D surface for $\\log p(f_1, f_2 | y_1, y_2)$.)*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
