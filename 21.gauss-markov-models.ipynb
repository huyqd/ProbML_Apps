{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c6a8bf",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning: Lecture 21 - Hidden Markov Models\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Welcome to Lecture 21 of Probabilistic Machine Learning! This lecture continues our exploration of time series models, moving beyond the linear Gaussian assumptions of Gauss-Markov models to introduce **Hidden Markov Models (HMMs)**. We will delve into the connections between Gaussian Processes and Gauss-Markov Processes, discuss how to learn the parameters of these models, and, crucially, address the challenge of inference when the world isn't linear or Gaussian.\n",
    "\n",
    "This notebook will provide theoretical insights and practical illustrations using **JAX** for numerical computations and **Plotly** for interactive visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3931bb5",
   "metadata": {},
   "source": [
    "#### 1. Reminder: Time Series as a Problem Class\n",
    "\n",
    "As a reminder from Lecture 20 (Slide 2), our goal for this week is to understand time series as a problem class. We've covered:\n",
    "\n",
    "* **Application Layer**: Data arriving as a stream.\n",
    "* **Model Structure Layer**: Markov Chains / Hidden Markov Models.\n",
    "* **Concrete Model Layer**: Gauss-Markov Models.\n",
    "* **Algorithm Layer**: Kalman Filter & RTS Smoother.\n",
    "\n",
    "Today, we focus on:\n",
    "\n",
    "* **Theory**: What is the connection between Gaussian processes and Gauss-Markov Processes?\n",
    "* **Parameters**: Can we learn the parameters of a Gauss-Markov model?\n",
    "* **Generalization**: What if the world isn't Gaussian?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed7452",
   "metadata": {},
   "source": [
    "#### 2. Recap: Markov Chains and their Algorithmic Structure\n",
    "\n",
    "A **Markov Chain** formalizes the notion of a stochastic process with a local finite memory, where the future state $x_t$ depends only on the immediate past state $x_{t-1}$ (Slide 3-4): $p(x_t | X_{0:t-1}) = p(x_t | x_{t-1})$. Observations $y_t$ are typically assumed to depend only on the current state $x_t$: $p(y_t | X) = p(y_t | x_t)$.\n",
    "\n",
    "This conditional independence structure allows for efficient inference operations (Slides 5-6), each performed in $\\mathcal{O}(T)$ time for $T$ time steps:\n",
    "\n",
    "* **Filtering**: Estimating the current state given all past and current observations.\n",
    "    * **Predict**: $p(x_t | Y_{0:t-1}) = \\int p(x_t | x_{t-1}) p(x_{t-1} | Y_{0:t-1}) dx_{t-1}$ (Chapman-Kolmogorov Equation)\n",
    "    * **Update**: $p(x_t | Y_{0:t}) = \\frac{p(y_t | x_t) p(x_t | Y_{0:t-1})}{p(y_t)}$ (Bayes' Theorem)\n",
    "* **Smoothing**: Estimating past states given all observations (past, current, and future).\n",
    "    * **Smooth**: $p(x_t | Y) = p(x_t | Y_{0:t}) \\int p(x_{t+1} | x_t) \\frac{p(x_{t+1} | Y)}{p(x_{t+1} | Y_{0:t})} dx_{t+1}$ (backward pass)\n",
    "\n",
    "If the relationships are linear and Gaussian (Gauss-Markov Models), these operations are analytic and given by the Kalman Filter and Rauch-Tung-Striebel Smoother (Slides 7-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29b1a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set JAX to use 64-bit floats for numerical stability\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "# --- Utility Functions (from Lecture 20, slightly adapted) ---\n",
    "def generate_linear_gaussian_data(\n",
    "    T=100,\n",
    "    state_dim=1,\n",
    "    obs_dim=1,\n",
    "    A=None,\n",
    "    Q_std=0.1,\n",
    "    H=None,\n",
    "    R_std=0.1,\n",
    "    m0=None,\n",
    "    P0_std=1.0,\n",
    "    key=None,\n",
    "):\n",
    "    \"\"\"Generates synthetic data from a linear Gaussian state-space model.\"\"\"\n",
    "    if key is None:\n",
    "        key = jax.random.PRNGKey(0)\n",
    "\n",
    "    if A is None:\n",
    "        A = jnp.eye(state_dim) * 0.9  # State transition matrix\n",
    "    if H is None:\n",
    "        H = jnp.eye(obs_dim, state_dim)  # Observation matrix\n",
    "\n",
    "    Q = jnp.eye(state_dim) * Q_std**2  # State noise covariance\n",
    "    R = jnp.eye(obs_dim) * R_std**2  # Observation noise covariance\n",
    "\n",
    "    if m0 is None:\n",
    "        m0 = jnp.zeros(state_dim)  # Initial state mean\n",
    "    P0 = jnp.eye(state_dim) * P0_std**2  # Initial state covariance\n",
    "\n",
    "    states = [m0]\n",
    "    observations = []\n",
    "\n",
    "    for t in range(T):\n",
    "        key, subkey_state, subkey_obs = jax.random.split(key, 3)\n",
    "        # State transition: x_t = A @ x_{t-1} + w_t, w_t ~ N(0, Q)\n",
    "        state_noise = jax.random.multivariate_normal(\n",
    "            subkey_state, jnp.zeros(state_dim), Q\n",
    "        )\n",
    "        next_state = A @ states[-1] + state_noise\n",
    "        states.append(next_state)\n",
    "\n",
    "        # Observation: y_t = H @ x_t + v_t, v_t ~ N(0, R)\n",
    "        obs_noise = jax.random.multivariate_normal(subkey_obs, jnp.zeros(obs_dim), R)\n",
    "        observation = H @ next_state + obs_noise\n",
    "        observations.append(observation)\n",
    "\n",
    "    return jnp.array(states[1:]), jnp.array(observations), A, Q, H, R, m0, P0\n",
    "\n",
    "\n",
    "def plot_time_series_results(\n",
    "    true_signal,\n",
    "    observations,\n",
    "    estimated_mean,\n",
    "    estimated_std=None,\n",
    "    title=\"\",\n",
    "    fig=None,\n",
    "    row=None,\n",
    "    col=None,\n",
    "):\n",
    "    \"\"\"Plots time series data with estimates and uncertainty.\"\"\"\n",
    "    if fig is None:\n",
    "        fig = go.Figure()\n",
    "    time_steps = jnp.arange(len(true_signal))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=time_steps,\n",
    "            y=np.asarray(true_signal).flatten(),\n",
    "            mode=\"lines\",\n",
    "            name=\"True Signal\",\n",
    "            line=dict(color=\"blue\", width=2),\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=time_steps,\n",
    "            y=np.asarray(observations).flatten(),\n",
    "            mode=\"markers\",\n",
    "            name=\"Observations\",\n",
    "            marker=dict(color=\"red\", size=4, opacity=0.6),\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=time_steps,\n",
    "            y=np.asarray(estimated_mean).flatten(),\n",
    "            mode=\"lines\",\n",
    "            name=\"Estimated Mean\",\n",
    "            line=dict(color=\"green\", width=1, dash=\"dash\"),\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    if estimated_std is not None:\n",
    "        estimated_std_np = np.asarray(estimated_std).flatten()\n",
    "        upper_bound = np.asarray(estimated_mean).flatten() + 2 * estimated_std_np\n",
    "        lower_bound = np.asarray(estimated_mean).flatten() - 2 * estimated_std_np\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.concatenate([time_steps, time_steps[::-1]]),\n",
    "                y=np.concatenate([upper_bound, lower_bound[::-1]]),\n",
    "                fill=\"toself\",\n",
    "                fillcolor=\"rgba(0,255,0,0.1)\",\n",
    "                line_color=\"rgba(255,255,255,0)\",\n",
    "                name=\"2 Std Dev Uncertainty\",\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title, title_x=0.5, xaxis_title=\"Time Step\", yaxis_title=\"Value\"\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Kalman Filter (from Lecture 20)\n",
    "@jax.jit\n",
    "def kalman_filter(observations, A, Q, H, R, m0, P0):\n",
    "    \"\"\"\n",
    "    Implements the Kalman Filter for a linear Gaussian state-space model.\n",
    "    \"\"\"\n",
    "    T = observations.shape[0]\n",
    "    state_dim = m0.shape[0]\n",
    "\n",
    "    filtered_means = jnp.zeros((T, state_dim))\n",
    "    filtered_covs = jnp.zeros((T, state_dim, state_dim))\n",
    "    predicted_means = jnp.zeros((T, state_dim))\n",
    "    predicted_covs = jnp.zeros((T, state_dim, state_dim))\n",
    "\n",
    "    m_prev = m0\n",
    "    P_prev = P0\n",
    "\n",
    "    for t in range(T):\n",
    "        # Prediction Step (Time Update)\n",
    "        m_minus = A @ m_prev  # Predictive mean\n",
    "        P_minus = A @ P_prev @ A.T + Q  # Predictive covariance\n",
    "\n",
    "        predicted_means = predicted_means.at[t].set(m_minus)\n",
    "        predicted_covs = predicted_covs.at[t].set(P_minus)\n",
    "\n",
    "        # Update Step (Measurement Update)\n",
    "        z = observations[t] - H @ m_minus  # Innovation residual\n",
    "        S = H @ P_minus @ H.T + R  # Innovation covariance\n",
    "        K = P_minus @ H.T @ jnp.linalg.inv(S)  # Kalman gain\n",
    "\n",
    "        m_t = m_minus + K @ z  # Updated mean\n",
    "        P_t = (jnp.eye(state_dim) - K @ H) @ P_minus  # Updated covariance\n",
    "\n",
    "        filtered_means = filtered_means.at[t].set(m_t)\n",
    "        filtered_covs = filtered_covs.at[t].set(P_t)\n",
    "\n",
    "        m_prev = m_t\n",
    "        P_prev = P_t\n",
    "\n",
    "    return filtered_means, filtered_covs, predicted_means, predicted_covs\n",
    "\n",
    "\n",
    "# RTS Smoother (from Lecture 20)\n",
    "@jax.jit\n",
    "def rts_smoother(filtered_means, filtered_covs, predicted_means, predicted_covs, A, Q):\n",
    "    \"\"\"\n",
    "    Implements the Rauch-Tung-Striebel (RTS) Smoother.\n",
    "    \"\"\"\n",
    "    T = filtered_means.shape[0]\n",
    "    state_dim = filtered_means.shape[1]\n",
    "\n",
    "    smoothed_means = jnp.copy(filtered_means)\n",
    "    smoothed_covs = jnp.copy(filtered_covs)\n",
    "\n",
    "    for t in reversed(range(T - 1)):\n",
    "        # Smoother Gain\n",
    "        G_t = jnp.linalg.solve(predicted_covs[t + 1].T, (filtered_covs[t] @ A.T).T).T\n",
    "\n",
    "        # Smoothed Mean\n",
    "        smoothed_means = smoothed_means.at[t].set(\n",
    "            filtered_means[t] + G_t @ (smoothed_means[t + 1] - predicted_means[t + 1])\n",
    "        )\n",
    "\n",
    "        # Smoothed Covariance\n",
    "        smoothed_covs = smoothed_covs.at[t].set(\n",
    "            filtered_covs[t]\n",
    "            + G_t @ (smoothed_covs[t + 1] - predicted_covs[t + 1]) @ G_t.T\n",
    "        )\n",
    "\n",
    "    return smoothed_means, smoothed_covs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903a601",
   "metadata": {},
   "source": [
    "#### 3. Connection between Gaussian Processes and Gauss-Markov Processes\n",
    "\n",
    "A fascinating theoretical connection exists between certain Gaussian Processes (GPs) and **Linear Time-Invariant Stochastic Differential Equations (LTI-SDEs)** (Slides 10-26). An LTI-SDE describes the local behavior of a Gaussian process. For our purposes, a linear, time-invariant SDE:\n",
    "\n",
    "$$dx(t) = Fx(t) dt + L d\\omega(t)$$\n",
    "\n",
    "where $d\\omega(t)$ is a Wiener process (Brownian motion), describes a Gaussian process with an analytic mean function $m(t)$ and covariance function $k(t, t')$ (Slide 25):\n",
    "\n",
    "$$m(t) = e^{Ft} x_0$$\n",
    "$$k(t, t') = \\int_{\\min(t,t')}^{\\max(t,t')} e^{F(\\max(t,t')-\\tau)} LL^T e^{F^T(\\max(t,t')-\\tau)} d\\tau$$\n",
    "\n",
    "Crucially, these LTI-SDEs can be **discretized exactly** to yield discrete, linear Gaussian transition models (Slide 25):\n",
    "\n",
    "$$p(x(t_{i+1}) | x(t_i)) = \\mathcal{N}(x(t_{i+1}); A_i x(t_i), Q_i)$$\n",
    "\n",
    "with $A_i = e^{F\\Delta t_i}$ and $Q_i = \\int_0^{\\Delta t_i} e^{F(\\Delta t_i - \\tau)} LL^T e^{F^T(\\Delta t_i - \\tau)} d\\tau$, where $\\Delta t_i = t_{i+1} - t_i$.\n",
    "\n",
    "This means that certain GPs (known as Gauss-Markov processes) can be exactly represented as discrete-time linear Gaussian state-space models, allowing their inference to be performed in linear time using the Kalman Filter and Smoother (Slide 26).\n",
    "\n",
    "### Examples of GPs and their SDE counterparts (Slides 28-31):\n",
    "\n",
    "* **Scaled Wiener Process**: $F=0, L=\\theta$. This corresponds to a GP with mean $x_0$ and covariance $k(t, t') = \\theta^2 \\min(t, t')$. Its discrete form has $A_i = I$ and $Q_i = \\theta^2 \\Delta t_i$.\n",
    "* **Ornstein-Uhlenbeck Process**: $F = -1/\\lambda, L = \\sqrt{2}\\theta/\\sqrt{\\lambda}$. This corresponds to a GP with an exponential kernel $k(t, t') = \\theta^2 e^{-|t-t'|/\\lambda}$.\n",
    "* **Integrated Wiener Velocity**: This is a non-scalar example (state includes position and velocity), leading to a polynomial spline kernel.\n",
    "\n",
    "Let's illustrate the discretization of a simple Wiener process SDE into its discrete-time Kalman filter parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02b8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Illustrating SDE Discretization (Wiener Process) ---\n",
    "\n",
    "\n",
    "def discretize_wiener_process(delta_t, theta_sq):\n",
    "    \"\"\"Discretizes a scaled Wiener process SDE into discrete-time Kalman filter parameters.\"\"\"\n",
    "    # SDE: dx(t) = theta * d_omega(t)  (F=0, L=theta)\n",
    "    # Discrete-time: x_{i+1} = A * x_i + w_i, w_i ~ N(0, Q)\n",
    "    # A = exp(F * delta_t) = exp(0 * delta_t) = I\n",
    "    # Q = integral_0^delta_t exp(F*(delta_t - tau)) * L @ L.T * exp(F.T*(delta_t - tau)) d_tau\n",
    "    # For F=0, L=theta: Q = integral_0^delta_t theta^2 I d_tau = theta^2 * delta_t * I\n",
    "\n",
    "    A_discrete = jnp.array([[1.0]])  # Identity matrix for 1D\n",
    "    Q_discrete = jnp.array([[theta_sq * delta_t]])\n",
    "    return A_discrete, Q_discrete\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "delta_t = 0.1\n",
    "theta_sq = 0.5  # Corresponds to Q_std^2 in generate_linear_gaussian_data\n",
    "A_wp, Q_wp = discretize_wiener_process(delta_t, theta_sq)\n",
    "\n",
    "print(\"\\n--- Wiener Process Discretization Example ---\")\n",
    "print(f\"For delta_t = {delta_t}, theta_sq = {theta_sq}:\")\n",
    "print(f\"Discrete A matrix: {A_wp}\")\n",
    "print(f\"Discrete Q matrix: {Q_wp}\")\n",
    "\n",
    "# Simulate a Wiener process using the discrete model and Kalman filter\n",
    "T_sim = 200\n",
    "H_sim = jnp.array([[1.0]])  # Direct observation\n",
    "R_std_sim = 0.2  # Observation noise\n",
    "R_sim = jnp.array([[R_std_sim**2]])\n",
    "m0_sim = jnp.array([0.0])\n",
    "P0_std_sim = 0.1\n",
    "P0_sim = jnp.array([[P0_std_sim**2]])\n",
    "\n",
    "key_sim = jax.random.PRNGKey(100)\n",
    "true_states_sim, observations_sim, _, _, _, _, _, _ = generate_linear_gaussian_data(\n",
    "    T=T_sim,\n",
    "    state_dim=1,\n",
    "    obs_dim=1,\n",
    "    A=A_wp,\n",
    "    Q_std=jnp.sqrt(Q_wp[0, 0]),\n",
    "    H=H_sim,\n",
    "    R_std=R_std_sim,\n",
    "    m0=m0_sim,\n",
    "    P0_std=P0_std_sim,\n",
    "    key=key_sim,\n",
    ")\n",
    "\n",
    "filtered_means_sim, filtered_covs_sim, _, _ = kalman_filter(\n",
    "    observations_sim, A_wp, Q_wp, H_sim, R_sim, m0_sim, P0_sim\n",
    ")\n",
    "filtered_stds_sim = jnp.sqrt(jnp.diagonal(filtered_covs_sim, axis1=1, axis2=2))\n",
    "\n",
    "fig_wiener = plot_time_series_results(\n",
    "    true_states_sim,\n",
    "    observations_sim,\n",
    "    filtered_means_sim,\n",
    "    filtered_stds_sim,\n",
    "    title=\"Simulated Wiener Process with Kalman Filter\",\n",
    ")\n",
    "fig_wiener.update_layout(height=600, width=800)\n",
    "fig_wiener.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2381457c",
   "metadata": {},
   "source": [
    "#### 4. Parameter Learning in Gauss-Markov Models\n",
    "\n",
    "A crucial question is: Can we learn the parameters (hyperparameters) of a Gauss-Markov model, such as $A, Q, H, R$? (Slide 33)\n",
    "\n",
    "Similar to learning kernel parameters in GPs (recap from Lecture 10, Slide 34), we can use **Bayesian Hierarchical Inference**. The key is to optimize the **model evidence (marginal likelihood)** $p(y | \\theta)$, where $\\theta$ represents all the unknown model parameters.\n",
    "\n",
    "For Gauss-Markov Models, the evidence can be computed efficiently in $\\mathcal{O}(N)$ time using the **Prediction Error Decomposition** (Slides 35-37):\n",
    "\n",
    "$$p(y | \\theta) = \\prod_{i=0}^N p(y_i | y_{0:i-1}, \\theta)$$\n",
    "\n",
    "Each term $p(y_i | y_{0:i-1}, \\theta)$ is the likelihood of the current observation given all previous observations, which can be computed directly from the Kalman filter's innovation residual $z_i$ and innovation covariance $S_i$:\n",
    "\n",
    "$$p(y_i | y_{0:i-1}, \\theta) = \\mathcal{N}(y_i; H m_i^-, H P_i^- H^T + R) = \\mathcal{N}(z_i; 0, S_i)$$\n",
    "\n",
    "Therefore, the log evidence is:\n",
    "\n",
    "$$\\log p(y | \\theta) = -\\frac{1}{2} \\sum_{i=1}^N \\left( z_i^T S_i^{-1} z_i + \\log |S_i| + \\log 2\\pi \\right)$$\n",
    "\n",
    "This objective function can be optimized using gradient-based methods (e.g., L-BFGS) to find the maximum likelihood estimates of the model parameters. The gradients of the log-evidence with respect to the parameters can be computed using automatic differentiation, making this a powerful approach for learning in linear Gaussian state-space models.\n",
    "\n",
    "While we won't implement a full parameter learning loop here (as it requires careful handling of parameter constraints and optimization), understanding this formulation is crucial. The Kalman filter naturally provides all the components needed to compute this likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c763953",
   "metadata": {},
   "source": [
    "#### 5. Generalization: What if the World Isn't Linear Gaussian? (Hidden Markov Models)\n",
    "\n",
    "The real world is often not perfectly linear or Gaussian. What happens then? This leads us to **Hidden Markov Models (HMMs)** in a broader sense, and various approximate Bayesian filtering and smoothing algorithms (Slides 38-39):\n",
    "\n",
    "| System Type              | State Transition $p(x_t \\mid x_{t-1})$                | Observation $p(y_t \\mid x_t)$                | Algorithm                                         |\n",
    "| :----------------------- | :---------------------------------------------------- | :------------------------------------------- | :------------------------------------------------ |\n",
    "| **Markovian System**     | General                                               | General                                      | General Bayesian filtering and smoothing           |\n",
    "| **Linear Gaussian**      | $\\mathcal{N}(x_t; A x_{t-1}, Q)$                      | $\\mathcal{N}(y_t; H x_t, R)$                 | Kalman filter, RTS smoother                       |\n",
    "| **Nonlinear Gaussian**   | $\\mathcal{N}(x_t; a(x_{t-1}), Q)$                     | $\\mathcal{N}(y_t; h(x_t), R)$                | Extended/Unscented/Particle filter, etc.          |\n",
    "| **Non-Gaussian Obs.**   | $\\mathcal{N}(x_t; A x_{t-1}, Q)$                      | $p(y_t \\mid h(x_t))$                         | (Requires approximations)                         |\n",
    "| **Hidden Markov Model**  | $p(x_t = \\Pi x_{t-1})$ (discrete)                     | $\\mathcal{N}(y_t; h(x_t), R)$                | Viterbi, Forward-Backward, Baum-Welch             |\n",
    "\n",
    "---\n",
    "\n",
    "For continuous systems with nonlinear dynamics and/or nonlinear observations, a number of **approximately Gaussian filters** have been developed:\n",
    "\n",
    "- **Extended Kalman Filter (EKF):**  \n",
    "    Linearizes the nonlinear functions around the current mean estimate using a Taylor expansion. This is an approximation, but often works well in practice.\n",
    "\n",
    "- **Unscented Kalman Filter (UKF):**  \n",
    "    Uses a deterministic sampling approach (the unscented transform) to propagate mean and covariance through the nonlinearities, avoiding explicit Jacobians.\n",
    "\n",
    "- **Particle Filter:**  \n",
    "    A non-parametric approach that represents the posterior distribution with a set of weighted particles. This method is suitable for highly nonlinear or non-Gaussian systems.\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:**  \n",
    "> These methods are beyond the scope of this introductory course, but are crucial for real-world applications where linearity and Gaussianity assumptions do not hold.  \n",
    "> Time series analysis is a vast field, and these advanced filters form its backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3bad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Illustrating Non-Linear System (Conceptual) ---\n",
    "\n",
    "\n",
    "def generate_nonlinear_data(T=100, noise_std=0.1, key=None):\n",
    "    \"\"\"Generates synthetic data from a simple nonlinear state-space model.\"\"\"\n",
    "    if key is None:\n",
    "        key = jax.random.PRNGKey(0)\n",
    "\n",
    "    states = [jnp.array([0.1])]\n",
    "    observations = []\n",
    "\n",
    "    for t in range(T):\n",
    "        key, subkey_state, subkey_obs = jax.random.split(key, 3)\n",
    "        # Nonlinear state transition: x_t = 0.5 * x_{t-1} + 25 * x_{t-1} / (1 + x_{t-1}^2) + 8 * cos(1.2 * t) + w_t\n",
    "        # This is a common benchmark system (from Särkkä's book)\n",
    "        nonlinear_term = (\n",
    "            0.5 * states[-1]\n",
    "            + 25 * states[-1] / (1 + states[-1] ** 2)\n",
    "            + 8 * jnp.cos(1.2 * t)\n",
    "        )\n",
    "        state_noise = jax.random.normal(subkey_state, (1,)) * noise_std * 2\n",
    "        next_state = nonlinear_term + state_noise\n",
    "        states.append(next_state)\n",
    "\n",
    "        # Nonlinear observation: y_t = x_t**2 / 20 + v_t\n",
    "        obs_noise = jax.random.normal(subkey_obs, (1,)) * noise_std\n",
    "        observation = states[-1] ** 2 / 20 + obs_noise\n",
    "        observations.append(observation)\n",
    "\n",
    "    return jnp.array(states[1:]), jnp.array(observations)\n",
    "\n",
    "\n",
    "# Generate nonlinear data\n",
    "T_nl = 100\n",
    "key_nl = jax.random.PRNGKey(200)\n",
    "true_states_nl, observations_nl = generate_nonlinear_data(\n",
    "    T=T_nl, noise_std=0.5, key=key_nl\n",
    ")\n",
    "\n",
    "print(\"\\n--- Simulating a Nonlinear System ---\")\n",
    "print(f\"Generated {T_nl} time steps of nonlinear data.\")\n",
    "\n",
    "# Attempt to filter with a *linear* Kalman Filter (will perform poorly)\n",
    "# We need to define linear A, Q, H, R for the Kalman filter, which won't match the true nonlinear system.\n",
    "# Let's use some 'best guess' linear parameters.\n",
    "A_linear_approx = jnp.array([[1.0]])  # Assume near constant velocity for small delta_t\n",
    "Q_linear_approx = jnp.array(\n",
    "    [[1.0]]\n",
    ")  # Large process noise to try to capture nonlinearity\n",
    "H_linear_approx = jnp.array([[0.1]])  # Simple linear observation approx\n",
    "R_linear_approx = jnp.array([[0.5**2]])  # Observation noise\n",
    "m0_linear_approx = jnp.array([0.0])\n",
    "P0_linear_approx = jnp.array([[1.0]])\n",
    "\n",
    "filtered_means_nl, filtered_covs_nl, _, _ = kalman_filter(\n",
    "    observations_nl,\n",
    "    A_linear_approx,\n",
    "    Q_linear_approx,\n",
    "    H_linear_approx,\n",
    "    R_linear_approx,\n",
    "    m0_linear_approx,\n",
    "    P0_linear_approx,\n",
    ")\n",
    "filtered_stds_nl = jnp.sqrt(jnp.diagonal(filtered_covs_nl, axis1=1, axis2=2))\n",
    "\n",
    "fig_nonlinear_fail = plot_time_series_results(\n",
    "    true_states_nl,\n",
    "    observations_nl,\n",
    "    filtered_means_nl,\n",
    "    filtered_stds_nl,\n",
    "    title=\"Nonlinear System: Kalman Filter (Linear Assumption) Performance\",\n",
    ")\n",
    "fig_nonlinear_fail.update_layout(height=600, width=800)\n",
    "fig_nonlinear_fail.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946580a",
   "metadata": {},
   "source": [
    "As expected, the linear Kalman Filter struggles to accurately track the highly nonlinear true state. Its estimates deviate significantly, and while its uncertainty grows, it doesn't fully capture the complex dynamics. This illustrates the need for more advanced, approximate non-linear filters (like EKF, UKF, or Particle Filters) when dealing with such systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6800a5ac",
   "metadata": {},
   "source": [
    "#### 6. Summary\n",
    "\n",
    "To summarize (Slide 40):\n",
    "\n",
    "* **Markov Chains** capture finite memory of a time series through conditional independence.\n",
    "* **Gauss-Markov models** map this state to linear algebra.\n",
    "* The **Kalman filter** is the name for the corresponding algorithm.\n",
    "* **SDEs (Stochastic Differential Equations)** are the continuous-time limit of discrete-time stochastic recurrence relations, providing a powerful theoretical framework for continuous-time Gauss-Markov systems.\n",
    "* Parameters of the model can be learnt by optimizing the (log) evidence, which is also $\\mathcal{O}(N)$.\n",
    "* **Non-Gaussian models** can be learnt by approximate inference, analogous to GP models, using methods like Extended/Unscented Kalman Filters or Particle Filters.\n",
    "\n",
    "Gauss-Markov models form the algorithmic scaffold for time-series models, and understanding their extensions to non-linear and non-Gaussian scenarios is key to applying probabilistic machine learning to real-world dynamic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f4885",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "**Exercise 1: Discretize Ornstein-Uhlenbeck Process**\n",
    "Based on Slide 29, implement a `discretize_ornstein_uhlenbeck_process(delta_t, theta_sq, lambda_param)` function that returns the discrete $A_i$ and $Q_i$ matrices for the Ornstein-Uhlenbeck process. Then, use this in the simulation and Kalman filter, similar to the Wiener process example.\n",
    "\n",
    "**Exercise 2: Visualize the Effect of Discretization Time Step**\n",
    "In the Wiener process simulation (Section 3), vary `delta_t` (e.g., 1.0, 0.5, 0.05). How does the smoothness of the true state and the performance of the Kalman filter change as `delta_t` decreases? Discuss the trade-off between approximation accuracy and computational cost.\n",
    "\n",
    "**Exercise 3: Conceptual Parameter Learning**\n",
    "Consider the `log_evidence` formula for Gauss-Markov models. If you wanted to learn the `Q_std` (state noise standard deviation) for a simple 1D linear model, conceptually describe the optimization loop you would set up. What JAX functionalities would you use (e.g., `jax.value_and_grad`)? (No need to implement, just describe the steps).\n",
    "\n",
    "**Exercise 4 (Advanced): Implement a Simple EKF Step (Conceptual)**\n",
    "Take the `generate_nonlinear_data` function. Conceptually, how would you modify the `kalman_filter` to become an `extended_kalman_filter`? Specifically, describe how you would compute the linearized `A` and `H` matrices at each time step using `jax.jacobian` around the current state estimate. What challenges might arise?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
