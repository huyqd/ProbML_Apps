{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "colab": {
            "provenance": [],
            "authorship_tag": "ABX9TyP5G65t64m+14+14+14+14",
            "include_colab_link": true
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro_shift"
            },
            "source": [
                "# Conformal Prediction Under Distribution Shift: Beyond I.I.D. Assumptions\n",
                "\n",
                "## Introduction\n",
                "\n",
                "In the previous lecture, we explored the foundations of Conformal Prediction, which provides powerful finite-sample coverage guarantees for prediction sets under the assumption that data is Independent and Identically Distributed (I.I.D.). While the I.I.D. assumption simplifies theoretical analysis and is often a starting point in machine learning, real-world data rarely adheres to it perfectly. Data distributions can *shift* over time or across different collection environments, posing significant challenges for uncertainty quantification.\n",
                "\n",
                "This notebook delves into advanced conformal methods designed to handle **distribution shifts**, focusing on scenarios where the underlying data generating process changes between training and testing. We will cover:\n",
                "\n",
                "* **Likelihood-Weighted Conformal Prediction**: A primary method for handling *covariate shift*, where the distribution of features ($X$) changes, but the conditional distribution of the response given features ($Y|X$) remains constant.\n",
                "* **Estimating Likelihood Ratios**: Practical techniques to estimate the necessary weights for likelihood-weighted conformal prediction using unlabeled data.\n",
                "* **Conformal Prediction for Structured-X Settings**: A more general theoretical framework for complex feature dependencies.\n",
                "* **Custom-Weighted Conformal Prediction**: A flexible approach that allows for arbitrary, fixed weights to prioritize certain data points.\n",
                "* **Adaptive Conformal Inference (ACI)**: An online method for sequential prediction that continuously adjusts to distribution changes over time.\n",
                "\n",
                "Understanding these extensions is crucial for deploying reliable machine learning models in dynamic environments. As before, we will use `JAX` and `scikit-learn` for practical code examples to illustrate these concepts."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "likelihood_weighted_cp"
            },
            "source": [
                "## 1. Likelihood-Weighted Conformal Prediction\n",
                "\n",
                "A common and important type of distribution shift is **covariate shift**. In this scenario, we assume:\n",
                "\n",
                "* Training data: $(X_i, Y_i) \\sim P = P_X \\times P_{Y|X}$ independently, for $i=1, \\ldots, n$.\n",
                "* Test data: $(X_{n+1}, Y_{n+1}) \\sim \\tilde{P} = \\tilde{P}_X \\times P_{Y|X}$ independently.\n",
                "\n",
                "Here, the conditional distribution of $Y|X$ (the relationship between features and response) is assumed to be the same for both training and test data, but the distribution of features $X$ is allowed to change, i.e., $\\tilde{P}_X \\ne P_X$.\n",
                "\n",
                "### Why Standard Conformal Prediction Fails Under Covariate Shift\n",
                "\n",
                "As empirically shown in Tibshirani et al. (2019) (Figure 1 in the original slides), if you apply standard split conformal prediction when covariate shift is present, the coverage guarantee often fails. The prediction intervals tend to under-cover. This happens because the calibration set, drawn from $P_X$, might not be representative of the test point, which is drawn from $\\tilde{P}_X$. The notion of exchangeability, crucial for the rank-based quantile argument, is broken when the training and test sets come from different feature distributions.\n",
                "\n",
                "To remedy this, we need to adapt the empirical distribution of conformity scores using **weights** that account for the shift. These weights are based on the likelihood ratio between the test and training feature distributions.\n",
                "\n",
                "### Revisiting Exchangeability and Quantiles (with Weights)\n",
                "\n",
                "Recall that the core of conformal prediction relies on the rank-based argument stemming from exchangeability. We need to generalize this to a \"weighted exchangeability\" setting. The key idea is that even if the random variables $R_1, \\ldots, R_{n+1}$ are not perfectly exchangeable, their probabilities can be related to a permutation-invariant function $g$ and individual weight functions $w_i(r_i)$.\n",
                "\n",
                "A sequence of random variables $R_1, \\ldots, R_{n+1}$ is **weighted exchangeable** if their joint density (or mass function) can be written as:\n",
                "\n",
                "$$f(r_1, \\ldots, r_{n+1}) = \\prod_{i=1}^{n+1} w_i(r_i) \\cdot g(r_1, \\ldots, r_{n+1})$$\n",
                "\n",
                "where $g$ is any permutation-invariant function. In this case, the probability that $R_{n+1}$ takes a specific value $r_i$ from the observed set $\\{r_1, \\\\ldots, r_{n+1}\\}$ (conditioned on observing this set) is not $1/(n+1)$ anymore, but rather:\n",
                "\n",
                "$$\\mathbb{P}(R_{n+1}=r_i | \\{R_1, \\ldots, R_{n+1}\\} = \\{r_1, \\ldots, r_{n+1}\\}) = \\frac{w_i(r_i)}{\\sum_{j=1}^{n+1} w_j(r_j)}$$\n",
                "\n",
                "This leads to **Lemma 1 (Quantile Lemma)** from the slides, which states that for weighted exchangeable random variables $Z_i$ with weights $w_1, \\ldots, w_{n+1}$, and a symmetric score function $V$, the conformal prediction set for $R_{n+1}$ (test score) can be formed using a weighted empirical distribution of scores:\n",
                "\n",
                "$$\\mathbb{P}\\left\\{R_{n+1}\\le \\text{Quantile}\\left(1-\\alpha;\\sum_{i=1}^{n}p_{i}^{w}(Z_{1},...,Z_{n+1})\\delta_{R_{i}}+p_{n+1}^{w}(Z_{1},...,Z_{n+1})\\delta_{\\infty}\\right)\\right\\}\\ge1-\\alpha.$$\n",
                "\n",
                "where $p_i^w$ are the normalized weights that sum to 1.\n",
                "\n",
                "### Application to Covariate Shift (Corollary 1)\n",
                "\n",
                "For the covariate shift problem, the weights $w_i(r_i)$ simplify considerably. If $w(x) = d\\tilde{P}_X(x) / dP_X(x)$ is the likelihood ratio (density ratio) between the test and training feature distributions, then the weights for the *training* points are effectively $1$ (as they come from $P_X$), and the weight for the *test* point is $w(X_{n+1})$.\n",
                "\n",
                "This leads to **Corollary 1** (Tibshirani et al., 2019), which defines the weighted conformal set as:\n",
                "\n",
                "$$\\hat{C}_{n}^{w}(x)=\\left\\{y:R_{n+1}^{(x,y)}\\le \\text{Quantile}\\left(1-\\alpha;\\sum_{i=1}^{n}\\pi_{i}^{w}(x)\\delta_{R_{i}^{(x,y)}}+\\pi_{n+1}^{w}(x)\\delta_{\\infty}\\right)\\right\\}$$\n",
                "\n",
                "where $R_i^{(x,y)}$ are conformity scores (e.g., absolute residuals) based on a base predictor, and the normalized weights $\\pi_i^w(x)$ are given by:\n",
                "\n",
                "$$\\pi_{i}^{w}(x)=\\frac{w(X_{i})}{\\sum_{j=1}^{n}w(X_{j})+w(x)},\\quad i=1,\\ldots,n$$\n",
                "$$\\pi_{n+1}^{w}(x)=\\frac{w(x)}{\\sum_{j=1}^{n}w(X_{j})+w(x)}$$\n",
                "\n",
                "This weighted conformal set guarantees $\\mathbb{P}(Y_{n+1}\\in\\hat{C}_{n}^{w}(X_{n+1}))\\ge1-\\alpha$. In a split conformal setup, this is conditional on the proper training set $D_1$.\n",
                "\n",
                "### Impact on Coverage and Length\n",
                "\n",
                "The middle row of Figure 1 in the original slides demonstrates that using these oracle weights restores coverage in the presence of covariate shift. However, the dispersion (variability) in coverage can be larger, and the prediction intervals tend to be wider, compared to standard conformal prediction without any shift but with an equivalent effective sample size. This is because non-uniform weights effectively reduce the \"effective\" number of calibration samples.\n",
                "\n",
                "### Code Example: Likelihood-Weighted Split Conformal Prediction\n",
                "\n",
                "This example simulates a covariate shift scenario and demonstrates how likelihood weighting can restore coverage. We will simulate data from two different $P_X$ distributions (training and test) but with the same $P_{Y|X}$. We will then estimate the likelihood ratio $w(x)$ using a logistic regression classifier trained to distinguish between the two $X$ distributions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "likelihood_weighted_cp_code"
            },
            "outputs": [],
            "source": [
                "import jax.numpy as jnp\n",
                "import jax.random as random\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Set a random seed for reproducibility\n",
                "key = random.PRNGKey(0)\n",
                "\n",
                "# --- 1. Simulate Data with Covariate Shift ---\n",
                "num_samples_train = 500\n",
                "num_samples_test = 500\n",
                "noise_std = 5.0\n",
                "\n",
                "# Training data (P_X)\n",
                "key, subkey = random.split(key)\n",
                "X_train_P = random.normal(subkey, (num_samples_train, 1)) * 2 # Broader distribution\n",
                "y_train_P = 2 * X_train_P.squeeze() + 3 + random.normal(key, (num_samples_train,)) * noise_std\n",
                "\n",
                "# Test data (tilde_P_X) - Shifted distribution\n",
                "key, subkey = random.split(key)\n",
                "X_test_tildeP = random.normal(subkey, (num_samples_test, 1)) * 0.5 + 2 # Narrower, shifted distribution\n",
                "y_test_tildeP = 2 * X_test_tildeP.squeeze() + 3 + random.normal(key, (num_samples_test,)) * noise_std\n",
                "\n",
                "print(f\"Training data shape: {X_train_P.shape}, {y_train_P.shape}\")\n",
                "print(f\"Test data shape: {X_test_tildeP.shape}, {y_test_tildeP.shape}\\n\")\n",
                "\n",
                "# --- 2. Split Training Data into Proper Training (D1) and Calibration (D2) ---\n",
                "X_D1, X_D2, y_D1, y_D2 = train_test_split(\n",
                "    X_train_P, y_train_P, test_size=0.5, random_state=key.get(0).tolist()[0]\n",
                ")\n",
                "n1 = X_D1.shape[0]\n",
                "n2 = X_D2.shape[0]\n",
                "print(f\"Proper training set size (n1): {n1}\")\n",
                "print(f\"Calibration set size (n2): {n2}\\n\")\n",
                "\n",
                "# --- 3. Train Base Predictor on Proper Training Set (D1) ---\n",
                "base_predictor = LinearRegression()\n",
                "base_predictor.fit(np.array(X_D1), np.array(y_D1))\n",
                "\n",
                "# --- 4. Estimate Likelihood Ratio (w(x)) ---\n",
                "# Combine X from P_X (training) and tilde_P_X (test) and assign labels 0 and 1\n",
                "X_combined = jnp.vstack([X_D2, X_test_tildeP]) # Use D2 for training classifier, as D2 is from P_X\n",
                "labels = jnp.concatenate([jnp.zeros(X_D2.shape[0]), jnp.ones(X_test_tildeP.shape[0])])\n",
                "\n",
                "# Scale X for logistic regression stability\n",
                "scaler = StandardScaler()\n",
                "X_combined_scaled = scaler.fit_transform(np.array(X_combined))\n",
                "X_test_tildeP_scaled = scaler.transform(np.array(X_test_tildeP))\n",
                "X_D2_scaled = scaler.transform(np.array(X_D2))\n",
                "\n",
                "# Train a logistic regression classifier to predict if X comes from P_X (0) or tilde_P_X (1)\n",
                "prob_classifier = LogisticRegression(random_state=key.get(1).tolist()[0], solver='lbfgs')\n",
                "prob_classifier.fit(X_combined_scaled, np.array(labels))\n",
                "\n",
                "# Predict P(C=1|X=x) for calibration set (D2) and test set\n",
                "p_D2 = prob_classifier.predict_proba(X_D2_scaled)[:, 1] # Probability of being from tilde_P_X\n",
                "p_test = prob_classifier.predict_proba(X_test_tildeP_scaled)[:, 1]\n",
                "\n",
                "# Estimate weights w(x) = p(C=1|x) / p(C=0|x) = p(C=1|x) / (1-p(C=1|x))\n",
                "# w(x) = d(tilde_P_X) / d(P_X)\n",
                "# Add small epsilon to avoid division by zero or inf\n",
                "epsilon = 1e-6\n",
                "weights_D2 = (p_D2 + epsilon) / (1 - p_D2 + epsilon)\n",
                "weights_test = (p_test + epsilon) / (1 - p_test + epsilon)\n",
                "\n",
                "print(f\"Estimated weights for calibration set (first 5): {weights_D2[:5]}\")\n",
                "print(f\"Estimated weights for test set (first 5): {weights_test[:5]}\\n\")\n",
                "\n",
                "# --- 5. Compute Calibration Scores (Absolute Residuals) on D2 ---\n",
                "y_D2_pred = base_predictor.predict(np.array(X_D2))\n",
                "calibration_residuals = jnp.abs(y_D2 - jnp.array(y_D2_pred))\n",
                "\n",
                "# --- 6. Compute Weighted Conformal Quantile ---\n",
                "alpha_level = 0.1 # Desired coverage: 1 - alpha = 0.9 (90%)\n",
                "\n",
                "# The weighted scores are $R_i / w(X_i)$ or equivalent ways to incorporate weights\n",
                "# According to the slide, we use the quantile of a weighted empirical distribution.\n",
                "# This means sorting by residual, and accumulating weights.\n",
                "\n",
                "# Sort calibration residuals and get their corresponding weights\n",
                "sorted_indices = jnp.argsort(calibration_residuals)\n",
                "sorted_residuals = calibration_residuals[sorted_indices]\n",
                "sorted_weights = weights_D2[sorted_indices]\n",
                "\n",
                "# Compute the weighted empirical CDF\n",
                "cumulative_weights = jnp.cumsum(sorted_weights)\n",
                "\n",
                "# We need to find the residual `r` such that its cumulative weight is >= (1-alpha) * (total weights + weight of test point)\n",
                "# The total sum of weights (including a hypothetical test point with weight 1 for the original formula)\n",
                "# The formula uses pi_i^w which normalizes these weights.\n",
                "# The core idea is to find the (1-alpha) quantile of the distribution formed by (residual, weight) pairs.\n",
                "# We can use a trick: for each test point, combine its score with calibration scores,\n",
                "# apply weights, and find its rank. This is essentially what the quantile form does.\n",
                "\n",
                "# For split conformal, we use the calibration set. We are essentially finding a quantile of the\n",
                "# distribution where each calibration point has weight w(X_i) / sum(w(X_j)).\n",
                "# The simplest practical way is to find the (1-alpha) * (sum of weights + test weight) percentile.\n",
                "# Let's find the threshold 'tau' such that the sum of weights for residuals <= tau is >= (1-alpha) * sum(weights)\n",
                "\n",
                "total_calibration_weight = jnp.sum(weights_D2)\n",
                "target_weight_quantile = (1 - alpha_level) * (total_calibration_weight + 1) # +1 for test point in original formula\n",
                "\n",
                "# Find the smallest residual whose cumulative weight meets the target\n",
                "weighted_conformal_quantile_idx = jnp.searchsorted(cumulative_weights, target_weight_quantile)\n",
                "\n",
                "conformal_quantile_weighted = sorted_residuals[weighted_conformal_quantile_idx]\n",
                "\n",
                "print(f\"Weighted conformal quantile: {conformal_quantile_weighted:.4f}\\n\")\n",
                "\n",
                "# --- 7. Form Conformal Prediction Band ---\n",
                "# Prediction on the test set (tilde_P_X)\n",
                "y_test_pred_mean = base_predictor.predict(np.array(X_test_tildeP))\n",
                "\n",
                "# The prediction intervals are formed using the *same* quantile, but still centered at the mean prediction.\n",
                "# [mean - quantile, mean + quantile]\n",
                "lower_bound_weighted = y_test_pred_mean - conformal_quantile_weighted\n",
                "upper_bound_weighted = y_test_pred_mean + conformal_quantile_weighted\n",
                "\n",
                "# --- Plotting Test Data and Prediction Band ---\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.scatter(X_train_P[:, 0], y_train_P, color='blue', label='Training Data (P_X)', alpha=0.5, s=20)\n",
                "plt.scatter(X_test_tildeP[:, 0], y_test_tildeP, color='orange', label='Test Data ($\\tilde{P}_X$)', alpha=0.5, s=20)\n",
                "plt.plot(X_test_tildeP[jnp.argsort(X_test_tildeP.squeeze()), 0], y_test_pred_mean[jnp.argsort(X_test_tildeP.squeeze())], color='red', linewidth=2, label='Regression Mean')\n",
                "\n",
                "# Plot the weighted conformal prediction band for the test data range\n",
                "plt.fill_between(\n",
                "    X_test_tildeP[jnp.argsort(X_test_tildeP.squeeze()), 0].squeeze(),\n",
                "    lower_bound_weighted[jnp.argsort(X_test_tildeP.squeeze())],\n",
                "    upper_bound_weighted[jnp.argsort(X_test_tildeP.squeeze())],\n",
                "    color='red', alpha=0.2, label=f'Weighted Conformal Band (1-$\\alpha$={1-alpha_level})'\n",
                ")\n",
                "\n",
                "plt.xlabel('X')\n",
                "plt.ylabel('Y')\n",
                "plt.title('Likelihood-Weighted Conformal Prediction Under Covariate Shift')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "# --- Simulate Test Coverage to Verify Guarantee ---\n",
                "coverage_count_weighted = 0\n",
                "for i in range(num_samples_test):\n",
                "    if (y_test_tildeP[i] >= lower_bound_weighted[i]) and \\\n",
                "       (y_test_tildeP[i] <= upper_bound_weighted[i]):\n",
                "        coverage_count_weighted += 1\n",
                "\n",
                "simulated_coverage_weighted = coverage_count_weighted / num_samples_test\n",
                "print(f\"Simulated weighted conformal coverage on test set: {simulated_coverage_weighted:.4f}\")\n",
                "print(f\"Desired coverage (at least): {1 - alpha_level:.4f}\")\n",
                "print(f\"Theoretical upper bound (approx): {1 - alpha_level + 1/(n2 + 1):.4f}\\n\")\n",
                "\n",
                "# --- Compare with Naive Conformal (without weighting) ---\n",
                "# This simulates standard split conformal prediction under covariate shift\n",
                "# It should show undercoverage.\n",
                "\n",
                "naive_conformal_quantile_idx = jnp.ceil((1 - alpha_level) * (n2 + 1)).astype(int)\n",
                "naive_conformal_quantile = jnp.sort(calibration_residuals)[naive_conformal_quantile_idx - 1]\n",
                "\n",
                "lower_bound_naive = y_test_pred_mean - naive_conformal_quantile\n",
                "upper_bound_naive = y_test_pred_mean + naive_conformal_quantile\n",
                "\n",
                "coverage_count_naive = 0\n",
                "for i in range(num_samples_test):\n",
                "    if (y_test_tildeP[i] >= lower_bound_naive[i]) and \\\n",
                "       (y_test_tildeP[i] <= upper_bound_naive[i]):\n",
                "        coverage_count_naive += 1\n",
                "\n",
                "simulated_coverage_naive = coverage_count_naive / num_samples_test\n",
                "print(f\"Simulated NAIVE conformal coverage on test set (should undercover): {simulated_coverage_naive:.4f}\")\n",
                "print(f\"Desired coverage: {1 - alpha_level:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "estimating_likelihood_ratio"
            },
            "source": [
                "## 2. Estimating the Likelihood Ratio from Unlabeled Data\n",
                "\n",
                "In real-world scenarios, the true likelihood ratio $w(x) = d\\tilde{P}_X(x) / dP_X(x)$ is typically unknown. However, if we have access to unlabeled data $X_{n+1}, \\ldots, X_{n+m}$ from the test distribution $\\tilde{P}_X$ (along with the training data $X_1, \\ldots, X_n$ from $P_X$), we can estimate this ratio.\n",
                "\n",
                "The core idea is to frame the problem as a **binary classification task**:\n",
                "\n",
                "1.  Create a combined dataset of features: $X_{\\text{combined}} = (X_1, \\ldots, X_n, X_{n+1}, \\ldots, X_{n+m})$.\n",
                "2.  Assign binary labels to these features:\n",
                "    * $C_i = 0$ for $X_i$ coming from $P_X$ (i.e., for $i=1, \\ldots, n$).\n",
                "    * $C_i = 1$ for $X_i$ coming from $\\tilde{P}_X$ (i.e., for $i=n+1, \\ldots, n+m$).\n",
                "3.  Train a probabilistic classifier (e.g., Logistic Regression, Random Forest Classifier) on $(X_{\\text{combined}}, C_{\\text{labels}})$ to estimate $P(C=1|X=x)$. Let this estimate be $\\hat{p}(x)$.\n",
                "\n",
                "The crucial insight is that the likelihood ratio $w(x)$ is directly related to the conditional odds ratio of this binary classification problem:\n",
                "\n",
                "$$\\frac{P(C=1|X=x)}{P(C=0|X=x)} = \\frac{P(C=1)}{P(C=0)} \\frac{d\\tilde{P}_X(x)}{dP_X(x)}$$\n",
                "\n",
                "Since we only need the likelihood ratio up to a proportionality constant for the weighted conformal procedure, we can use the following as our estimated weight function:\n",
                "\n",
                "$$\\hat{w}(x) = \\frac{\\hat{p}(x)}{1 - \\hat{p}(x)}$$\n",
                "\n",
                "The better calibrated the classifier $\\hat{p}(x)$ is, the more accurate our estimated weights $\\hat{w}(x)$ will be. The bottom row of Figure 1 in the original slides demonstrates the effectiveness of this estimation strategy, showing restored coverage when using weights derived from Logistic Regression or Random Forests.\n",
                "\n",
                "The code example in the previous section (Likelihood-Weighted Split Conformal Prediction) already incorporates this estimation strategy."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "structured_x_settings"
            },
            "source": [
                "## 3. Conformal Prediction for Structured-X Settings\n",
                "\n",
                "Beyond simple covariate shift, we can consider even more general scenarios where the features themselves have a complex, dependent structure. This is captured by **Theorem 2** in the original slides, which considers data distributed according to:\n",
                "\n",
                "* Features: $(X_1, \\ldots, X_{n+1}) \\sim \\Lambda$, where $\\Lambda$ can be an arbitrary joint distribution (not necessarily i.i.d.).\n",
                "* Responses: $Y_i|X_i \\sim P_{Y|X}$, independently, for $i=1, \\ldots, n+1$.\n",
                "\n",
                "In this setting, the conformal set is defined using a generalized form of weights $p_i^\\lambda(x_1, \\ldots, x_{n+1})$ that account for the complex dependencies within the feature vector:\n",
                "\n",
                "$$\\hat{C}_{n}^{\\lambda}(x)=\\left\\{y:R_{n+1}^{(x,y)}\\le \\text{Quantile}\\left(1-\\alpha;\\sum_{i=1}^{n}p_{i}^{\\lambda}(X_{1},...,X_{n},x)\\delta_{R_{i}^{(x,y)}}+p_{n+1}^{\\lambda}(X_{1},...,X_{n},x)\\delta_{\\infty}\\right)\\right\\}$$\n",
                "\n",
                "where $R_i^{(x,y)}$ are conformity scores (as before), and $p_i^\\lambda$ are probabilities derived from the density function of $\\Lambda$ under permutations:\n",
                "\n",
                "$$p_{i}^{\\lambda}(x_{1},...,x_{n+1})=\\frac{\\sum_{\\sigma:\\sigma(n+1)=i}\\lambda(x_{\\sigma(1)},...,x_{\\sigma(n+1)})}{\\sum_{\\sigma}\\lambda(x_{\\sigma(1)},...,x_{\\sigma(n+1)})}, \\quad i=1,\\ldots,n+1$$\n",
                "\n",
                "This theorem provides a very general guarantee of $\\mathbb{P}(Y_{n+1}\\in\\hat{C}_{n}^{\\lambda}(X_{n+1}))\\ge1-\\alpha$. However, computing these weights $p_i^\\lambda$ can be extraordinarily difficult due to the combinatorial sums over permutations, making this approach computationally intractable for most real-world scenarios unless $\\Lambda$ has a very specific, easily factorizable structure (e.g., Markov property for time series). This method is primarily of theoretical interest, demonstrating the extensibility of conformal principles.\n",
                "\n",
                "### Code Example: Conformal Prediction for Structured-X Settings (Conceptual)\n",
                "\n",
                "This example illustrates the theoretical concept of calculating $p_i^\\lambda$ for a very small `n`. It defines a simple $\\Lambda$ (joint distribution of features) where order matters. **Warning**: This implementation is purely conceptual and will not scale to larger `n` due to the factorial complexity of permutations. It serves to show the *logic* behind the weight calculation, not a practical implementation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "structured_x_settings_code"
            },
            "outputs": [],
            "source": [
                "import jax.numpy as jnp\n",
                "import jax.random as random\n",
                "import numpy as np\n",
                "import itertools # For generating permutations\n",
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "# Set a random seed for reproducibility\n",
                "key = random.PRNGKey(0)\n",
                "\n",
                "# --- 1. Simulate Very Small Structured Data ---\n",
                "# For conceptual illustration, let's use n=2 training points.\n",
                "# We'll have (X1, Y1), (X2, Y2) as training, and (x_test, y_test) as the query point.\n",
                "# So total N+1 = 3 points.\n",
                "\n",
                "num_training_points = 2 # n\n",
                "noise_std = 0.5\n",
                "\n",
                "# Define a simple sequential dependency for Lambda (e.g., Markov-like)\n",
                "# Lambda will be P(X1) * P(X2|X1) * P(X3|X2)\n",
                "\n",
                "def P_X_initial(x): # Probability of the first X\n",
                "    return jnp.exp(-((x - 0.0)**2) / (2 * 1.0**2)) / jnp.sqrt(2 * jnp.pi * 1.0**2)\n",
                "\n",
                "def P_X_conditional(x_curr, x_prev): # P(X_curr | X_prev)\n",
                "    # Simple dependency: X_curr is drawn from a normal distribution centered at X_prev\n",
                "    return jnp.exp(-((x_curr - x_prev)**2) / (2 * 0.5**2)) / jnp.sqrt(2 * jnp.pi * 0.5**2)\n",
                "\n",
                "def Lambda_density(x_sequence): # Lambda(x1, x2, ..., x_N+1)\n",
                "    # Assumes x_sequence is (x_1, x_2, ..., x_{n+1})\n",
                "    if len(x_sequence) == 0: return 1.0\n",
                "    if len(x_sequence) == 1: return P_X_initial(x_sequence[0])\n",
                "    \n",
                "    density = P_X_initial(x_sequence[0])\n",
                "    for i in range(1, len(x_sequence)):\n",
                "        density *= P_X_conditional(x_sequence[i], x_sequence[i-1])\n",
                "    return density\n",
                "\n",
                "# Simulate training X values (these are fixed for the calculation)\n",
                "X_train_values_fixed = jnp.array([1.0, 2.0]) # Our X1, X2\n",
                "y_train_values = 2 * X_train_values_fixed + random.normal(key, (num_training_points,)) * noise_std\n",
                "\n",
                "X_test_point_fixed = jnp.array([1.5]) # Our x_test\n",
                "y_test_point_true = 2 * X_test_point_fixed[0] + random.normal(key, (1,)) * noise_std # True y for test point\n",
                "\n",
                "print(f\"Training X values: {X_train_values_fixed}\")\n",
                "print(f\"Test X point: {X_test_point_fixed.item()}\\n\")\n",
                "\n",
                "# --- 2. Full Conformal Setup for a Query Point (x_test, y_candidate) ---\n",
                "# For simplicity, we'll try to find if a specific y_candidate is in the set for x_test\n",
                "y_candidate_example = jnp.array([4.5]) # A hypothetical y value for x_test\n",
                "\n",
                "# Combined sequence of X values for the permutation sums\n",
                "X_all_points = jnp.concatenate([X_train_values_fixed, X_test_point_fixed]).tolist()\n",
                "n_plus_1 = len(X_all_points)\n",
                "\n",
                "print(f\"All X points (fixed): {X_all_points}\")\n",
                "print(f\"Total points (n+1): {n_plus_1}\\n\")\n",
                "\n",
                "def calculate_p_lambda(x_values_sequence_fixed, query_x_idx_in_sequence):\n",
                "    \"\"\"\n",
                "    Calculates p_i^lambda weights for the structured-X setting.\n",
                "    x_values_sequence_fixed: List of fixed X values (x1, ..., xn, x_test).\n",
                "    query_x_idx_in_sequence: The 0-indexed position of the test point x_test in x_values_sequence_fixed.\n",
                "    \n",
                "    NOTE: This is computationally expensive, as it iterates through (n+1)! permutations.\n",
                "    Only for very small 'n'.\n",
                "    \"\"\"\n",
                "    n_plus_1 = len(x_values_sequence_fixed)\n",
                "    all_indices = list(range(n_plus_1))\n",
                "    \n",
                "    denominator_sum = 0.0\n",
                "    numerator_sums = jnp.zeros(n_plus_1) # For each p_i^lambda\n",
                "\n",
                "    # Iterate through all (n+1)! permutations of the indices\n",
                "    for perm_indices in itertools.permutations(all_indices):\n",
                "        # Reconstruct the X sequence based on the permutation of fixed values\n",
                "        perm_x_sequence = [x_values_sequence_fixed[idx] for idx in perm_indices]\n",
                "        \n",
                "        # Calculate lambda for this permuted sequence\n",
                "        lambda_val = Lambda_density(perm_x_sequence)\n",
                "        denominator_sum += lambda_val\n",
                "        \n",
                "        # The (n+1)-th element of the permuted sequence is perm_x_sequence[n_plus_1 - 1]\n",
                "        # We need to find which original index maps to this position.\n",
                "        original_idx_at_last_pos = perm_indices[n_plus_1 - 1]\n",
                "        numerator_sums = numerator_sums.at[original_idx_at_last_pos].add(lambda_val)\n",
                "    \n",
                "    if denominator_sum == 0: # Avoid division by zero\n",
                "        print(\"Warning: Denominator sum is zero. Check Lambda_density or input values.\")\n",
                "        return jnp.ones(n_plus_1) / n_plus_1 # Fallback to uniform weights\n",
                "        \n",
                "    p_lambda_weights = numerator_sums / denominator_sum\n",
                "    return p_lambda_weights\n",
                "\n",
                "# Calculate the weights p_i^lambda (for fixed X values)\n",
                "p_lambda_for_fixed_X_values = calculate_p_lambda(X_all_points, num_training_points)\n",
                "\n",
                "print(f\"Calculated p_lambda weights: {p_lambda_for_fixed_X_values}\\n\")\n",
                "print(f\"Sum of p_lambda weights: {jnp.sum(p_lambda_for_fixed_X_values):.4f}\\n\")\n",
                "\n",
                "# --- 3. Compute Scores R_i^(x,y) ---\n",
                "# In the full conformal setting, a base predictor is trained on augmented data for each (x,y).\n",
                "# Here, (X_train_values_fixed, y_train_values) + (X_test_point_fixed, y_candidate_example).\n",
                "\n",
                "X_augmented = jnp.concatenate([X_train_values_fixed, X_test_point_fixed]).reshape(-1, 1)\n",
                "y_augmented = jnp.concatenate([y_train_values, y_candidate_example])\n",
                "\n",
                "base_predictor_for_scores = LinearRegression()\n",
                "base_predictor_for_scores.fit(np.array(X_augmented), np.array(y_augmented))\n",
                "\n",
                "all_residuals_for_scores = jnp.abs(y_augmented - jnp.array(base_predictor_for_scores.predict(np.array(X_augmented))))\n",
                "\n",
                "R_n_plus_1_query = all_residuals_for_scores[-1] # Residual for the query point\n",
                "\n",
                "print(f\"All residuals for scores (n+1 points): {all_residuals_for_scores}\")\n",
                "print(f\"Residual for query point (R_n+1^(x,y)): {R_n_plus_1_query:.4f}\\n\")\n",
                "\n",
                "# --- 4. Form Weighted Quantile for a Specific y_candidate ---\n",
                "alpha_level = 0.1 # 90% coverage\n",
                "\n",
                "# Create a weighted empirical distribution for the residuals\n",
                "# The delta_infinity term is handled by effectively treating the test point's weight separately\n",
                "# Here we are trying to check if R_n+1^(x,y) <= Quantile(1-alpha; sum(pi_i^w * delta_R_i) + pi_n+1^w * delta_inf)\n",
                "\n",
                "# For the sum term, we take the original training residuals, weighted by p_lambda\n",
                "# The p_lambda are already calculated using the specific X_test_point.\n",
                "training_residuals_for_quantile = all_residuals_for_scores[:-1]\n",
                "training_p_lambda_weights = p_lambda_for_fixed_X_values[:-1]\n",
                "p_n_plus_1_lambda = p_lambda_for_fixed_X_values[-1] # Weight for the test point\n",
                "\n",
                "# Create pairs of (residual, weight) for the training points\n",
                "weighted_training_score_pairs = sorted([(training_residuals_for_quantile[i], training_p_lambda_weights[i]) for i in range(num_training_points)])\n",
                "\n",
                "cumulative_weighted_sum = 0.0\n",
                "weighted_quantile = 0.0\n",
                "target_cumulative_weight = 1 - alpha_level # We want 1-alpha quantile of the normalized distribution\n",
                "\n",
                "# Iterate through sorted weighted training scores to find the quantile\n",
                "# Note: This is an approximate way to find the quantile of a discrete weighted distribution.\n",
                "for res, weight in weighted_training_score_pairs:\n",
                "    cumulative_weighted_sum += weight\n",
                "    if cumulative_weighted_sum >= target_cumulative_weight:\n",
                "        weighted_quantile = res\n",
                "        break\n",
                "else:\n",
                "    # If target not reached, it means quantile is larger than max training residual.\n",
                "    # Given delta_infinity, it will be infinity unless the target is very low.\n",
                "    # For simplicity, if target isn't met by sum of training weights, it means the quantile is effectively large.\n",
                "    weighted_quantile = jnp.inf # If target is higher than sum of training weights\n",
                "\n",
                "print(f\"Weighted quantile for this query point (approx): {weighted_quantile:.4f}\")\n",
                "\n",
                "# Check if the query point's residual is in the set\n",
                "is_in_set = R_n_plus_1_query <= weighted_quantile\n",
                "print(f\"Is (X_test_point={X_test_point_fixed.item()}, Y_candidate={y_candidate_example.item()}) in the conformal set? {is_in_set}\")\n",
                "\n",
                "print(\"\\nNOTE: This code is a conceptual demonstration for very small N due to the factorial complexity of permutations.\")\n",
                "print(\"A practical implementation for structured-X settings would require specialized algorithms to compute p_lambda weights efficiently (e.g., for specific Markov structures).\" )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "custom_weighted_cp"
            },
            "source": [
                "## 4. Custom-Weighted Conformal Prediction\n",
                "\n",
                "While likelihood-weighted conformal prediction targets specific distribution shifts like covariate shift, **custom-weighted conformal prediction** (Barber et al., 2023) takes a more general approach. Instead of inferring weights from a likelihood ratio, this method allows for fixed, arbitrary weights $w_i \\in [0,1]$ for each training point. This can be useful when one has prior knowledge about the relevance or representativeness of certain training data points for the test distribution, without necessarily needing to model the exact shift.\n",
                "\n",
                "### Fixed Arbitrary Weights (Theorem 3)\n",
                "\n",
                "Given fixed weights $w_i \\in [0,1]$ for $i=1, \\ldots, n$, we define normalized weights $\\tilde{w}_i$ including a unit weight for the test point:\n",
                "\n",
                "$$\\tilde{w}_{i}=\\frac{w_{i}}{w_{1}+\\cdots+w_{n}+1}, \\quad i=1,\\ldots,n$$\n",
                "$$\\tilde{w}_{n+1}=\\frac{1}{w_{1}+\\cdots+w_{n}+1}$$\n",
                "\n",
                "The weighted conformal set is then formed as:\n",
                "\n",
                "$$\\hat{C}_{n}^{w}(x)=\\left\\{y:R_{n+1}^{(x,y)}\\le \\text{Quantile}\\left(1-\\alpha;\\sum_{i=1}^{n}\\tilde{w}_{i}\\delta_{R_{i}^{(x,y)}}+\\tilde{w}_{n+1}\\delta_{\\infty}\\right)\\right\\}$$\n",
                "\n",
                "where $R_i^{(x,y)}$ are conformity scores based on a base predictor. Importantly, **Theorem 3** states that *without any assumptions on the joint distribution of $Z_i=(X_i,Y_i)$*, this set satisfies:\n",
                "\n",
                "$$\\mathbb{P}(Y_{n+1}\\in\\hat{C}_{n}^{w}(X_{n+1}))\\ge1-\\alpha-\\sum_{i=1}^{n}\\tilde{w}_{i}\\cdot TV(R(Z),R(Z^{i}))$$\n",
                "\n",
                "Here, $TV(A,B)$ is the total variation distance between the distributions of random variables $A$ and $B$, and $R(Z^i)$ denotes the score vector if $Z_i$ and $Z_{n+1}$ were swapped. The term $\\sum_{i=1}^{n}\\tilde{w}_{i}\\cdot TV(R(Z),R(Z^{i}))$ is called the **coverage gap**.\n",
                "\n",
                "**Interpretation of the Coverage Gap**: This result implies that the guarantee is approximate, with a \"gap\" that depends on how much swapping $Z_i$ with $Z_{n+1}$ changes the distribution of scores. If the training data points $Z_i$ are highly representative of the test data $Z_{n+1}$ (i.e., small $TV$ distances), and/or if we assign large weights $\\tilde{w}_i$ to such representative points, the coverage gap will be small.\n",
                "\n",
                "**Special Cases**:\n",
                "* **I.I.D. Setting**: If the data is truly I.I.D. (or exchangeable), then $TV(R(Z), R(Z^i))=0$ for all $i$. In this case, the coverage gap is zero, and the guarantee becomes exact $1-\\alpha$. This means that even with arbitrary fixed weights, conformal prediction remains valid under I.I.D. data.\n",
                "* **Split Version**: Similar to likelihood-weighted CP, a split conformal version exists where the base predictor is trained on an external dataset $Z_0$. The coverage gap then becomes conditional on $Z_0$.\n",
                "\n",
                "### Nonsymmetric Score Functions (Theorem 4)\n",
                "\n",
                "Traditional conformal prediction assumes the score function $V$ is symmetric with respect to its inputs. However, some models (e.g., autoregressive models in time series) are inherently non-symmetric as they depend on the order of data points. **Theorem 4** addresses this by introducing a \"random swap\" into the score computation:\n",
                "\n",
                "It uses conformity scores $R_i^{(x,y), K}$ where the vector of augmented data $(Z_1, \\ldots, Z_n, (x,y))$ has its components $K$ and $n+1$ swapped, and $K$ is randomly chosen from a multinomial distribution defined by the normalized weights $\\tilde{w}_i$.\n",
                "\n",
                "This extension broadens the applicability of custom-weighted conformal prediction to a wider range of algorithms and structured data settings (e.g., time series with decaying weights, as shown in Figure 2 of the original slides), while still providing a controlled coverage guarantee that depends on the coverage gap.\n",
                "\n",
                "### Code Example: Custom-Weighted Split Conformal Prediction\n",
                "\n",
                "This example demonstrates Custom-Weighted Split Conformal Prediction with arbitrary fixed weights. We'll simulate a regression problem and assign weights to calibration points, for instance, to reflect their perceived relevance or quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "custom_weighted_cp_code"
            },
            "outputs": [],
            "source": [
                "import jax.numpy as jnp\n",
                "import jax.random as random\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "# Set a random seed for reproducibility\n",
                "key = random.PRNGKey(0)\n",
                "\n",
                "# --- 1. Simulate Regression Data ---\n",
                "num_samples_train = 500\n",
                "noise_std = 5.0\n",
                "\n",
                "X_data_np = np.linspace(-5, 5, num_samples_train)[:, None]\n",
                "y_true = 2 * X_data_np.squeeze() + 3\n",
                "y_data_np = y_true + np.random.normal(0, noise_std, num_samples_train)\n",
                "\n",
                "X_data = jnp.array(X_data_np)\n",
                "y_data = jnp.array(y_data_np)\n",
                "\n",
                "print(f\"Total samples: {num_samples_train}\\n\")\n",
                "\n",
                "# --- 2. Split Data into Proper Training (D1) and Calibration (D2) ---\n",
                "X_D1, X_D2, y_D1, y_D2 = train_test_split(\n",
                "    X_data, y_data, test_size=0.5, random_state=key.get(0).tolist()[0]\n",
                ")\n",
                "n1 = X_D1.shape[0]\n",
                "n2 = X_D2.shape[0]\n",
                "print(f\"Proper training set size (n1): {n1}\")\n",
                "print(f\"Calibration set size (n2): {n2}\\n\")\n",
                "\n",
                "# --- 3. Assign Custom Weights to Calibration Points ---\n",
                "# For demonstration, let's assign higher weights to points closer to 0 (more central/reliable)\n",
                "# Weights should be in [0, 1]. A simple decaying function based on |X|.\n",
                "custom_weights_D2 = 1.0 - jnp.abs(X_D2.squeeze()) / jnp.max(jnp.abs(X_D2.squeeze()))\n",
                "custom_weights_D2 = jnp.clip(custom_weights_D2, 0.1, 1.0) # Ensure min weight of 0.1\n",
                "\n",
                "print(f\"Custom weights for calibration set (first 5): {custom_weights_D2[:5]}\\n\")\n",
                "\n",
                "# --- 4. Train Base Predictor on Proper Training Set (D1) ---\n",
                "base_predictor = LinearRegression()\n",
                "base_predictor.fit(np.array(X_D1), np.array(y_D1))\n",
                "\n",
                "# --- 5. Compute Calibration Scores (Absolute Residuals) on D2 ---\n",
                "y_D2_pred = base_predictor.predict(np.array(X_D2))\n",
                "calibration_residuals = jnp.abs(y_D2 - jnp.array(y_D2_pred))\n",
                "print(f\"Calibration residuals (first 5): {calibration_residuals[:5]}\\n\")\n",
                "\n",
                "# --- 6. Compute Custom-Weighted Conformal Quantile ---\n",
                "alpha_level = 0.1 # Desired coverage: 1 - alpha = 0.9 (90%)\n",
                "\n",
                "# Normalize custom weights to sum to 1 + weight for the test point (which is 1)\n",
                "sum_of_fixed_weights = jnp.sum(custom_weights_D2)\n",
                "denominator = sum_of_fixed_weights + 1 # +1 for the implicit unit weight of the test point\n",
                "normalized_weights_D2 = custom_weights_D2 / denominator\n",
                "\n",
                "# Create pairs of (residual, normalized_weight) and sort by residual\n",
                "sorted_weighted_pairs = sorted(zip(calibration_residuals, normalized_weights_D2))\n",
                "\n",
                "cumulative_normalized_weights = 0.0\n",
                "custom_weighted_conformal_quantile = 0.0\n",
                "\n",
                "# Target for the (1-alpha) quantile, where the sum of weights is 1 (after normalization)\n",
                "target_cumulative_normalized_weight = 1 - alpha_level\n",
                "\n",
                "for res, norm_weight in sorted_weighted_pairs:\n",
                "    cumulative_normalized_weights += norm_weight\n",
                "    if cumulative_normalized_weights >= target_cumulative_normalized_weight:\n",
                "        custom_weighted_conformal_quantile = res\n",
                "        break\n",
                "else:\n",
                "    # Fallback if target not reached (e.g., all weights are very small, or alpha is too low)\n",
                "    custom_weighted_conformal_quantile = sorted_weighted_pairs[-1][0] # Max residual\n",
                "\n",
                "print(f\"Custom-weighted conformal quantile: {custom_weighted_conformal_quantile:.4f}\\n\")\n",
                "\n",
                "# --- 7. Form Conformal Prediction Band ---\n",
                "num_test_points = 500\n",
                "X_test_np = np.linspace(X_data.min() - 1, X_data.max() + 1, num_test_points)[:, None]\n",
                "y_test_true = 2 * X_test_np.squeeze() + 3 + np.random.normal(0, noise_std, num_test_points)\n",
                "\n",
                "X_test = jnp.array(X_test_np)\n",
                "y_test = jnp.array(y_test_true)\n",
                "\n",
                "y_test_pred_mean = base_predictor.predict(np.array(X_test))\n",
                "\n",
                "lower_bound_custom_weighted = y_test_pred_mean - custom_weighted_conformal_quantile\n",
                "upper_bound_custom_weighted = y_test_pred_mean + custom_weighted_conformal_quantile\n",
                "\n",
                "# --- Plotting Results ---\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.scatter(X_D1[:, 0], y_D1, color='blue', label='Proper Training Data (D1)', alpha=0.5, s=20)\n",
                "plt.scatter(X_D2[:, 0], y_D2, color='green', label='Calibration Data (D2)', alpha=0.5, s=20)\n",
                "plt.scatter(X_D2[:, 0], y_D2, c=custom_weights_D2, cmap='viridis', s=custom_weights_D2 * 50, alpha=0.8, label='Calibration Weights (Size & Color)')\n",
                "cbar = plt.colorbar(plt.cm.ScalarMappable(cmap='viridis'), ax=plt.gca())\n",
                "cbar.set_label('Custom Weight')\n",
                "\n",
                "plt.plot(X_test[jnp.argsort(X_test.squeeze()), 0], y_test_pred_mean[jnp.argsort(X_test.squeeze())], color='red', linewidth=2, label='Regression Mean')\n",
                "\n",
                "plt.fill_between(\n",
                "    X_test[jnp.argsort(X_test.squeeze()), 0].squeeze(),\n",
                "    lower_bound_custom_weighted[jnp.argsort(X_test.squeeze())],\n",
                "    upper_bound_custom_weighted[jnp.argsort(X_test.squeeze())],\n",
                "    color='red', alpha=0.2, label=f'Custom-Weighted Conformal Band (1-$\\alpha$={1-alpha_level})'\n",
                ")\n",
                "\n",
                "plt.xlabel('X')\n",
                "plt.ylabel('Y')\n",
                "plt.title('Custom-Weighted Conformal Prediction')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "# --- Simulate Test Coverage ---\n",
                "coverage_count_custom_weighted = 0\n",
                "for i in range(num_test_points):\n",
                "    if (y_test[i] >= lower_bound_custom_weighted[i]) and \\\n",
                "       (y_test[i] <= upper_bound_custom_weighted[i]):\n",
                "        coverage_count_custom_weighted += 1\n",
                "\n",
                "simulated_coverage_custom_weighted = coverage_count_custom_weighted / num_test_points\n",
                "print(f\"Simulated custom-weighted conformal coverage on test set: {simulated_coverage_custom_weighted:.4f}\")\n",
                "print(f\"Desired coverage (approx): {1 - alpha_level:.4f}\")\n",
                "\n",
                "print(\"\\nNOTE: The coverage guarantee for custom weights is approximate (1-alpha - coverage_gap).\")\n",
                "print(\"The coverage_gap depends on the total variation distance, which is hard to compute.\")\n",
                "print(\"This example illustrates the mechanics, not a strict coverage verification for non-IID data.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "adaptive_conformal_inference"
            },
            "source": [
                "## 5. Adaptive Conformal Inference (ACI)\n",
                "\n",
                "While the previous methods deal with a fixed, known shift, **Adaptive Conformal Inference (ACI)** (Gibbs and Candès, 2021) is designed for **sequential prediction problems** where the data distribution can shift arbitrarily and continuously over time. It's an online method that dynamically adjusts the confidence level of prediction sets to maintain coverage.\n",
                "\n",
                "Assume we have a sequence of observations $(X_t, Y_t)$ indexed by time $t=1, 2, 3, \\ldots$. At each time $t$, we have a method that can produce a prediction set $C_t^\\beta$ for $Y_t$ at any nominal level $\\beta \\in \\mathbb{R}$. All that's required is that $C_t^\\beta = \\emptyset$ for $\\beta \\le 0$ and $C_t^\\beta = \\mathcal{Y}$ for $\\beta \\ge 1$.\n",
                "\n",
                "### The ACI Algorithm\n",
                "\n",
                "ACI aims to maintain a realized coverage as close to $1-\\alpha$ as possible, where $\\alpha \\in (0,1)$ is a prespecified error tolerance. It initializes $\\alpha_0 = \\alpha$ and performs updates for the working error level $\\alpha_t$ according to:\n",
                "\n",
                "$$\\alpha_{t+1} = \\alpha_t - \\eta(\\text{err}_t - \\alpha), \\quad t=0, 1, 2, \\ldots$$",
                "\n",
                "where:\n",
                "* $\\text{err}_t = 1\\{Y_t \\notin C_{1-\\alpha_t}^t\\}$ is an indicator variable that is 1 if the prediction set at time $t$ fails to cover $Y_t$, and 0 otherwise.\n",
                "* $\\eta > 0$ is a step size.\n",
                "\n",
                "These updates are intuitive: if the set fails to cover ($err_t=1$), we decrease $\\alpha_t$ (making future sets larger and more conservative). If it covers ($err_t=0$), we increase $\\alpha_t$ (making future sets smaller and more efficient).\n",
                "\n",
                "### Key Properties\n",
                "\n",
                "* **Boundedness (Lemma 2)**: The ACI iterates $\\alpha_t$ are always uniformly bounded within $[-\\eta, 1+\\eta]$. This self-correcting property prevents $\\alpha_t$ from diverging.\n",
                "\n",
                "* **Long-Run Coverage (Theorem 5)**: This is the most profound result. For any interval $[t_0+1, t_0+T]$, the average error satisfies:\n",
                "    $$\\left|\\frac{1}{T}\\sum_{t=t_0+1}^{t_0+T}\\text{err}_t - \\alpha\\right| \\le \\frac{1+2\\eta}{\\eta T}$$\n",
                "    In particular, as $T \\to \\infty$, the long-run average error converges to $\\alpha$:\n",
                "    $$\\lim_{T\\to\\infty}\\frac{1}{T}\\sum_{t=1}^{T}\\text{err}_t = \\alpha$$\n",
                "    This implies that ACI achieves long-run coverage of $1-\\alpha$ *always*, regardless of the sequence $(X_t, Y_t)$ (even if chosen adversarially) and without any distributional assumptions beyond the existence of prediction sets $C_t^\\beta$.\n",
                "\n",
                "* **Connection to Online Gradient Descent**: ACI can be viewed as an instance of online gradient descent applied to a convex optimization problem, where the goal is to minimize cumulative errors over time.\n",
                "\n",
                "Figure 3 from the original slides illustrates ACI's performance on financial time series data, showing that its local coverage frequencies remain stable even during periods when a non-adaptive method fails.\n",
                "\n",
                "### Code Example: Adaptive Conformal Inference (ACI)\n",
                "\n",
                "This example demonstrates Adaptive Conformal Inference in a simulated sequential setting with a gradual distribution shift. At each timestep, a prediction set is generated based on a sliding window of past data, and the `alpha` level is updated using the ACI algorithm to maintain long-run coverage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "adaptive_conformal_inference_code"
            },
            "outputs": [],
            "source": [
                "import jax.numpy as jnp\n",
                "import jax.random as random\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from collections import deque # For sliding window\n",
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "# Set a random seed for reproducibility\n",
                "key = random.PRNGKey(0)\n",
                "\n",
                "# --- 1. Simulate Sequential Data with Gradual Shift ---\n",
                "total_timesteps = 500\n",
                "initial_X_mean = 0.0\n",
                "shift_magnitude = 0.01 # How much X_mean shifts per timestep\n",
                "noise_std = 5.0\n",
                "regression_slope = 2.0\n",
                "regression_intercept = 3.0\n",
                "\n",
                "X_stream = jnp.zeros(total_timesteps)\n",
                "Y_stream = jnp.zeros(total_timesteps)\n",
                "true_X_means = jnp.zeros(total_timesteps)\n",
                "\n",
                "current_X_mean = initial_X_mean\n",
                "for t in range(total_timesteps):\n",
                "    key, subkey = random.split(key)\n",
                "    X_t = random.normal(subkey, (1,))[0] + current_X_mean\n",
                "    Y_t = regression_slope * X_t + regression_intercept + random.normal(key, (1,))[0] * noise_std\n",
                "    \n",
                "    X_stream = X_stream.at[t].set(X_t)\n",
                "    Y_stream = Y_stream.at[t].set(Y_t)\n",
                "    true_X_means = true_X_means.at[t].set(current_X_mean)\n",
                "    \n",
                "    # Gradual shift: mean of X increases over time\n",
                "    current_X_mean += shift_magnitude\n",
                "\n",
                "print(f\"Simulated {total_timesteps} timesteps of data with gradual shift.\\n\")\n",
                "\n",
                "# --- 2. ACI Algorithm Setup ---\n",
                "alpha_target = 0.1 # Desired average error rate (1 - coverage)\n",
                "eta = 0.1 # ACI step size\n",
                "window_size = 100 # Size of the sliding window for base predictor training/calibration\n",
                "\n",
                "alpha_t_history = [alpha_target] # History of alpha_t values, starting with alpha_0\n",
                "err_t_history = [] # History of err_t (0 or 1)\n",
                "coverage_history = [] # History of actual coverage at each step\n",
                "prediction_set_widths = [] # History of prediction set widths\n",
                "\n",
                "# Deques for sliding window (training and calibration combined for simplicity in this online context)\n",
                "data_window_X = deque(maxlen=window_size)\n",
                "data_window_Y = deque(maxlen=window_size)\n",
                "\n",
                "base_predictor = LinearRegression()\n",
                "\n",
                "print(f\"ACI setup: target alpha={alpha_target}, eta={eta}, window_size={window_size}\\n\")\n",
                "\n",
                "# --- 3. Run ACI over the Sequential Data Stream ---\n",
                "for t in range(total_timesteps):\n",
                "    # Add current data point to window\n",
                "    data_window_X.append(X_stream[t])\n",
                "    data_window_Y.append(Y_stream[t])\n",
                "\n",
                "    if len(data_window_X) < 20: # Need enough data to train a simple model\n",
                "        # Not enough data for prediction set, skip this step but record current alpha\n",
                "        alpha_t_history.append(alpha_t_history[-1])\n",
                "        err_t_history.append(0) # Assume no error for warm-up\n",
                "        coverage_history.append(1) # Assume 100% coverage for warm-up\n",
                "        prediction_set_widths.append(np.nan) # No valid width yet\n",
                "        continue\n",
                "\n",
                "    # Current alpha level from previous step's update\n",
                "    current_alpha = alpha_t_history[-1]\n",
                "\n",
                "    # --- Generate Prediction Set C_{1-alpha_t}^t (using simple split conformal on current window) ---\n",
                "    # For simplicity, we'll use the whole window as calibration data\n",
                "    # In a true split setting, you'd split the window into training/calibration\n",
                "    \n",
                "    # Here, we'll train on the first half of the window and calibrate on the second half\n",
                "    # This mimics a split conformal approach within the sliding window\n",
                "    window_X_np = np.array(data_window_X).reshape(-1, 1)\n",
                "    window_Y_np = np.array(data_window_Y)\n",
                "\n",
                "    # Simple split of current window (e.g., first half for training, second for calibration)\n",
                "    split_idx = len(window_X_np) // 2\n",
                "    X_train_window, y_train_window = window_X_np[:split_idx], window_Y_np[:split_idx]\n",
                "    X_cal_window, y_cal_window = window_X_np[split_idx:], window_Y_np[split_idx:]\n",
                "\n",
                "    if len(X_cal_window) == 0: # Ensure calibration set is not empty\n",
                "        alpha_t_history.append(alpha_t_history[-1])\n",
                "        err_t_history.append(0)\n",
                "        coverage_history.append(1)\n",
                "        prediction_set_widths.append(np.nan)\n",
                "        continue\n",
                "    \n",
                "    # Train base predictor on training part of the window\n",
                "    base_predictor.fit(X_train_window, y_train_window)\n",
                "\n",
                "    # Compute calibration scores (absolute residuals) on calibration part of the window\n",
                "    y_cal_pred = base_predictor.predict(X_cal_window)\n",
                "    calibration_scores_window = jnp.abs(y_cal_window - jnp.array(y_cal_pred))\n",
                "\n",
                "    n_cal_window = len(X_cal_window)\n",
                "    # Calculate quantile for current_alpha (which is the error rate for C_{1-alpha_t}^t)\n",
                "    # We need 1 - current_alpha coverage.\n",
                "    adjusted_rank_index_window = jnp.ceil((1 - current_alpha) * (n_cal_window + 1)).astype(int)\n",
                "    sorted_scores_window = jnp.sort(calibration_scores_window)\n",
                "    \n",
                "    if adjusted_rank_index_window <= 0:\n",
                "        q_t = sorted_scores_window[0]\n",
                "    elif adjusted_rank_index_window > n_cal_window:\n",
                "        q_t = sorted_scores_window[-1]\n",
                "    else:\n",
                "        q_t = sorted_scores_window[adjusted_rank_index_window - 1]\n",
                "\n",
                "    # Predict for the current X_t (the current point in the stream)\n",
                "    X_t_reshaped = X_stream[t].reshape(-1, 1)\n",
                "    y_t_pred_mean = base_predictor.predict(np.array(X_t_reshaped))[0]\n",
                "    \n",
                "    # Form the prediction set for Y_t using current_alpha\n",
                "    lower_bound_t = y_t_pred_mean - q_t\n",
                "    upper_bound_t = y_t_pred_mean + q_t\n",
                "\n",
                "    # Check if Y_t is covered by C_{1-alpha_t}^t\n",
                "    is_covered = (Y_stream[t] >= lower_bound_t) and (Y_stream[t] <= upper_bound_t)\n",
                "    err_t = 1 - is_covered # err_t is 1 if not covered, 0 if covered\n",
                "    \n",
                "    # Store histories\n",
                "    err_t_history.append(err_t)\n",
                "    coverage_history.append(is_covered)\n",
                "    prediction_set_widths.append(upper_bound_t - lower_bound_t)\n",
                "\n",
                "    # --- ACI Update Rule ---\n",
                "    alpha_next = current_alpha - eta * (err_t - alpha_target)\n",
                "    \n",
                "    # Clip alpha to a reasonable range, e.g., (0, 1) or [-eta, 1+eta] as per Lemma 2.\n",
                "    # Clipping to (0,1) for practical purposes often helps keep sets non-empty/non-trivial.\n",
                "    alpha_next = jnp.clip(alpha_next, 0.001, 0.999) \n",
                "\n",
                "    alpha_t_history.append(alpha_next)\n",
                "\n",
                "    if (t + 1) % 50 == 0:\n",
                "        print(f\"Timestep {t+1}: Current alpha={current_alpha:.4f}, err_t={err_t}, new alpha={alpha_next:.4f}\")\n",
                "        print(f\"  Coverage at t={t+1}: {is_covered}, Prediction Set Width: {prediction_set_widths[-1]:.2f}\")\n",
                "\n",
                "# Remove initial warm-up entries for plotting if necessary\n",
                "alpha_t_history = alpha_t_history[1:] # First entry was alpha_0\n",
                "\n",
                "print(\"\\n--- Simulation Complete ---\")\n",
                "\n",
                "# --- 4. Plotting Results --- \n",
                "fig, axes = plt.subplots(3, 1, figsize=(14, 15), sharex=True)\n",
                "\n",
                "timesteps = jnp.arange(total_timesteps)\n",
                "\n",
                "# Plot 1: True X Mean Shift\n",
                "axes[0].plot(timesteps, true_X_means, label='True Mean of X', color='purple', linestyle='--')\n",
                "axes[0].set_ylabel('X Mean')\n",
                "axes[0].set_title('Simulated Distribution Shift Over Time (Mean of X)')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True)\n",
                "\n",
                "# Plot 2: Adaptive Alpha\n",
                "axes[1].plot(timesteps, alpha_t_history, label='Adaptive $\\alpha_t$', color='blue')\n",
                "axes[1].axhline(alpha_target, color='red', linestyle=':', label=f'Target $\\alpha$={alpha_target}')\n",
                "axes[1].set_ylabel('$\\alpha_t$ (Error Level)')\n",
                "axes[1].set_title('Adaptive Error Level ($\\\\alpha_t$) by ACI')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True)\n",
                "\n",
                "# Plot 3: Long-Run Coverage\n",
                "cumulative_errors = jnp.cumsum(jnp.array(err_t_history))\n",
                "average_errors = cumulative_errors / (jnp.arange(len(err_t_history)) + 1)\n",
                "\n",
                "axes[2].plot(timesteps, 1 - average_errors, label='Average Coverage', color='green')\n",
                "axes[2].axhline(1 - alpha_target, color='red', linestyle=':', label=f'Target Coverage (1-$\\alpha$)={1-alpha_target}')\n",
                "\n",
                "# Moving average for local coverage (e.g., over a window)\n",
                "window_for_local_coverage = 50\n",
                "local_coverage = jnp.convolve(jnp.array(coverage_history), jnp.ones(window_for_local_coverage)/window_for_local_coverage, mode='valid')\n",
                "axes[2].plot(timesteps[window_for_local_coverage-1:], local_coverage, label=f'Local Coverage ({window_for_local_coverage}-step avg)', color='cyan', alpha=0.7)\n",
                "\n",
                "axes[2].set_xlabel('Timestep (t)')\n",
                "axes[2].set_ylabel('Coverage')\n",
                "axes[2].set_title('Actual Coverage Over Time (ACI)')\n",
                "axes[2].set_ylim(0.5, 1.05) # Keep y-axis reasonable\n",
                "axes[2].legend()\n",
                "axes[2].grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Final average error rate: {average_errors[-1]:.4f}\")\n",
                "print(f\"Target error rate (alpha): {alpha_target:.4f}\")\n",
                "print(\"\\nNOTE: ACI aims for long-run average coverage. Local coverage may fluctuate, especially with strong shifts or small windows.\")\n",
                "print(\"The simple linear regression model inside ACI might not adapt perfectly to non-linear relationships or complex shifts.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "conclusion_shift"
            },
            "source": [
                "## Conclusion: Conformal Prediction in a Dynamic World\n",
                "\n",
                "This notebook has extended our understanding of Conformal Prediction beyond the ideal I.I.D. setting to more realistic scenarios involving distribution shifts. The advancements in likelihood-weighted, custom-weighted, and adaptive conformal prediction techniques are vital for building robust and reliable machine learning systems in dynamic and unpredictable environments.\n",
                "\n",
                "Key insights:\n",
                "\n",
                "* **Covariate Shift**: Standard conformal prediction fails under covariate shift due to broken exchangeability. Likelihood-weighted conformal prediction restores validity by re-weighting calibration scores based on the density ratio between test and training feature distributions.\n",
                "* **Weight Estimation**: The necessary likelihood ratios can be estimated using a binary classifier trained to distinguish between features from the source and target distributions.\n",
                "* **Generalization**: The theoretical framework extends to highly structured feature dependencies (Theorem 2) and allows for arbitrary custom weights (Theorem 3), albeit with potential computational challenges or approximate guarantees.\n",
                "* **Adaptive Online Learning**: ACI provides a powerful solution for truly online, sequential prediction where distributions shift arbitrarily, guaranteeing long-run average coverage without strong assumptions.\n",
                "\n",
                "These sophisticated conformal methods underscore the flexibility and adaptability of the framework, enabling practitioners to provide statistically valid uncertainty quantification even when faced with the inherent complexities of real-world data and distribution changes. As machine learning models are deployed in increasingly complex and critical applications, the ability to provide reliable uncertainty estimates under distribution shift becomes ever more important."
            ]
        }
    ]
}