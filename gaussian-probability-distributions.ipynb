{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b56075",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfcc3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy.stats as jss  # For standard distributions like norm\n",
    "import numpy as np  # For plotting\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets  # For interactive controls\n",
    "from IPython.display import display  # To display widgets and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a6e029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6be421",
   "metadata": {},
   "source": [
    "\n",
    "# Lecture 06: Gaussian Probability Distributions\n",
    "\n",
    "Based on the lecture slides by Philipp Hennig (SS 2023).\n",
    "\n",
    "Welcome to Lecture 06! This session focuses on the Gaussian distribution, which is arguably the most important distribution in probabilistic machine learning. We will explore its key properties, how it relates to linear algebra, and why this makes inference particularly tractable. We'll use JAX to implement the concepts and work through realistic examples.\n",
    "\n",
    "## Review: Exponential Families\n",
    "\n",
    "As we saw in Lecture 05, many common probability distributions belong to the **Exponential Family**. A distribution is in this family if its probability density/mass function can be written as:\n",
    "$$p_w(x) = h(x) \\exp[\\phi(x)^T w - \\log Z(w)]$$\n",
    "where $h(x)$ is the base measure, $\\phi(x)$ are the sufficient statistics, $w$ are the natural parameters, and $Z(w)$ is the partition function. This structure is powerful because it simplifies things like finding conjugate priors and performing maximum likelihood estimation.\n",
    "\n",
    "## The Univariate Gaussian Distribution\n",
    "\n",
    "Let's start with the familiar **univariate Gaussian** (or Normal) distribution, denoted as $N(x; \\mu, \\sigma^2)$. Its probability density function (PDF) is:\n",
    "$$N(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "Here, $\\mu$ is the mean and $\\sigma^2$ is the variance.\n",
    "\n",
    "In Lecture 05, we showed that the univariate Gaussian is an Exponential Family member. Its sufficient statistics are $\\phi(x) = \\begin{bmatrix} x \\\\ -x^2/2 \\end{bmatrix}$ and its natural parameters are $w = \\begin{bmatrix} \\mu/\\sigma^2 \\\\ 1/\\sigma^2 \\end{bmatrix}$.\n",
    "\n",
    "Let's use JAX to work with the univariate Gaussian. JAX's `jax.scipy.stats` module provides convenient functions for standard distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03eacc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "PDF",
         "type": "scatter",
         "x": {
          "bdata": "AAAAABA+eD0QPvg9jC46PhA+eD7KJps+jC66Pk422T4QPvg+6aILP8omGz+rqio/jC46P22yST9ONlk/L7poPxA+eD/44IM/6aKLP9pkkz/KJps/uuiiP6uqqj+cbLI/jC66P3zwwT9tssk/XnTRP0422T8++OA/L7roPyB88D8QPvg/AAAAQPjgA0DxwQdA6aILQOGDD0DaZBNA0kUXQMomG0DCBx9AuugiQLPJJkCrqipAo4suQJxsMkCUTTZAjC46QIQPPkB88EFAddFFQG2ySUBlk01AXnRRQFZVVUBONllARhddQD74YEA32WRAL7poQCebbEAgfHBAGF10QBA+eEAIH3xAAACAQHzwgUD44INAddGFQPHBh0BtsolA6aKLQGWTjUDhg49AXXSRQNpkk0BWVZVA0kWXQE42mUDKJptARhedQMIHn0A++KBAuuiiQDfZpECzyaZAL7qoQKuqqkAnm6xAo4uuQB98sECcbLJAGF20QJRNtkAQPrhAjC66QAgfvECED75AAADAQA==",
          "dtype": "f4"
         },
         "y": {
          "bdata": "8NjSOnsFBDuEkiQ7kTVMO0s7fDtYD5s7Esa9O+0x5zv0Lgw8OjgpPJ9VSzwvNXM8nsiQPJ6Xqzzzbso8OrntPIzxCj2TrCE9+kI7PbnnVz0pync9QoqNPSX1oD1ZM7Y9ZU3NPQJG5j2njAA+/d0OPloNHj7aDC4+I8o+Pk4uUD7pHWI+GHl0PuaNgz4P74w+WUqWPraInz4Kkqg+gk2xPhaiuT7ydsE+8LPIPhhCzz4SDNU+nv7ZPgIJ3j5bHeE+/DDjPqM85D6jPOQ+/DDjPlod4T4ACd4+nf7ZPhAM1T4XQs8+8LPIPvJ2wT4Uork+f02xPgiSqD60iJ8+VkqWPgzvjD7mjYM+GHl0PukdYj5OLlA+Hco+PtYMLj5WDR4++t0OPqWMAD4CRuY9ZU3NPU4ztj0g9aA9PYqNPSLKdz2y51c99EI7PZOsIT2M8Qo9OrntPOBuyjyZl6s8lciQPCc1czyfVUs8OjgpPPQuDDzQMec7Aca9O08Pmzs8O3w7hDVMO4SSJDt7BQQ78NjSOg==",
          "dtype": "f4"
         }
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Univariate Gaussian PDF (mu=3.0, sigma^2=0.8)"
        },
        "xaxis": {
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "title": {
          "text": "p(x)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "histnorm": "probability density",
         "marker": {
          "color": "green"
         },
         "name": "Samples",
         "nbinsx": 30,
         "opacity": 0.6,
         "type": "histogram",
         "x": {
          "bdata": "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",
          "dtype": "f4"
         }
        },
        {
         "line": {
          "color": "red",
          "width": 2
         },
         "mode": "lines",
         "name": "True PDF",
         "type": "scatter",
         "x": {
          "bdata": "AAAAABA+eD0QPvg9jC46PhA+eD7KJps+jC66Pk422T4QPvg+6aILP8omGz+rqio/jC46P22yST9ONlk/L7poPxA+eD/44IM/6aKLP9pkkz/KJps/uuiiP6uqqj+cbLI/jC66P3zwwT9tssk/XnTRP0422T8++OA/L7roPyB88D8QPvg/AAAAQPjgA0DxwQdA6aILQOGDD0DaZBNA0kUXQMomG0DCBx9AuugiQLPJJkCrqipAo4suQJxsMkCUTTZAjC46QIQPPkB88EFAddFFQG2ySUBlk01AXnRRQFZVVUBONllARhddQD74YEA32WRAL7poQCebbEAgfHBAGF10QBA+eEAIH3xAAACAQHzwgUD44INAddGFQPHBh0BtsolA6aKLQGWTjUDhg49AXXSRQNpkk0BWVZVA0kWXQE42mUDKJptARhedQMIHn0A++KBAuuiiQDfZpECzyaZAL7qoQKuqqkAnm6xAo4uuQB98sECcbLJAGF20QJRNtkAQPrhAjC66QAgfvECED75AAADAQA==",
          "dtype": "f4"
         },
         "y": {
          "bdata": "8NjSOnsFBDuEkiQ7kTVMO0s7fDtYD5s7Esa9O+0x5zv0Lgw8OjgpPJ9VSzwvNXM8nsiQPJ6Xqzzzbso8OrntPIzxCj2TrCE9+kI7PbnnVz0pync9QoqNPSX1oD1ZM7Y9ZU3NPQJG5j2njAA+/d0OPloNHj7aDC4+I8o+Pk4uUD7pHWI+GHl0PuaNgz4P74w+WUqWPraInz4Kkqg+gk2xPhaiuT7ydsE+8LPIPhhCzz4SDNU+nv7ZPgIJ3j5bHeE+/DDjPqM85D6jPOQ+/DDjPlod4T4ACd4+nf7ZPhAM1T4XQs8+8LPIPvJ2wT4Uork+f02xPgiSqD60iJ8+VkqWPgzvjD7mjYM+GHl0PukdYj5OLlA+Hco+PtYMLj5WDR4++t0OPqWMAD4CRuY9ZU3NPU4ztj0g9aA9PYqNPSLKdz2y51c99EI7PZOsIT2M8Qo9OrntPOBuyjyZl6s8lciQPCc1czyfVUs8OjgpPPQuDDzQMec7Aca9O08Pmzs8O3w7hDVMO4SSJDt7BQQ78NjSOg==",
          "dtype": "f4"
         }
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Histogram of Samples vs. True PDF"
        },
        "xaxis": {
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "title": {
          "text": "Density"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set a random seed for reproducibility\n",
    "key = jrandom.PRNGKey(42)\n",
    "\n",
    "\n",
    "# Define the standard log PDF of a univariate Gaussian\n",
    "def univariate_gaussian_log_pdf(x, mu, sigma_sq):\n",
    "    \"\"\"\n",
    "    Log PDF of a univariate Gaussian N(x; mu, sigma_sq).\n",
    "    \"\"\"\n",
    "    # jss.norm.logpdf handles the calculation directly\n",
    "    return jss.norm.logpdf(x, loc=mu, scale=jnp.sqrt(sigma_sq))  # scale is std dev\n",
    "\n",
    "\n",
    "# Example: Plotting a Gaussian PDF\n",
    "mu_example = 3.0\n",
    "sigma_sq_example = 0.8\n",
    "x_values = jnp.linspace(0, 6, 100)\n",
    "pdf_values = jnp.exp(\n",
    "    univariate_gaussian_log_pdf(x_values, mu_example, sigma_sq_example)\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.array(x_values), y=np.array(pdf_values), mode=\"lines\", name=\"PDF\")\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"Univariate Gaussian PDF (mu={mu_example}, sigma^2={sigma_sq_example})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"p(x)\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Example: Sampling from a univariate Gaussian\n",
    "key, subkey = jrandom.split(key)  # Split the key for new randomness\n",
    "num_samples = 500\n",
    "samples = (\n",
    "    jrandom.normal(subkey, (num_samples,)) * jnp.sqrt(sigma_sq_example) + mu_example\n",
    ")\n",
    "\n",
    "# Plot histogram of samples and true PDF using Plotly\n",
    "hist = go.Histogram(\n",
    "    x=np.array(samples),\n",
    "    nbinsx=30,\n",
    "    histnorm=\"probability density\",\n",
    "    opacity=0.6,\n",
    "    marker_color=\"green\",\n",
    "    name=\"Samples\",\n",
    ")\n",
    "pdf_line = go.Scatter(\n",
    "    x=np.array(x_values),\n",
    "    y=np.array(pdf_values),\n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"red\", width=2),\n",
    "    name=\"True PDF\",\n",
    ")\n",
    "fig_hist = go.Figure([hist, pdf_line])\n",
    "fig_hist.update_layout(\n",
    "    title=\"Histogram of Samples vs. True PDF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Density\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig_hist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b2e18",
   "metadata": {},
   "source": [
    "## Closure Properties: The Magic of Gaussians\n",
    "\n",
    "One of the most powerful aspects of Gaussian distributions is their **closure properties** under linear operations. This means that if you start with Gaussian random variables and perform certain operations (like adding them, applying linear transformations, looking at subsets, or conditioning on some values), the resulting distributions are **still Gaussian**. This makes exact inference possible using linear algebra.\n",
    "\n",
    "### 1. Products of Gaussians are Gaussians\n",
    "\n",
    "The product of two Gaussian probability densities is proportional to another Gaussian density. This is crucial for Bayesian inference because the posterior is proportional to the prior times the likelihood. If both are Gaussian, the posterior is also Gaussian.\n",
    "\n",
    "\n",
    "\n",
    "If \n",
    "$p_1(x) = \\mathcal{N}(x; \\mu_1, \\sigma_1^2)$ \n",
    "and \n",
    "$p_2(x) = \\mathcal{N}(x; \\mu_2, \\sigma_2^2)$, \n",
    "their product is:\n",
    "\n",
    "$$\n",
    "p_1(x) p_2(x) \\propto \\mathcal{N}(x; \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)\n",
    "$$\n",
    "\n",
    "where the parameters of the resulting Gaussian are:\n",
    "\n",
    "$$\n",
    "\\sigma_{\\text{post}}^2 = \\left( \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} \\right)^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left( \\frac{\\mu_1}{\\sigma_1^2} + \\frac{\\mu_2}{\\sigma_2^2} \\right)\n",
    "$$\n",
    "\n",
    "Notice that the **precisions** ($1/\\sigma^2$) add up, and the new mean is a precision-weighted average of the original means.\n",
    "\n",
    "#### Realistic Example: Combining Measurements\n",
    "\n",
    "Imagine you are trying to determine the precise location of an object. You have two independent sensors, each providing a measurement that is a noisy estimate of the object's position. You can model each sensor's measurement uncertainty as a Gaussian distribution centered at the true position.\n",
    "\n",
    "- **Sensor A:** Measurement $y_A$, modeled as $\\mathcal{N}(y_A; \\text{true\\_pos}, \\sigma_A^2)$.\n",
    "- **Sensor B:** Measurement $y_B$, modeled as $\\mathcal{N}(y_B; \\text{true\\_pos}, \\sigma_B^2)$.\n",
    "\n",
    "If you have a prior belief about the object's position, say $p(\\text{true\\_pos}) = \\mathcal{N}(\\text{true\\_pos}; \\mu_{\\text{prior}}, \\sigma_{\\text{prior}}^2)$, the posterior distribution of the true position after observing both measurements is proportional to the prior times the likelihoods:\n",
    "\n",
    "$$\n",
    "p(\\text{true\\_pos} \\mid y_A, y_B) \\propto p(\\text{true\\_pos}) \\, p(y_A \\mid \\text{true\\_pos}) \\, p(y_B \\mid \\text{true\\_pos})\n",
    "$$\n",
    "\n",
    "Since $p(y \\mid \\text{true\\_pos}) = \\mathcal{N}(y; \\text{true\\_pos}, \\sigma^2)$ is symmetric and can be viewed as a Gaussian in the variable $\\text{true\\_pos}$ with mean $y$ and variance $\\sigma^2$, the posterior is a product of three Gaussian densities (one from the prior, two from the likelihoods). The resulting posterior distribution for $\\text{true\\_pos}$ will be Gaussian.\n",
    "\n",
    "Let's implement the combination of two univariate Gaussians in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b395bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: N(mu=50.0, sigma^2=5.0)\n",
      "Sensor Reading: 52.0 (with variance 2.0)\n",
      "Posterior: N(mu=51.43, sigma^2=1.43)\n",
      "\n",
      "Sensor B Reading: 51.0 (with variance 1.5)\n",
      "Posterior after Sensor B: N(mu=51.22, sigma^2=0.73)\n"
     ]
    }
   ],
   "source": [
    "# Function to combine (multiply) two univariate Gaussian densities\n",
    "def combine_univariate_gaussians(mu1, sigma_sq1, mu2, sigma_sq2):\n",
    "    \"\"\"\n",
    "    Combines two univariate Gaussian densities N(x; mu1, sigma_sq1) * N(x; mu2, sigma_sq2).\n",
    "    Returns the parameters (mu_post, sigma_sq_post) of the resulting Gaussian (up to a normalization constant).\n",
    "    \"\"\"\n",
    "    # Calculate precisions (inverse variances)\n",
    "    lambda1 = 1.0 / sigma_sq1\n",
    "    lambda2 = 1.0 / sigma_sq2\n",
    "\n",
    "    # Calculate the precision and variance of the resulting Gaussian\n",
    "    lambda_post = lambda1 + lambda2\n",
    "    sigma_sq_post = 1.0 / lambda_post\n",
    "\n",
    "    # Calculate the mean of the resulting Gaussian\n",
    "    mu_post = sigma_sq_post * (lambda1 * mu1 + lambda2 * mu2)\n",
    "\n",
    "    return mu_post, sigma_sq_post\n",
    "\n",
    "\n",
    "# Example: Combining a prior belief with a sensor reading\n",
    "prior_mu = 50.0  # Prior belief about object position (e.g., meters)\n",
    "prior_sigma_sq = 5.0  # Prior uncertainty\n",
    "\n",
    "sensor_reading = 52.0\n",
    "sensor_noise_sq = 2.0  # Variance of sensor noise\n",
    "# Combine prior and sensor reading\n",
    "# The likelihood N(sensor_reading | true_pos) is treated as a Gaussian in true_pos with mean sensor_reading\n",
    "mu_posterior, sigma_sq_posterior = combine_univariate_gaussians(\n",
    "    prior_mu, prior_sigma_sq, sensor_reading, sensor_noise_sq\n",
    ")\n",
    "\n",
    "print(f\"Prior: N(mu={prior_mu}, sigma^2={prior_sigma_sq})\")\n",
    "print(f\"Sensor Reading: {sensor_reading} (with variance {sensor_noise_sq})\")\n",
    "print(f\"Posterior: N(mu={mu_posterior:.2f}, sigma^2={sigma_sq_posterior:.2f})\")\n",
    "\n",
    "# Notice the posterior mean is a value between the prior mean and the sensor reading,\n",
    "# weighted by their respective precisions. The posterior variance is smaller than both,\n",
    "# indicating increased certainty after incorporating the measurement.\n",
    "\n",
    "# Let's add a second sensor reading\n",
    "sensor_reading_B = 51.0\n",
    "sensor_noise_sq_B = 1.5  # Variance of sensor B noise\n",
    "\n",
    "# Combine the current posterior (after Sensor A) with Sensor B reading\n",
    "mu_posterior_final, sigma_sq_posterior_final = combine_univariate_gaussians(\n",
    "    mu_posterior, sigma_sq_posterior, sensor_reading_B, sensor_noise_sq_B\n",
    ")\n",
    "\n",
    "print(f\"\\nSensor B Reading: {sensor_reading_B} (with variance {sensor_noise_sq_B})\")\n",
    "print(\n",
    "    f\"Posterior after Sensor B: N(mu={mu_posterior_final:.2f}, sigma^2={sigma_sq_posterior_final:.2f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121cec4",
   "metadata": {},
   "source": [
    "## The Multivariate Gaussian Distribution\n",
    "\n",
    "The multivariate Gaussian distribution extends the Gaussian to multiple dimensions. For a random vector $x \\in \\mathbb{R}^n$, the PDF is:\n",
    "\n",
    "$$\n",
    "N(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\right)\n",
    "$$\n",
    "\n",
    "Here, $\\mu \\in \\mathbb{R}^n$ is the mean vector, and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is the covariance matrix. The covariance matrix must be symmetric positive definite (SPD).\n",
    "\n",
    "- **Mean Vector ($\\mu$):** A vector where each element is the mean of the corresponding dimension.\n",
    "- **Covariance Matrix ($\\Sigma$):** A matrix where the diagonal elements are the variances of each dimension, and the off-diagonal elements are the covariances between pairs of dimensions. $\\Sigma_{ij} = \\mathrm{Cov}(x_i, x_j)$.\n",
    "- **Symmetric Positive Definite (SPD):** A matrix $\\Sigma$ is SPD if $\\Sigma = \\Sigma^\\top$ and for any non-zero vector $v \\in \\mathbb{R}^n$, $v^\\top \\Sigma v > 0$. This ensures the quadratic form in the exponent is always non-negative and the distribution is valid.\n",
    "\n",
    "Let's implement the log PDF and sampling for a multivariate Gaussian in JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791dfc7",
   "metadata": {},
   "source": [
    "The core idea behind sampling from a general multivariate Gaussian $\\mathcal{N}(\\mu, \\Sigma)$ is to start with samples from a standard normal distribution $\\mathcal{N}(0, I)$, where $0$ is the zero vector and $I$ is the identity matrix. Samples from $\\mathcal{N}(0, I)$ are simply vectors where each element is drawn independently from a univariate standard normal distribution $\\mathcal{N}(0, 1)$.\n",
    "\n",
    "If $z \\sim \\mathcal{N}(0, I)$, we can transform $z$ to get a sample $x \\sim \\mathcal{N}(\\mu, \\Sigma)$ using a linear transformation:\n",
    "\n",
    "$$x = L z + \\mu\n",
    "$$where $L$ is a matrix such that $\\Sigma = L L^T$. This is because the covariance of $x$ will be $\\text{Cov}(Lz + \\mu) = L \\text{Cov}(z) L^T = L I L^T = L L^T = \\Sigma$. The mean of $x$ is $E[Lz + \\mu] = L E[z] + \\mu = L \\cdot 0 + \\mu = \\mu$. Since linear transformations of Gaussian variables are Gaussian, $x$ will follow the desired distribution.\n",
    "\n",
    "The matrix $L$ can be obtained in a couple of ways, leading to different sampling methods: Cholesky Decomposition or Eigenvalue Decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b545b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Configure JAX for 64-bit precision, which can be useful for numerical stability\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "# We'll generate a new key inside the interactive function to get fresh samples each time parameters change\n",
    "initial_key = jrandom.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90ad2cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c382ccf1f4ca42848c9ed34f35ae3351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Gaussian Parameters:'), HBox(children=(FloatSlider(value=0.0, description='Mu_0:',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Interactive Widgets (Equivalent to Streamlit Controls) ---\n",
    "\n",
    "# Sliders for the mean vector mu = [mu_0, mu_1]\n",
    "mu0_slider = widgets.FloatSlider(\n",
    "    min=-5.0, max=5.0, value=0.0, step=0.1, description=\"Mu_0:\"\n",
    ")\n",
    "mu1_slider = widgets.FloatSlider(\n",
    "    min=-5.0, max=5.0, value=0.0, step=0.1, description=\"Mu_1:\"\n",
    ")\n",
    "\n",
    "# Sliders for the covariance matrix parameters (variances and correlation)\n",
    "# Sigma = [[S11, rho*sqrt(S11*S22)], [rho*sqrt(S11*S22), S22]]\n",
    "s11_slider = widgets.FloatSlider(\n",
    "    min=0.1, max=5.0, value=1.0, step=0.1, description=\"Sigma_11:\"\n",
    ")  # Start variance above 0\n",
    "s22_slider = widgets.FloatSlider(\n",
    "    min=0.1, max=5.0, value=1.0, step=0.1, description=\"Sigma_22:\"\n",
    ")  # Start variance above 0\n",
    "rho_slider = widgets.FloatSlider(\n",
    "    min=-0.99, max=0.99, value=0.0, step=0.01, description=\"rho:\"\n",
    ")  # Correlation coefficient\n",
    "\n",
    "# Slider for the number of samples\n",
    "n_samples_slider = widgets.IntSlider(\n",
    "    min=10,\n",
    "    max=1000,\n",
    "    value=200,\n",
    "    step=10,\n",
    "    description=\"Num Samples:\",\n",
    "    style={\"description_width\": \"initial\"},  # Show full description\n",
    ")\n",
    "\n",
    "# Radio buttons for sampling method selection\n",
    "method_radio = widgets.RadioButtons(\n",
    "    options=[\"Cholesky\", \"Eigenvalue\"], description=\"Sampling Method:\", disabled=False\n",
    ")\n",
    "\n",
    "controls = widgets.VBox(\n",
    "    [\n",
    "        widgets.Label(\"Gaussian Parameters:\"),\n",
    "        widgets.HBox([mu0_slider, mu1_slider]),\n",
    "        widgets.HBox([s11_slider, s22_slider]),\n",
    "        widgets.HBox([rho_slider, n_samples_slider]),\n",
    "        method_radio,\n",
    "    ]\n",
    ")\n",
    "display(controls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128e378",
   "metadata": {},
   "source": [
    "Method 1: Cholesky Decomposition\n",
    "\n",
    "The Cholesky decomposition is a way to factor a symmetric positive definite (SPD) matrix $\\Sigma$ into the product of a lower triangular matrix $L$ and its transpose: $\\Sigma = L L^T$. This is a standard and numerically stable decomposition for SPD matrices. Here’s what this means and why it’s useful:\n",
    "\n",
    "**What is Cholesky Decomposition?**\n",
    "\n",
    "- A matrix is *symmetric* if it equals its transpose ($\\Sigma = \\Sigma^T$), and *positive definite* if $v^T \\Sigma v > 0$ for any nonzero vector $v$.\n",
    "- The Cholesky decomposition guarantees that for any SPD matrix, there exists a unique lower triangular matrix $L$ (with positive diagonal entries) such that $\\Sigma = L L^T$.\n",
    "- The matrix $L$ is easy to compute using efficient numerical algorithms, and it is always well-defined for SPD matrices.\n",
    "\n",
    "**How Does the Cholesky Decomposition Algorithm Work?**\n",
    "\n",
    "Given a symmetric positive definite matrix $\\Sigma$, the Cholesky decomposition finds a unique lower triangular matrix $L$ such that $\\Sigma = L L^T$. The algorithm proceeds as follows:\n",
    "\n",
    "Suppose $\\Sigma$ is an $n \\times n$ matrix with entries $\\Sigma_{ij}$. The entries of $L$ are computed one row at a time using:\n",
    "\n",
    "- For each diagonal entry ($i = j$):\n",
    "    $$\n",
    "    L_{ii} = \\sqrt{\\Sigma_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2}\n",
    "    $$\n",
    "- For each off-diagonal entry ($i > j$):\n",
    "    $$\n",
    "    L_{ij} = \\frac{1}{L_{jj}} \\left( \\Sigma_{ij} - \\sum_{k=1}^{j-1} L_{ik} L_{jk} \\right)\n",
    "    $$\n",
    "- For $i < j$, $L_{ij} = 0$ (since $L$ is lower triangular).\n",
    "\n",
    "**Step-by-step:**\n",
    "1. Start with the first row/column, compute $L_{11} = \\sqrt{\\Sigma_{11}}$.\n",
    "2. For each subsequent row $i$, compute $L_{ij}$ for $j < i$ using the formula above, then compute $L_{ii}$.\n",
    "3. Continue until all entries are filled.\n",
    "\n",
    "**Why does this work?**  \n",
    "At each step, the algorithm ensures that the part of $\\Sigma$ explained so far matches $L L^T$ up to the current row/column. The square root and division steps are valid because $\\Sigma$ is positive definite, so all diagonal elements remain positive.\n",
    "\n",
    "**Numerical Stability:**  \n",
    "Cholesky is more stable and efficient than general matrix decompositions for SPD matrices, and is widely used in scientific computing and machine learning for this reason.\n",
    "\n",
    "**In Practice:**  \n",
    "You rarely need to implement this by hand—libraries like NumPy, SciPy, and JAX provide efficient, optimized routines (e.g., `jnp.linalg.cholesky(Sigma)`). But understanding the algorithm helps explain why it only works for SPD matrices and why it is so efficient.\n",
    "\n",
    "**Why is Cholesky Decomposition Helpful for Sampling?**\n",
    "\n",
    "When we want to sample from a multivariate Gaussian $\\mathcal{N}(\\mu, \\Sigma)$, we need to generate random vectors $x$ such that their covariance matches $\\Sigma$. The trick is to start with samples $z$ from a standard normal distribution $\\mathcal{N}(0, I)$ (which is easy to generate), and then transform them so that they have the desired covariance.\n",
    "\n",
    "- If $z \\sim \\mathcal{N}(0, I)$, then $x = L z + \\mu$ will have mean $\\mu$ and covariance $\\Sigma$.\n",
    "- This works because:\n",
    "    - $\\mathbb{E}[x] = L \\mathbb{E}[z] + \\mu = \\mu$\n",
    "    - $\\text{Cov}(x) = L \\text{Cov}(z) L^T = L I L^T = L L^T = \\Sigma$\n",
    "\n",
    "**Intuition:**  \n",
    "The Cholesky factor $L$ “shapes” the spherical cloud of standard normal samples into the elliptical cloud described by $\\Sigma$. Each sample $z$ is first stretched and rotated by $L$, then shifted by the mean $\\mu$.\n",
    "\n",
    "**Summary:**  \n",
    "Cholesky decomposition is a fast, stable way to generate samples from any multivariate Gaussian, by transforming standard normal samples. It’s widely used in probabilistic modeling, Bayesian inference, and machine learning whenever we need to work with multivariate Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e809920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cholesky_sample(raw_samples, mu, Sigma, circle_pts):\n",
    "    # Cholesky decomposition: Sigma = L @ L.T\n",
    "    L = jnp.linalg.cholesky(Sigma)\n",
    "    # Transform samples and circle: x = L @ z + mu\n",
    "    transformed_samples = jnp.dot(raw_samples, L.T)\n",
    "    shifted_samples = transformed_samples + mu\n",
    "    transformed_circle_pts = jnp.dot(L, circle_pts)\n",
    "    shifted_circle_pts = transformed_circle_pts + mu[:, None]\n",
    "\n",
    "    return (\n",
    "        transformed_samples,\n",
    "        shifted_samples,\n",
    "        transformed_circle_pts,\n",
    "        shifted_circle_pts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0b4cb",
   "metadata": {},
   "source": [
    "Method 2: Eigenvalue Decomposition\n",
    "Eigenvalue decomposition is a fundamental concept in linear algebra, especially useful for understanding the structure of symmetric matrices like covariance matrices in Gaussian distributions.\n",
    "\n",
    "#### What is Eigenvalue Decomposition?\n",
    "\n",
    "Given a symmetric matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$ (such as a covariance matrix), eigenvalue decomposition expresses $\\Sigma$ as:\n",
    "$$\n",
    "\\Sigma = V D V^\\top\n",
    "$$\n",
    "where:\n",
    "- $V$ is an $n \\times n$ orthogonal matrix whose columns are the eigenvectors of $\\Sigma$.\n",
    "- $D$ is a diagonal matrix whose diagonal entries are the eigenvalues of $\\Sigma$.\n",
    "\n",
    "The eigenvectors represent directions in space where the transformation $\\Sigma$ acts as simple scaling, and the eigenvalues tell us how much scaling occurs along each direction.\n",
    "\n",
    "#### How Does the Algorithm Work?\n",
    "\n",
    "1. **Find Eigenvalues and Eigenvectors:**  \n",
    "    For a symmetric matrix $\\Sigma$, solve the equation\n",
    "    $$\n",
    "    \\Sigma v = \\lambda v\n",
    "    $$\n",
    "    for each eigenvector $v$ and its corresponding eigenvalue $\\lambda$.\n",
    "\n",
    "2. **Form the Matrices:**  \n",
    "    - Stack the normalized eigenvectors as columns to form $V$.\n",
    "    - Place the eigenvalues on the diagonal of $D$.\n",
    "\n",
    "3. **Reconstruct the Matrix:**  \n",
    "    The original matrix can be reconstructed as $V D V^\\top$.\n",
    "\n",
    "#### How Does Eigenvalue Decomposition Help with Sampling?\n",
    "\n",
    "To sample from a multivariate Gaussian $\\mathcal{N}(\\mu, \\Sigma)$:\n",
    "1. **Sample $z$ from $\\mathcal{N}(0, I)$:**  \n",
    "    $z$ is a vector of independent standard normal variables.\n",
    "\n",
    "2. **Transform the Samples:**  \n",
    "    Compute $L = V \\sqrt{D}$, where $\\sqrt{D}$ is a diagonal matrix with $\\sqrt{\\lambda_i}$ on the diagonal.  \n",
    "    The sample is then:\n",
    "    $$\n",
    "    x = L z + \\mu = V \\sqrt{D} z + \\mu\n",
    "    $$\n",
    "    This transformation stretches and rotates the standard normal samples to match the covariance structure of $\\Sigma$.\n",
    "\n",
    "#### Comparison: Eigenvalue vs. Cholesky Decomposition\n",
    "\n",
    "- **Cholesky Decomposition:**  \n",
    "  - Only works for symmetric positive definite matrices.\n",
    "  - Decomposes $\\Sigma$ as $L L^\\top$, where $L$ is lower triangular.\n",
    "  - Fast and numerically stable.\n",
    "  - Commonly used for sampling because of its efficiency.\n",
    "\n",
    "- **Eigenvalue Decomposition:**  \n",
    "  - Works for all symmetric matrices (including positive semi-definite).\n",
    "  - Decomposes $\\Sigma$ as $V D V^\\top$.\n",
    "  - Provides insight into the principal directions (eigenvectors) and variances (eigenvalues).\n",
    "  - Can be more numerically sensitive if eigenvalues are close to zero or negative due to rounding errors.\n",
    "\n",
    "#### Which is Better for Sampling?\n",
    "\n",
    "- **Cholesky** is generally preferred for sampling from multivariate Gaussians because it is faster and more numerically stable for positive definite matrices (which all valid covariance matrices should be).\n",
    "- **Eigenvalue decomposition** is useful for understanding the geometry of the distribution (principal axes and variances), and can be used for sampling, especially when the covariance matrix is only positive semi-definite (some eigenvalues may be zero).\n",
    "\n",
    "**Summary Table:**\n",
    "\n",
    "| Method      | Speed      | Stability   | Insight into Geometry | Handles Semi-definite |\n",
    "|-------------|------------|-------------|----------------------|----------------------|\n",
    "| Cholesky    | Fast       | Very good   | No                   | No                   |\n",
    "| Eigenvalue  | Slower     | Can be less | Yes                  | Yes                  |\n",
    "\n",
    "In practice, use Cholesky for efficient sampling, and eigenvalue decomposition when you want to analyze or visualize the principal directions and variances of your Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba799e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigenvalue_sample(raw_samples, mu, Sigma, circle_pts):\n",
    "    # Eigenvalue decomposition: Sigma = V @ D @ V.T\n",
    "    D_diag, V = jnp.linalg.eigh(Sigma)  # Eigenvalues and eigenvectors\n",
    "    # Ensure eigenvalues are non-negative for sqrt\n",
    "    D_sqrt = jnp.diag(jnp.sqrt(jnp.maximum(0.0, D_diag)))\n",
    "    L_eigen = jnp.dot(V, D_sqrt)  # L = V @ sqrt(D)\n",
    "    # Transform samples and circle: x = L_eigen @ z + mu\n",
    "    transformed_samples = jnp.dot(raw_samples, L_eigen.T)\n",
    "    shifted_samples = transformed_samples + mu\n",
    "    transformed_circle_pts = jnp.dot(L_eigen, circle_pts)\n",
    "    shifted_circle_pts = transformed_circle_pts + mu[:, None]\n",
    "\n",
    "    return (\n",
    "        transformed_samples,\n",
    "        shifted_samples,\n",
    "        transformed_circle_pts,\n",
    "        shifted_circle_pts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53768df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot(mu_0, mu_1, S11, S22, rho, N_samples, sampling_method):\n",
    "    \"\"\"\n",
    "    Updates the plot based on the selected parameters and sampling method.\n",
    "    \"\"\"\n",
    "    # Construct the mean vector and covariance matrix from widget values\n",
    "    mu = jnp.asarray([mu_0, mu_1])\n",
    "    # Ensure S11 and S22 are positive for sqrt\n",
    "    s11_safe = jnp.maximum(S11, 1e-6)\n",
    "    s22_safe = jnp.maximum(S22, 1e-6)\n",
    "    # Ensure rho is within valid range [-1, 1] for correlation\n",
    "    rho_safe = jnp.clip(rho, -0.999, 0.999)\n",
    "\n",
    "    S12 = rho_safe * jnp.sqrt(s11_safe * s22_safe)\n",
    "    Sigma = jnp.asarray([[s11_safe, S12], [S12, s22_safe]])\n",
    "\n",
    "    # Regenerate key for fresh samples each time\n",
    "    global initial_key\n",
    "    initial_key, subkey = jrandom.split(initial_key)\n",
    "\n",
    "    raw_samples = jrandom.normal(\n",
    "        subkey, shape=(N_samples, 2)\n",
    "    )  # Raw samples from N(0, I)\n",
    "\n",
    "    transformed_samples = None\n",
    "    shifted_samples = None\n",
    "    transformed_circle_pts = None\n",
    "    shifted_circle_pts = None\n",
    "    method_title = \"\"\n",
    "    error_message = None\n",
    "\n",
    "    # Circles for visualization (unit circle)\n",
    "    theta = jnp.linspace(0, 2 * jnp.pi, 100)\n",
    "    circle_pts = jnp.stack([jnp.cos(theta), jnp.sin(theta)])  # Unit circle\n",
    "\n",
    "    # Perform sampling based on the selected method\n",
    "    try:\n",
    "        if sampling_method == \"Cholesky\":\n",
    "            (\n",
    "                transformed_samples,\n",
    "                shifted_samples,\n",
    "                transformed_circle_pts,\n",
    "                shifted_circle_pts,\n",
    "            ) = cholesky_sample(raw_samples, mu, Sigma, circle_pts)\n",
    "            method_title = \"Cholesky Decomposition\"\n",
    "\n",
    "        elif sampling_method == \"Eigenvalue\":\n",
    "            (\n",
    "                transformed_samples,\n",
    "                shifted_samples,\n",
    "                transformed_circle_pts,\n",
    "                shifted_circle_pts,\n",
    "            ) = eigenvalue_sample(raw_samples, mu, Sigma, circle_pts)\n",
    "            method_title = \"Eigenvalue Decomposition\"\n",
    "\n",
    "    except jnp.linalg.LinAlgError:\n",
    "        error_message = (\n",
    "            \"Error: Covariance matrix is not positive definite. Adjust parameters.\"\n",
    "        )\n",
    "        print(error_message)  # Print to console for debugging\n",
    "\n",
    "    # --- Plotting using Plotly ---\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if shifted_samples is not None:\n",
    "        # Add traces for samples\n",
    "        fig.add_trace(\n",
    "            go.Scattergl(\n",
    "                x=np.array(raw_samples[:, 0]),\n",
    "                y=np.array(raw_samples[:, 1]),\n",
    "                mode=\"markers\",\n",
    "                name=\"Raw Samples (N(0, I))\",\n",
    "                marker=dict(size=5, opacity=0.6, color=\"gray\"),\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scattergl(\n",
    "                x=np.array(transformed_samples[:, 0]),\n",
    "                y=np.array(transformed_samples[:, 1]),\n",
    "                mode=\"markers\",\n",
    "                name=\"Transformed Samples (L @ z)\",\n",
    "                marker=dict(size=5, opacity=0.6, color=\"blue\"),\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scattergl(\n",
    "                x=np.array(shifted_samples[:, 0]),\n",
    "                y=np.array(shifted_samples[:, 1]),\n",
    "                mode=\"markers\",\n",
    "                name=\"Shifted Samples (L @ z + mu)\",\n",
    "                marker=dict(size=5, opacity=0.6, color=\"red\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add traces for circles\n",
    "        fig.add_trace(\n",
    "            go.Scattergl(\n",
    "                x=np.array(circle_pts[0, :]),\n",
    "                y=np.array(circle_pts[1, :]),\n",
    "                mode=\"lines\",\n",
    "                name=\"Unit Circle\",\n",
    "                line=dict(dash=\"dash\", color=\"gray\"),\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scattergl(\n",
    "                x=np.array(transformed_circle_pts[0, :]),\n",
    "                y=np.array(transformed_circle_pts[1, :]),\n",
    "                mode=\"lines\",\n",
    "                name=\"Transformed Circle\",\n",
    "                line=dict(dash=\"dash\", color=\"blue\"),\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scattergl(\n",
    "                x=np.array(shifted_circle_pts[0, :]),\n",
    "                y=np.array(shifted_circle_pts[1, :]),\n",
    "                mode=\"lines\",\n",
    "                name=\"Shifted Circle\",\n",
    "                line=dict(dash=\"dash\", color=\"red\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f\"Gaussian Sampling via {method_title}\",\n",
    "            xaxis_title='<span class=\"math-inline\">x1</span>',\n",
    "            yaxis_title='<span class=\"math-inline\">x2</span>',\n",
    "            xaxis=dict(scaleanchor=\"y\", scaleratio=1),  # Ensure equal aspect ratio\n",
    "            yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "            hovermode=\"closest\",\n",
    "            showlegend=True,\n",
    "            width=1200,  # Set a fixed width for the plot\n",
    "            height=600,  # Set a fixed height for the plot\n",
    "        )\n",
    "\n",
    "        # Set limits dynamically based on the shifted samples to ensure they are visible\n",
    "        min_vals = jnp.min(shifted_samples, axis=0)\n",
    "        max_vals = jnp.max(shifted_samples, axis=0)\n",
    "        range_vals = jnp.max(\n",
    "            max_vals - min_vals\n",
    "        )  # Find the largest range in either dimension\n",
    "        center = jnp.mean(shifted_samples, axis=0)\n",
    "\n",
    "        # Set limits to be centered around the mean, covering a bit more than the sample range\n",
    "        padding = range_vals * 0.3  # Add 30% padding\n",
    "        x_range = [\n",
    "            float(center[0] - range_vals / 2 - padding),\n",
    "            float(center[0] + range_vals / 2 + padding),\n",
    "        ]\n",
    "        y_range = [\n",
    "            float(center[1] - range_vals / 2 - padding),\n",
    "            float(center[1] + range_vals / 2 + padding),\n",
    "        ]\n",
    "\n",
    "        fig.update_layout(xaxis_range=x_range, yaxis_range=y_range)\n",
    "\n",
    "    else:\n",
    "        # Display an empty plot or an error message if sampling failed\n",
    "        fig.update_layout(title=error_message if error_message else \"Plotting Error\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd2f9e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16067dfee0874404a59d8a943abd0efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Gaussian Parameters:'), HBox(children=(FloatSlider(value=0.0, description='Mu_0:',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8677d87cb9574bd68173007eba7fcbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Link widgets to the update function ---\n",
    "\n",
    "# Use interactive_output to link widget values to the function's arguments\n",
    "interactive_plot = widgets.interactive_output(\n",
    "    update_plot,\n",
    "    {\n",
    "        \"mu_0\": mu0_slider,\n",
    "        \"mu_1\": mu1_slider,\n",
    "        \"S11\": s11_slider,\n",
    "        \"S22\": s22_slider,\n",
    "        \"rho\": rho_slider,\n",
    "        \"N_samples\": n_samples_slider,\n",
    "        \"sampling_method\": method_radio,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Arrange widgets and the plot output\n",
    "controls = widgets.VBox(\n",
    "    [\n",
    "        widgets.Label(\"Gaussian Parameters:\"),\n",
    "        widgets.HBox([mu0_slider, mu1_slider]),\n",
    "        widgets.HBox([s11_slider, s22_slider]),\n",
    "        widgets.HBox([rho_slider, n_samples_slider]),\n",
    "        method_radio,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the controls and the plot\n",
    "display(controls)\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0f0fa",
   "metadata": {},
   "source": [
    "Visualizing the Sampling Process\n",
    "\n",
    "The plot above shows the transformation steps:\n",
    "\n",
    "* Raw Samples (Gray): These are the initial samples drawn from the standard normal distribution $\\mathcal{N}(0, I)$. They form a spherical cloud centered at the origin.\n",
    "\n",
    "* Transformed Samples (Blue): These samples are obtained by multiplying the raw samples by the matrix $L$ (from either Cholesky or Eigenvalue decomposition). This step scales and rotates the spherical cloud according to the covariance structure.\n",
    "\n",
    "* Shifted Samples (Red): These are the final samples, obtained by adding the mean vector $\\mu$ to the transformed samples. This shifts the center of the elliptical cloud to the mean $\\mu$.\n",
    "\n",
    "The circles similarly show how the unit circle (gray) is transformed by $L$ (blue) and then shifted by $\\mu$ (red), illustrating the shape and location of the resulting Gaussian distribution.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Sampling from a multivariate Gaussian distribution is a fundamental technique. We learned that it can be done by transforming samples from a standard normal distribution using a matrix derived from the covariance matrix (via Cholesky or Eigenvalue decomposition) and shifting by the mean $\\mu$. This interactive visualization helps demonstrate how the mean and covariance matrix completely define the location and shape of the Gaussian distribution.\n",
    "\n",
    "Understanding these sampling methods provides a deeper insight into the structure of Gaussian distributions and their role in probabilistic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964fa796",
   "metadata": {},
   "source": [
    "### Closure Properties: The Magic of Gaussians\n",
    "\n",
    "One of the most powerful aspects of Gaussian distributions is their closure properties under linear operations. This means that if you start with Gaussian random variables and perform certain operations (like adding them, applying linear transformations, looking at subsets, or conditioning on some values), the resulting distributions are still Gaussian. This makes exact inference possible using linear algebra.\n",
    "\n",
    "#### 1. Products of Gaussians are Gaussians\n",
    "\n",
    "The product of two Gaussian probability densities is proportional to another Gaussian density. This is crucial for Bayesian inference because the posterior is proportional to the prior times the likelihood. If both are Gaussian, the posterior is also Gaussian.\n",
    "\n",
    "**Univariate Case:**\n",
    "\n",
    "If $p_1(x) = \\mathcal{N}(x; \\mu_1, \\sigma_1^2)$ and $p_2(x) = \\mathcal{N}(x; \\mu_2, \\sigma_2^2)$, their product is:\n",
    "\n",
    "$$\n",
    "p_1(x) p_2(x) \\propto \\mathcal{N}(x; \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)\n",
    "$$\n",
    "\n",
    "where the parameters of the resulting Gaussian are:\n",
    "\n",
    "$$\n",
    "\\sigma_{\\text{post}}^2 = \\left(\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left(\\frac{\\mu_1}{\\sigma_1^2} + \\frac{\\mu_2}{\\sigma_2^2}\\right)\n",
    "$$\n",
    "\n",
    "Notice that the precisions ($1/\\sigma^2$) add up, and the new mean is a precision-weighted average of the original means.\n",
    "\n",
    "**Multivariate Case:**\n",
    "\n",
    "If $p_1(x) = \\mathcal{N}(x; \\mu_1, \\Sigma_1)$ and $p_2(x) = \\mathcal{N}(x; \\mu_2, \\Sigma_2)$ are two multivariate Gaussian densities over the same variable $x \\in \\mathbb{R}^n$, their product is proportional to another multivariate Gaussian density:\n",
    "\n",
    "$$\n",
    "p_1(x) p_2(x) \\propto \\mathcal{N}(x; \\mu_{\\text{post}}, \\Sigma_{\\text{post}})\n",
    "$$\n",
    "\n",
    "The parameters of the resulting Gaussian are given by:\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\text{post}}^{-1} = \\Sigma_1^{-1} + \\Sigma_2^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\text{post}}^{-1} \\mu_{\\text{post}} = \\Sigma_1^{-1} \\mu_1 + \\Sigma_2^{-1} \\mu_2\n",
    "$$\n",
    "\n",
    "These formulas generalize the univariate case: the precision matrices (inverse covariance matrices) add, and the precision-weighted means add.\n",
    "\n",
    "**Realistic Example: Combining Sensor Measurements for 2D Position**\n",
    "\n",
    "Let's extend the sensor fusion example to 2D. Imagine you're tracking an object's position in a 2D plane $(x, y)$. You have a prior belief about its position, modeled as a 2D Gaussian $p(\\text{pos}) = \\mathcal{N}(\\text{pos}; \\mu_{\\text{prior}}, \\Sigma_{\\text{prior}})$. You receive measurements from two sensors, each providing a noisy 2D position estimate.\n",
    "\n",
    "- **Sensor A**: Measurement $\\mathbf{y}_A \\in \\mathbb{R}^2$, modeled as $\\mathcal{N}(\\mathbf{y}_A; \\text{true\\_pos}, \\Sigma_A)$. The likelihood $p(\\mathbf{y}_A|\\text{true\\_pos})$ is $\\mathcal{N}(\\mathbf{y}_A; \\text{true\\_pos}, \\Sigma_A)$.\n",
    "\n",
    "- **Sensor B**: Measurement $\\mathbf{y}_B \\in \\mathbb{R}^2$, modeled as $\\mathcal{N}(\\mathbf{y}_B; \\text{true\\_pos}, \\Sigma_B)$. The likelihood $p(\\mathbf{y}_B|\\text{true\\_pos})$ is $\\mathcal{N}(\\mathbf{y}_B; \\text{true\\_pos}, \\Sigma_B)$.\n",
    "\n",
    "The posterior distribution of the true position after observing both measurements is proportional to the prior times the likelihoods:\n",
    "\n",
    "$$\n",
    "p(\\text{true\\_pos} | \\mathbf{y}_A, \\mathbf{y}_B) \\propto p(\\text{true\\_pos}) p(\\mathbf{y}_A|\\text{true\\_pos}) p(\\mathbf{y}_B|\\text{true\\_pos}).\n",
    "$$\n",
    "\n",
    "Similar to the univariate case, $p(\\mathbf{y}|\\text{true\\_pos}) = \\mathcal{N}(\\mathbf{y}; \\text{true\\_pos}, \\Sigma)$ can be viewed as a Gaussian in the variable $\\text{true\\_pos}$ with mean $\\mathbf{y}$ and covariance $\\Sigma$. The posterior is a product of three multivariate Gaussian densities (prior and two likelihoods), resulting in a new multivariate Gaussian for $\\text{true\\_pos}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c23fd63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: N(mu=[10. 20.], Sigma=\n",
      "[[5. 1.]\n",
      " [1. 5.]])\n",
      "Sensor A Reading: [11. 21.] (with Sigma=\n",
      "[[1.  0.2]\n",
      " [0.2 1. ]])\n",
      "Posterior after Sensor A: N(mu=[10.833333 20.833334], Sigma=\n",
      "[[0.8333333  0.16666669]\n",
      " [0.16666669 0.8333333 ]])\n",
      "\n",
      "Sensor B Reading: [10.5 20.8] (with Sigma=\n",
      "[[1.2 0.1]\n",
      " [0.1 1.2]])\n",
      "Posterior after Sensor B: N(mu=[10.697018 20.810226], Sigma=\n",
      "[[0.49015585 0.07506154]\n",
      " [0.07506154 0.49015588]])\n"
     ]
    }
   ],
   "source": [
    "# Function to combine (multiply) two multivariate Gaussian densities\n",
    "def combine_multivariate_gaussians(mu1, Sigma1, mu2, Sigma2):\n",
    "    \"\"\"\n",
    "    Combines two multivariate Gaussian densities N(x; mu1, Sigma1) * N(x; mu2, Sigma2).\n",
    "    Returns the parameters (mu_post, Sigma_post) of the resulting Gaussian (up to a normalization constant).\n",
    "    \"\"\"\n",
    "    # Calculate precision matrices (inverse covariances)\n",
    "    # Use jnp.linalg.inv for matrix inversion\n",
    "    Sigma1_inv = jnp.linalg.inv(Sigma1)\n",
    "    Sigma2_inv = jnp.linalg.inv(Sigma2)\n",
    "\n",
    "    # Calculate the precision matrix and covariance matrix of the resulting Gaussian\n",
    "    Sigma_post_inv = Sigma1_inv + Sigma2_inv\n",
    "    Sigma_post = jnp.linalg.inv(Sigma_post_inv)\n",
    "\n",
    "    # Calculate the mean of the resulting Gaussian\n",
    "    mu_post = jnp.dot(Sigma_post, jnp.dot(Sigma1_inv, mu1) + jnp.dot(Sigma2_inv, mu2))\n",
    "\n",
    "    return mu_post, Sigma_post\n",
    "\n",
    "\n",
    "# Example: Combining a prior belief with a 2D sensor reading\n",
    "prior_mu_2d = jnp.array([10.0, 20.0])  # Prior belief about 2D position\n",
    "prior_Sigma_2d = jnp.array([[5.0, 1.0], [1.0, 5.0]])  # Prior uncertainty (covariance)\n",
    "\n",
    "sensor_reading_2d = jnp.array([11.0, 21.0])  # Sensor A reading\n",
    "sensor_noise_Sigma_2d = jnp.array(\n",
    "    [[1.0, 0.2], [0.2, 1.0]]\n",
    ")  # Covariance of sensor A noise\n",
    "\n",
    "# Combine prior and sensor reading A\n",
    "# The likelihood N(sensor_reading_2d | true_pos) is treated as a Gaussian in true_pos with mean sensor_reading_2d\n",
    "mu_prior_sA, Sigma_prior_sA = combine_multivariate_gaussians(\n",
    "    prior_mu_2d, prior_Sigma_2d, sensor_reading_2d, sensor_noise_Sigma_2d\n",
    ")\n",
    "\n",
    "print(f\"Prior: N(mu={prior_mu_2d}, Sigma=\\n{prior_Sigma_2d})\")\n",
    "print(f\"Sensor A Reading: {sensor_reading_2d} (with Sigma=\\n{sensor_noise_Sigma_2d})\")\n",
    "print(f\"Posterior after Sensor A: N(mu={mu_prior_sA}, Sigma=\\n{Sigma_prior_sA})\")\n",
    "\n",
    "# Let's add a second sensor reading\n",
    "sensor_reading_2d_B = jnp.array([10.5, 20.8])  # Sensor B reading\n",
    "sensor_noise_Sigma_2d_B = jnp.array(\n",
    "    [[1.2, 0.1], [0.1, 1.2]]\n",
    ")  # Covariance of sensor B noise\n",
    "\n",
    "# Combine the current posterior (after Sensor A) with Sensor B reading\n",
    "mu_posterior_final, Sigma_posterior_final = combine_multivariate_gaussians(\n",
    "    mu_prior_sA, Sigma_prior_sA, sensor_reading_2d_B, sensor_noise_Sigma_2d_B\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nSensor B Reading: {sensor_reading_2d_B} (with Sigma=\\n{sensor_noise_Sigma_2d_B})\"\n",
    ")\n",
    "print(\n",
    "    f\"Posterior after Sensor B: N(mu={mu_posterior_final}, Sigma=\\n{Sigma_posterior_final})\"\n",
    ")\n",
    "\n",
    "# Notice how the posterior mean moves towards the sensor readings, weighted by their precision.\n",
    "# The diagonal elements of the posterior covariance matrix (variances) decrease, indicating reduced uncertainty in both dimensions.\n",
    "# The off-diagonal elements (covariances) also update based on the relationships in the prior and likelihood covariances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c378504",
   "metadata": {},
   "source": [
    "### 2. Linear Transformations of Gaussians are Gaussians\n",
    "\n",
    "If $z \\sim \\mathcal{N}(\\mu, \\Sigma)$ is an $n$-dimensional Gaussian vector and $y = Az + b$ is a linear transformation, where $A$ is an $m \\times n$ matrix and $b$ is an $m$-dimensional vector, then the resulting $m$-dimensional vector $y$ is also Gaussian:\n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{N}(A\\mu + b,\\, A\\Sigma A^\\top)\n",
    "$$\n",
    "\n",
    "- The mean is the transformed mean: $A\\mu + b$\n",
    "- The covariance is transformed by $A$ and $A^\\top$: $A\\Sigma A^\\top$\n",
    "\n",
    "**Realistic Example: Predicting Related Variables**\n",
    "\n",
    "Suppose you have a Gaussian model of a person's height and weight ($z = [\\text{height},\\, \\text{weight}]^\\top \\sim \\mathcal{N}(\\mu_z, \\Sigma_z)$). You are interested in predicting a new variable, say, their estimated calorie intake based on a simple linear model:\n",
    "\n",
    "$$\n",
    "\\text{calories} = a \\cdot \\text{height} + b \\cdot \\text{weight} + c\n",
    "$$\n",
    "\n",
    "This is a linear transformation $y = Az + b$, where $A = [a,\\, b]$ and $b = [c]$. The distribution of estimated calorie intake will be Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c32b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Gaussian (Height, Weight): mu = [175.  75.], Sigma = [[60. 25.]\n",
      " [25. 40.]]\n",
      "Linear Transformation for Calories: A = [[ 5. 10.]], b = [-500.]\n",
      "Transformed Gaussian (Estimated Calories): mu_y = [1125.], Sigma_y = [[8000.]]\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear transformation of a 2D Gaussian\n",
    "mu_z = jnp.array([175.0, 75.0])  # Mean height (cm), mean weight (kg)\n",
    "Sigma_z = jnp.array([[60.0, 25.0], [25.0, 40.0]])  # Covariance matrix\n",
    "\n",
    "# Define a linear transformation for estimated calorie intake\n",
    "# Estimated Calories = 5 * height + 10 * weight - 500\n",
    "A = jnp.array([[5.0, 10.0]])  # Matrix A (1x2)\n",
    "b = jnp.array([-500.0])  # Vector b (1,)\n",
    "\n",
    "# Calculate the mean and covariance of the transformed variable y (estimated calories)\n",
    "mu_y = jnp.dot(A, mu_z) + b\n",
    "Sigma_y = jnp.dot(jnp.dot(A, Sigma_z), A.T)\n",
    "\n",
    "print(f\"\\nOriginal Gaussian (Height, Weight): mu = {mu_z}, Sigma = {Sigma_z}\")\n",
    "print(f\"Linear Transformation for Calories: A = {A}, b = {b}\")\n",
    "print(f\"Transformed Gaussian (Estimated Calories): mu_y = {mu_y}, Sigma_y = {Sigma_y}\")\n",
    "\n",
    "# The distribution of estimated calorie intake is Gaussian N(mu_y, Sigma_y). Sigma_y will be a 1x1 matrix (a scalar variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5787a598",
   "metadata": {},
   "source": [
    "### 3. Marginals of Gaussians are Gaussians\n",
    "\n",
    "If a Gaussian random vector $z$ is partitioned into two sub-vectors, $z=\\begin{bmatrix}x \\\\ y\\end{bmatrix}$, where $x \\in \\mathbb{R}^m$ and $y \\in \\mathbb{R}^k$ (so $n=m+k$), and its distribution is $z \\sim \\mathcal{N}\\left(\\begin{bmatrix}\\mu_x \\\\ \\mu_y\\end{bmatrix}, \\begin{bmatrix}\\Sigma_{xx} & \\Sigma_{xy} \\\\ \\Sigma_{yx} & \\Sigma_{yy}\\end{bmatrix}\\right)$, then the marginal distribution of $x$ (obtained by integrating out $y$) is also Gaussian:\n",
    "\n",
    "$$\n",
    "p(x) = \\int p(x, y) \\, dy = \\mathcal{N}(\\mu_x, \\Sigma_{xx})\n",
    "$$\n",
    "\n",
    "The marginal distribution of any subset of variables in a multivariate Gaussian is simply a Gaussian with the corresponding sub-vector of the mean and the corresponding submatrix of the covariance matrix.\n",
    "\n",
    "#### Realistic Example: Focusing on a Subset of Health Metrics\n",
    "\n",
    "Suppose you have collected various health metrics from patients and modeled their joint distribution as a multivariate Gaussian. These metrics might include blood pressure, heart rate, cholesterol level, and blood sugar. If you only want to analyze the distribution of blood pressure and heart rate, their marginal joint distribution is a 2D Gaussian. Its mean vector is just the means of blood pressure and heart rate from the full mean vector, and its covariance matrix is the $2 \\times 2$ submatrix of the full covariance matrix corresponding to blood pressure and heart rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc7903d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original 4D Gaussian (Health Metrics): \n",
      " mu = [120.  70. 180.  90.]\n",
      "Original Sigma:\n",
      " [[10.   3.   1.   0.5]\n",
      " [ 3.   5.   0.8  0.3]\n",
      " [ 1.   0.8 20.   5. ]\n",
      " [ 0.5  0.3  5.   8. ]]\n",
      "Marginal distribution of Blood Pressure and Cholesterol (indices \n",
      " [0 2]):\n",
      "  Marginal Mean:\n",
      " [120. 180.]\n",
      "  Marginal Sigma:\n",
      " [[10.  1.]\n",
      " [ 1. 20.]]\n"
     ]
    }
   ],
   "source": [
    "# Example: Marginalizing a 4D Gaussian\n",
    "# Let z = [BP, HR, Cholesterol, BloodSugar]^T be a 4D Gaussian\n",
    "mu_z_full = jnp.array([120.0, 70.0, 180.0, 90.0])  # Example mean vector\n",
    "Sigma_z_full = jnp.array(\n",
    "    [\n",
    "        [10.0, 3.0, 1.0, 0.5],\n",
    "        [3.0, 5.0, 0.8, 0.3],\n",
    "        [1.0, 0.8, 20.0, 5.0],\n",
    "        [0.5, 0.3, 5.0, 8.0],\n",
    "    ]\n",
    ")  # Example 4x4 SPD covariance matrix\n",
    "\n",
    "# We want the marginal distribution of Blood Pressure (index 0) and Cholesterol (index 2).\n",
    "# This corresponds to selecting dimensions 0 and 2.\n",
    "selected_indices = jnp.array([0, 2])\n",
    "\n",
    "# The marginal mean is the subset of the mean vector\n",
    "mu_marginal = mu_z_full[selected_indices]\n",
    "\n",
    "# The marginal covariance is the submatrix corresponding to the selected indices\n",
    "Sigma_marginal = Sigma_z_full[jnp.ix_(selected_indices, selected_indices)]\n",
    "\n",
    "print(f\"\\nOriginal 4D Gaussian (Health Metrics): \\n mu = {mu_z_full}\")\n",
    "print(\"Original Sigma:\\n\", Sigma_z_full)\n",
    "print(\n",
    "    f\"Marginal distribution of Blood Pressure and Cholesterol (indices \\n {selected_indices}):\"\n",
    ")\n",
    "print(f\"  Marginal Mean:\\n {mu_marginal}\")\n",
    "print(\"  Marginal Sigma:\\n\", Sigma_marginal)\n",
    "\n",
    "# The marginal distribution p([BP, Cholesterol]) is Gaussian N(mu_marginal, Sigma_marginal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282ddd6",
   "metadata": {},
   "source": [
    "### 4. Conditionals of Gaussians are Gaussians\n",
    "\n",
    "If a Gaussian random vector $z=\\begin{bmatrix}x \\\\ y\\end{bmatrix}$ is partitioned as above, the conditional distribution of $x$ given that $y$ takes a specific observed value is also Gaussian:\n",
    "\n",
    "$$\n",
    "p(x \\mid y) = \\mathcal{N}(\\mu_{x \\mid y}, \\Sigma_{x \\mid y})\n",
    "$$\n",
    "\n",
    "The parameters of the conditional distribution are given by:\n",
    "\n",
    "$$\n",
    "\\mu_{x \\mid y} = \\mu_x + \\Sigma_{xy} \\Sigma_{yy}^{-1} (y - \\mu_y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{x \\mid y} = \\Sigma_{xx} - \\Sigma_{xy} \\Sigma_{yy}^{-1} \\Sigma_{yx}\n",
    "$$\n",
    "\n",
    "These formulas are derived from the structure of the multivariate Gaussian PDF. $\\Sigma_{yy}^{-1}$ is the inverse of the covariance matrix of the observed variables $y$.\n",
    "\n",
    "#### Realistic Example: Predicting Unobserved Variables Based on Observations\n",
    "\n",
    "This property is fundamental for prediction and state estimation.\n",
    "\n",
    "- **Predicting missing data**: If you have a joint Gaussian model of several variables and you've only observed a subset of them ($y$), you can predict the distribution of the unobserved variables ($x$) using the conditional distribution $p(x \\mid y)$. The conditional mean $\\mu_{x \\mid y}$ gives you the best linear prediction of $x$ given $y$, and the conditional covariance $\\Sigma_{x \\mid y}$ quantifies the uncertainty in this prediction.\n",
    "\n",
    "- **Kalman Filter**: The Kalman filter, a widely used algorithm for state estimation in dynamic systems, relies heavily on the fact that if the system dynamics are linear and noise is Gaussian, the posterior distribution of the system state remains Gaussian and can be updated analytically using these conditional formulas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69435a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original 2D Gaussian: mu = [1. 2.], Sigma = [[1.  0.5]\n",
      " [0.5 1. ]]\n",
      "Observed y = 2.5\n",
      "Conditional distribution p(x | y=2.5):\n",
      "  Conditional Mean (mu_x|y): 1.25\n",
      "  Conditional Variance (Sigma_x|y): 0.75\n"
     ]
    }
   ],
   "source": [
    "# Example: Conditioning a 2D Gaussian\n",
    "# Let z = [x, y]^T be a 2D Gaussian\n",
    "mu_z_cond = jnp.array([1.0, 2.0])  # mu_x, mu_y\n",
    "Sigma_z_cond = jnp.array(\n",
    "    [[1.0, 0.5], [0.5, 1.0]]\n",
    ")  # Sigma_xx, Sigma_xy, Sigma_yx, Sigma_yy\n",
    "\n",
    "# Partition the mean and covariance for scalar x and scalar y\n",
    "mu_x = mu_z_cond[0]\n",
    "mu_y = mu_z_cond[1]\n",
    "Sigma_xx = Sigma_z_cond[0, 0]\n",
    "Sigma_xy = Sigma_z_cond[0, 1]\n",
    "Sigma_yx = Sigma_z_cond[1, 0]  # For scalars, Sigma_yx = Sigma_xy\n",
    "Sigma_yy = Sigma_z_cond[1, 1]\n",
    "\n",
    "# Assume we observe y = 2.5\n",
    "observed_y = 2.5\n",
    "\n",
    "# Calculate the conditional mean and variance of x given y\n",
    "# Need Sigma_yy_inv (which is just 1/Sigma_yy for a scalar)\n",
    "Sigma_yy_inv = 1.0 / Sigma_yy\n",
    "\n",
    "mu_x_given_y = mu_x + Sigma_xy * Sigma_yy_inv * (observed_y - mu_y)\n",
    "Sigma_x_given_y = Sigma_xx - Sigma_xy * Sigma_yy_inv * Sigma_yx\n",
    "\n",
    "print(f\"\\nOriginal 2D Gaussian: mu = {mu_z_cond}, Sigma = {Sigma_z_cond}\")\n",
    "print(f\"Observed y = {observed_y}\")\n",
    "print(f\"Conditional distribution p(x | y={observed_y}):\")\n",
    "print(f\"  Conditional Mean (mu_x|y): {mu_x_given_y}\")\n",
    "print(f\"  Conditional Variance (Sigma_x|y): {Sigma_x_given_y}\")\n",
    "\n",
    "# The conditional distribution p(x | y=observed_y) is Gaussian N(mu_x_given_y, Sigma_x_given_y).\n",
    "# The conditional mean is adjusted from the marginal mean mu_x based on how far the observation y is from its mean mu_y, scaled by the covariance Sigma_xy and the inverse variance of y.\n",
    "# The conditional variance Sigma_x|y is less than or equal to the marginal variance Sigma_xx, reflecting reduced uncertainty about x after observing y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044ad64",
   "metadata": {},
   "source": [
    "### Gaussian Inference is Linear Algebra\n",
    "\n",
    "The core insight from these closure properties is that Gaussian inference is fundamentally linear algebra. If all random variables in your model are jointly Gaussian and their relationships are linear, then any marginal or conditional distribution you want to compute will also be Gaussian, and its mean and covariance can be found by applying linear algebra operations (matrix multiplication, inversion, addition) to the means and covariances of the original joint distribution.\n",
    "\n",
    "This makes Gaussian models highly tractable for inference, even in high dimensions, provided you can handle the necessary matrix computations.\n",
    "\n",
    "The slides summarize this with the theorem:\n",
    "\n",
    "If $p(x) = \\mathcal{N}(x; \\mu, \\Sigma)$ and $p(y \\mid x) = \\mathcal{N}(y; Ax + b, \\Lambda)$, then the posterior $p(x \\mid y)$ is Gaussian $\\mathcal{N}(x; \\mu_{\\text{post}}, \\Sigma_{\\text{post}})$ with the update equations we saw for conditioning. These equations are just matrix manipulations of the parameters $\\mu, \\Sigma, A, b, \\Lambda$.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Lecture 06 established the Gaussian distribution as a cornerstone of probabilistic machine learning due to its remarkable closure properties under linear operations:\n",
    "\n",
    "- Products of Gaussian densities are proportional to Gaussian densities.\n",
    "- Linear transformations of Gaussian variables result in Gaussian variables.\n",
    "- Marginal distributions of Gaussian variables are Gaussian.\n",
    "- Linear conditional distributions of Gaussian variables are Gaussian.\n",
    "\n",
    "These properties imply that for linear Gaussian models, Bayesian inference is analytically tractable and can be performed using linear algebra. This is a powerful tool for modeling and inference in many real-world applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
