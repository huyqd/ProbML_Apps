{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning: Lecture 15 - Gaussian Process Classification (Laplace Approximation) with JAX and Plotly\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Welcome to Lecture 15 on Gaussian Process Classification! This notebook builds directly on our previous discussion (Lecture 14) about Logistic Regression and the initial introduction to the Laplace Approximation. While Lecture 14 focused on understanding the problem of non-Gaussian posteriors and finding the Maximum A Posteriori (MAP) estimate using Newton's method, this lecture dives deeper into the *full* Laplace Approximation.\n",
    "\n",
    "We will explore the Hessian matrix in more detail, understand its role in quantifying uncertainty, and see how to compute predictive probabilities for new data points. All computations will leverage **JAX** for efficiency, and visualizations will be interactive using **Plotly**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Recap: Gaussian Process Classification & Non-Gaussian Posterior\n",
    "\n",
    "As established in Lecture 14, Gaussian Process Classification models binary outputs (e.g., $y \\in \\{-1, +1\\}$) by transforming a latent function $f(x)$ through a sigmoid link function. We place a Gaussian Process (GP) prior over this latent function:\n",
    "\n",
    "$$p(f) = \\mathcal{GP}(f; m, k)$$\n",
    "\n",
    "where $m$ is the mean function and $k$ is the covariance (kernel) function. The likelihood of an observation $y$ given the latent function value $f_x$ is given by the logistic sigmoid:\n",
    "\n",
    "$$p(y | f_x) = \\sigma(y f_x) = \\begin{cases} \\sigma(f_x) & \\text{if } y = 1 \\\\ 1 - \\sigma(f_x) & \\text{if } y = -1 \\end{cases}$$\n",
    "\n",
    "The core challenge arises when we try to compute the posterior distribution $p(f_X | Y)$ over the latent function values at the training points $X$. Because the likelihood function (sigmoid) is non-Gaussian, the resulting posterior is also non-Gaussian, making exact analytical inference intractable. The log posterior is given by:\n",
    "\n",
    "$$\\log p(f_X | Y) = \\sum_{i=1}^n \\log \\sigma(y_i f_{x_i}) - \\frac{1}{2}(f_X - m_X)^T K_{XX}^{-1} (f_X - m_X) + \\text{const.}$$\n",
    "\n",
    "The non-elliptical contours of this log posterior (as seen in Lecture 14, Slide 21-23, and Lecture 15, Slide 4-6) visually represent this intractability.\n",
    "\n",
    "Let's start by setting up our imports and common utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np  # Still using numpy for data generation\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set JAX to use 64-bit floats for numerical stability, matching numpy's default\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def sigmoid(f):\n",
    "    \"\"\"Logistic sigmoid function.\"\"\"\n",
    "    return 1 / (1 + jnp.exp(-f))\n",
    "\n",
    "\n",
    "def rbf_kernel(X1, X2, length_scale=1.0):\n",
    "    \"\"\"Radial Basis Function (RBF) kernel.\"\"\"\n",
    "    sqdist = jnp.sum(X1**2, 1)[:, None] + jnp.sum(X2**2, 1) - 2 * jnp.dot(X1, X2.T)\n",
    "    return jnp.exp(-0.5 * (1 / length_scale**2) * sqdist)\n",
    "\n",
    "\n",
    "def generate_data(type=\"separable\", n_samples=100):\n",
    "    \"\"\"Generates synthetic 2D classification data (using numpy for random generation).\"\"\"\n",
    "    np.random.seed(42)  # Ensure reproducibility for data generation\n",
    "    if type == \"separable\":\n",
    "        mean1 = [-1, 0.5]\n",
    "        cov1 = [[0.5, 0.2], [0.2, 0.5]]\n",
    "        data1 = np.random.multivariate_normal(mean1, cov1, n_samples // 2)\n",
    "        labels1 = np.ones(n_samples // 2) * -1\n",
    "        mean2 = [1, -0.5]\n",
    "        cov2 = [[0.5, 0.2], [0.2, 0.5]]\n",
    "        data2 = np.random.multivariate_normal(mean2, cov2, n_samples // 2)\n",
    "        labels2 = np.ones(n_samples // 2)\n",
    "    elif type == \"overlapping\":\n",
    "        mean1 = [-0.5, 0.5]\n",
    "        cov1 = [[1.0, 0.5], [0.5, 1.0]]\n",
    "        data1 = np.random.multivariate_normal(mean1, cov1, n_samples // 2)\n",
    "        labels1 = np.ones(n_samples // 2) * -1\n",
    "        mean2 = [0.5, -0.5]\n",
    "        cov2 = [[1.0, 0.5], [0.5, 1.0]]\n",
    "        data2 = np.random.multivariate_normal(mean2, cov2, n_samples // 2)\n",
    "        labels2 = np.ones(n_samples // 2)\n",
    "    elif type == \"intermingled\":\n",
    "        r1 = np.random.rand(n_samples // 2) * 2\n",
    "        theta1 = np.random.rand(n_samples // 2) * 2 * np.pi\n",
    "        data1 = np.array(\n",
    "            [\n",
    "                r1 * np.cos(theta1) + np.random.randn(n_samples // 2) * 0.2,\n",
    "                r1 * np.sin(theta1) + np.random.randn(n_samples // 2) * 0.2,\n",
    "            ]\n",
    "        ).T\n",
    "        labels1 = np.ones(n_samples // 2) * -1\n",
    "        r2 = np.random.rand(n_samples // 2) * 2 + 1.5\n",
    "        theta2 = np.random.rand(n_samples // 2) * 2 * np.pi\n",
    "        data2 = np.array(\n",
    "            [\n",
    "                r2 * np.cos(theta2) + np.random.randn(n_samples // 2) * 0.2,\n",
    "                r2 * np.sin(theta2) + np.random.randn(n_samples // 2) * 0.2,\n",
    "            ]\n",
    "        ).T\n",
    "        labels2 = np.ones(n_samples // 2)\n",
    "        all_data = np.vstack((data1, data2))\n",
    "        all_labels = np.hstack((labels1, labels2))\n",
    "        perm = np.random.permutation(n_samples)\n",
    "        data1 = all_data[all_labels == -1]\n",
    "        data2 = all_data[all_labels == 1]\n",
    "    X = np.vstack((data1, data2))\n",
    "    y = np.hstack((labels1, labels2))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_data_plotly(X, y, title=\"\", fig=None, row=None, col=None):\n",
    "    \"\"\"Plots 2D classification data using Plotly.\"\"\"\n",
    "    if fig is None:\n",
    "        fig = go.Figure()\n",
    "\n",
    "    # Convert JAX arrays to NumPy for Plotly\n",
    "    X_np = np.asarray(X)\n",
    "    y_np = np.asarray(y)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_np[y_np == -1, 0],\n",
    "            y=X_np[y_np == -1, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=\"maroon\", symbol=\"circle\"),\n",
    "            name=\"Class -1\",\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_np[y_np == 1, 0],\n",
    "            y=X_np[y_np == 1, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                color=\"skyblue\", symbol=\"circle\", line=dict(width=1, color=\"skyblue\")\n",
    "            ),\n",
    "            name=\"Class +1\",\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title_text=title, title_x=0.5)\n",
    "    fig.update_xaxes(title_text=\"$x_1$\", range=[-4, 4], row=row, col=col)\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"$x_2$\",\n",
    "        range=[-4, 4],\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1,\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. The Laplace Approximation Revisited\n",
    "\n",
    "To overcome the intractability, we use approximation methods. The **Laplace Approximation** provides a local Gaussian approximation to a non-Gaussian distribution $p(\\theta)$. The steps are:\n",
    "\n",
    "1.  **Find the MAP estimate (mode)**: Identify $\\hat{\\theta} = \\arg \\max_{\\theta} \\log p(\\theta)$. At this point, the gradient is zero: $\\nabla \\log p(\\hat{\\theta}) = 0$.\n",
    "2.  **Second-order Taylor Expansion**: Approximate $\\log p(\\theta)$ around $\\hat{\\theta}$ using a Taylor series up to the second order:\n",
    "    $$\\log p(\\theta) \\approx \\log p(\\hat{\\theta}) + \\frac{1}{2}(\\theta - \\hat{\\theta})^T \\Psi (\\theta - \\hat{\\theta})$$\n",
    "    where $\\Psi = \\nabla\\nabla^T \\log p(\\hat{\\theta})$ is the Hessian matrix evaluated at $\\hat{\\theta}$.\n",
    "3.  **Gaussian Approximation**: Define the Laplace approximation $q(\\theta)$ as a Gaussian distribution with mean $\\hat{\\theta}$ and covariance matrix $-\\Psi^{-1}$:\n",
    "    $$q(\\theta) = \\mathcal{N}(\\theta; \\hat{\\theta}, -\\Psi^{-1})$$\n",
    "\n",
    "For GP Classification, we apply this to the posterior $p(f_X | Y)$, approximating it as $q(f_X) = \\mathcal{N}(f_X; \\hat{f}, \\hat{\\Sigma})$, where $\\hat{f}$ is the MAP estimate of $f_X$ and $\\hat{\\Sigma} = -(\\nabla\\nabla^T \\log p(f_X | Y)|_{f_X=\\hat{f}})^{-1}$.\n",
    "\n",
    "The Laplace approximation is computationally efficient and often works well for logistic regression due to the concavity of the log posterior, which implies a single global maximum.\n",
    "\n",
    "Let's define the `log_posterior` function that we will maximize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_posterior(f_X, K_XX, y_labels):\n",
    "    \"\"\"\n",
    "    Calculates the log posterior of f_X given observations y_labels.\n",
    "    Assumes mean_X = 0.\n",
    "    \"\"\"\n",
    "    log_likelihood_term = jnp.sum(-jnp.log(1 + jnp.exp(-y_labels * f_X)))\n",
    "    log_prior_term = -0.5 * f_X.T @ jnp.linalg.solve(K_XX, f_X)\n",
    "    return log_likelihood_term + log_prior_term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Deriving the Hessian for GP Classification\n",
    "\n",
    "A key component of the Laplace Approximation is the Hessian matrix. Let's derive it for our GP classification log posterior. Recall the log posterior (assuming $m_X = 0$ for simplicity):\n",
    "\n",
    "$$L(f_X) = \\log p(f_X | Y) = \\sum_{i=1}^n \\log \\sigma(y_i f_{x_i}) - \\frac{1}{2}f_X^T K_{XX}^{-1} f_X$$\n",
    "\n",
    "First, let's re-state the gradient (first derivative) with respect to $f_X$. The $j$-th element of the gradient vector is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial f_{x_j}} = \\left( \\frac{y_j + 1}{2} - \\sigma(f_{x_j}) \\right) - (K_{XX}^{-1} f_X)_j$$\n",
    "\n",
    "Now, we need the Hessian (second derivative matrix) $\\mathbf{H} = \\nabla\\nabla^T L(f_X)$. The elements of the Hessian are $H_{jk} = \\frac{\\partial^2 L}{\\partial f_{x_j} \\partial f_{x_k}}$.\n",
    "\n",
    "Let's look at the two terms separately:\n",
    "\n",
    "1.  **Hessian of the log-likelihood term**: $\\sum_{i=1}^n \\log \\sigma(y_i f_{x_i})$\n",
    "    The second derivative of $\\log \\sigma(z)$ with respect to $z$ is $-\\sigma(z)(1-\\sigma(z))$.\n",
    "    So, for the $j$-th diagonal element, considering $z = y_j f_{x_j}$ and $y_j^2=1$:\n",
    "    $$\\frac{\\partial^2 \\log \\sigma(y_j f_{x_j})}{\\partial f_{x_j}^2} = y_j^2 \\left( -\\sigma(y_j f_{x_j})(1 - \\sigma(y_j f_{x_j})) \\right) = -\\sigma(y_j f_{x_j})(1 - \\sigma(y_j f_{x_j}))$$\n",
    "    Since $\\sigma(y_j f_{x_j})(1 - \\sigma(y_j f_{x_j})) = \\sigma(f_{x_j})(1 - \\sigma(f_{x_j}))$, this term is always non-positive. Let $w_i = \\sigma(f_{x_i})(1 - \\sigma(f_{x_i}))$. We can represent this part of the Hessian as a diagonal matrix $-W$, where $W = \\text{diag}(w_1, ..., w_n)$. Note that $0 < w_i < 1$.\n",
    "\n",
    "2.  **Hessian of the log-prior term**: $-\\frac{1}{2}f_X^T K_{XX}^{-1} f_X$\n",
    "    The second derivative of this quadratic form with respect to $f_X$ is simply $-K_{XX}^{-1}$.\n",
    "\n",
    "Combining these, the full Hessian matrix is:\n",
    "\n",
    "$$\\mathbf{H} = \\nabla\\nabla^T \\log p(f_X | Y) = -W - K_{XX}^{-1} = -(W + K_{XX}^{-1})$$\n",
    "\n",
    "Since $W$ is a diagonal matrix with positive entries and $K_{XX}^{-1}$ is positive definite (as $K_{XX}$ is a covariance matrix), the matrix $W + K_{XX}^{-1}$ is positive definite. Therefore, $-(W + K_{XX}^{-1})$ is negative definite, which confirms that the log posterior is a **concave function**. This concavity is crucial because it guarantees that Newton's method will converge to the unique global maximum (the MAP estimate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Newton Optimization for Mode-Finding (Revisited) with JAX\n",
    "\n",
    "Given the explicit form of the Hessian, we can now fully implement Newton's method to find the MAP estimate $\\hat{f}$. Newton-Raphson is an efficient optimization algorithm that uses both the first and second derivatives of the objective function. The update rule is:\n",
    "\n",
    "$$f^{new} = f^{old} - \\mathbf{H}^{-1} \\mathbf{g}$$\n",
    "\n",
    "where $\\mathbf{g}$ is the gradient and $\\mathbf{H}$ is the Hessian. Substituting our derived forms:\n",
    "\n",
    "$$f^{new} = f^{old} - (-(W + K_{XX}^{-1}))^{-1} \\left( (\\tilde{\\mathbf{y}} - \\mathbf{\\pi}(f^{old})) - K_{XX}^{-1} f^{old} \\right)$$\n",
    "\n",
    "$$f^{new} = f^{old} + (W + K_{XX}^{-1})^{-1} \\left( (\\tilde{\\mathbf{y}} - \\mathbf{\\pi}(f^{old})) - K_{XX}^{-1} f^{old} \\right)$$\n",
    "\n",
    "With JAX, we can automatically compute these gradients and Hessians, making our code cleaner and less prone to manual derivation errors. We will use `jax.grad` and `jax.hessian` transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit  # JIT compile the optimization step for performance\n",
    "def newton_step(f_X, K_XX, y_labels):\n",
    "    \"\"\"Performs one Newton update step for maximizing the log posterior.\"\"\"\n",
    "    # We want to maximize log_posterior, so we minimize -log_posterior\n",
    "    neg_log_posterior = lambda f: -log_posterior(f, K_XX, y_labels)\n",
    "\n",
    "    # Compute gradient and Hessian of the *negative* log posterior\n",
    "    grad_neg_log_post = jax.grad(neg_log_posterior)(f_X)\n",
    "    hess_neg_log_post = jax.hessian(neg_log_posterior)(f_X)\n",
    "\n",
    "    # Newton update: f_new = f_old - H_inv * grad\n",
    "    delta_f = jnp.linalg.solve(hess_neg_log_post, grad_neg_log_post)\n",
    "    f_X_new = f_X - delta_f\n",
    "    return f_X_new\n",
    "\n",
    "\n",
    "def find_map_f_newton_jax(\n",
    "    X_train, y_train, kernel_func, length_scale, max_iter=100, tol=1e-5\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds the MAP estimate hat_f using Newton's method with JAX autodiff.\n",
    "    Returns hat_f and the kernel matrix K_XX.\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    f_X = jnp.zeros(n)  # Initialize f_X with zeros\n",
    "    K_XX = kernel_func(X_train, X_train, length_scale)\n",
    "    K_XX += 1e-6 * jnp.eye(n)  # Add jitter for numerical stability\n",
    "    y_labels_jax = jnp.array(y_train)\n",
    "\n",
    "    print(\"Starting Newton's method with JAX...\")\n",
    "    for i in range(max_iter):\n",
    "        f_X_new = newton_step(f_X, K_XX, y_labels_jax)\n",
    "        change = jnp.linalg.norm(f_X_new - f_X)\n",
    "        if change < tol:\n",
    "            print(f\"Converged in {i + 1} iterations. Final change: {change:.6f}\")\n",
    "            break\n",
    "        f_X = f_X_new\n",
    "    else:\n",
    "        print(\n",
    "            f\"Newton's method did not converge after {max_iter} iterations. Final change: {change:.6f}\"\n",
    "        )\n",
    "    return f_X, K_XX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Numerical Stability and Efficient Computations (Advanced)\n",
    "\n",
    "The direct inversion of $(W + K_{XX}^{-1})$ in the Newton step and for computing the posterior covariance can be numerically unstable, especially when $W$ has very small eigenvalues (e.g., when sigmoid outputs are very close to 0 or 1). Rasmussen & Williams (2006) propose a more stable approach using the matrix inversion lemma and a re-parameterization involving the matrix $B = I + W^{1/2} K W^{1/2}$.\n",
    "\n",
    "From the matrix inversion lemma, we have:\n",
    "$$\n",
    "(W + K_{XX}^{-1})^{-1} = K_{XX} - K_{XX} W^{1/2} (I + W^{1/2} K_{XX} W^{1/2})^{-1} W^{1/2} K_{XX}\n",
    "$$\n",
    "Let $B = I + W^{1/2} K_{XX} W^{1/2}$. Then the inverse becomes $K_{XX} - K_{XX} W^{1/2} B^{-1} W^{1/2} K_{XX}$. This form is often more numerically stable as $B$ is symmetric positive definite with eigenvalues $\\ge 1$.\n",
    "\n",
    "While we won't implement this optimized form in detail for this introductory notebook, it's important to be aware that practical implementations often incorporate such numerical considerations for robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Computing Predictions with Laplace Approximation (Full Posterior)\n",
    "\n",
    "Once we have the MAP estimate $\\hat{f}$ and the approximate posterior covariance $\\hat{\\Sigma} = (W + K_{XX}^{-1})^{-1}$ (evaluated at $\\hat{f}$), we can make predictions for new test points $x_*$. The approximate posterior distribution for the latent function $f_{x_*}$ at a new point $x_*$ is given by:\n",
    "\n",
    "$$q(f_{x_*} | y) = \\mathcal{N}(f_{x_*}; \\mu_{q(f_{x_*}|y)}, \\Sigma_{q(f_{x_*}|y)})$$\n",
    "\n",
    "The mean of this predictive distribution (assuming $m_X = 0$):\n",
    "$$\\mu_{q(f_{x_*}|y)} = k_{x_*X} K_{XX}^{-1} \\hat{f}$$\n",
    "\n",
    "The variance of this predictive distribution is (from Lecture 15, Slide 17):\n",
    "$$\\Sigma_{q(f_{x_*}|y)} = k_{x_*x_*} - k_{x_*X} K_{XX}^{-1} k_{X x_*} + k_{x_*X} K_{XX}^{-1} \\hat{\\Sigma} K_{XX}^{-1} k_{X x_*}$$\n",
    "\n",
    "A more compact and often numerically stable form for the variance, derived using the matrix inversion lemma (as shown on Slide 17), is:\n",
    "$$\\Sigma_{q(f_{x_*}|y)} = k_{x_*x_*} - k_{x_*X}(K_{XX} + W^{-1})^{-1}k_{X x_*}$$\n",
    "\n",
    "This predictive distribution $q(f_{x_*} | y)$ is Gaussian. However, to get the predictive probability for the label, $p(y_*=1 | x_*, Y)$, we need to compute $E_{q(f_{x_*}|y)}[\\sigma(f_{x_*})$. This integral is still analytically intractable. Common approximations include:\n",
    "\n",
    "1.  **Plug-in Approximation**: $\\hat{\\pi}_{x_*} = \\sigma(\\mu_{q(f_{x_*}|y)})$. This is the simplest and what we used for plotting the decision boundary in Lecture 14.\n",
    "2.  **MacKay's Approximation (1992)**: A more sophisticated approximation that accounts for the variance $s^2 = \\Sigma_{q(f_{x_*}|y)}$:\n",
    "    $$E[\\sigma(f)] \\approx \\sigma\\left(\\frac{\\mu}{\\sqrt{1 + \\frac{\\pi}{8}s^2}}\\right)$$\n",
    "    This approximation is often more accurate than the plug-in method, especially when the variance is large.\n",
    "\n",
    "Let's implement these prediction steps and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_laplace_gp_classification_jax(\n",
    "    X_test, X_train, hat_f, K_XX, kernel_func, length_scale\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the mean and variance of the latent function at test points\n",
    "    using the Laplace Approximation, and then predictive probabilities (with JAX).\n",
    "    \"\"\"\n",
    "    # 1. Compute W_hat at the MAP estimate hat_f\n",
    "    pi_hat_f = sigmoid(hat_f)\n",
    "    W_hat = jnp.diag(pi_hat_f * (1 - pi_hat_f))\n",
    "\n",
    "    # 2. Compute the approximate posterior covariance Sigma_hat\n",
    "    # Sigma_hat = (W_hat + K_XX^-1)^-1\n",
    "    K_XX_inv = jnp.linalg.inv(K_XX)\n",
    "    Sigma_hat = jnp.linalg.inv(W_hat + K_XX_inv)\n",
    "\n",
    "    # 3. Compute kernel matrices for test points\n",
    "    K_x_X_test = kernel_func(X_test, X_train, length_scale)\n",
    "    K_test_test = kernel_func(X_test, X_test, length_scale)\n",
    "\n",
    "    # 4. Mean of the approximate predictive latent function q(f_* | y)\n",
    "    mu_latent_pred = K_x_X_test @ K_XX_inv @ hat_f\n",
    "\n",
    "    # 5. Variance of the approximate predictive latent function q(f_* | y)\n",
    "    # Using the more stable form from slide 17: k_x*x* - k_x*X @ (K_XX + W_hat^-1)^-1 @ k_X x*\n",
    "    # Ensure W_hat_inv is computed robustly, avoiding division by zero if pi_hat_f is exactly 0 or 1\n",
    "    w_diag = pi_hat_f * (1 - pi_hat_f)\n",
    "    w_diag = jnp.where(\n",
    "        w_diag < 1e-10, 1e-10, w_diag\n",
    "    )  # Clamp small values to avoid division by zero\n",
    "    W_hat_inv = jnp.diag(1 / w_diag)\n",
    "\n",
    "    Sigma_latent_pred = K_test_test - K_x_X_test @ jnp.linalg.solve(\n",
    "        (K_XX + W_hat_inv), K_x_X_test.T\n",
    "    )\n",
    "\n",
    "    # Extract diagonal for individual variances (s^2 for each test point)\n",
    "    s2_latent_pred = jnp.diag(Sigma_latent_pred)\n",
    "\n",
    "    # 6. Compute predictive probabilities (E[sigma(f)] approximations)\n",
    "    # Plug-in approximation\n",
    "    prob_pred_plugin = sigmoid(mu_latent_pred)\n",
    "\n",
    "    # MacKay's approximation (1992)\n",
    "    prob_pred_mackay = sigmoid(\n",
    "        mu_latent_pred / jnp.sqrt(1 + (jnp.pi / 8) * s2_latent_pred)\n",
    "    )\n",
    "\n",
    "    return mu_latent_pred, s2_latent_pred, prob_pred_plugin, prob_pred_mackay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the full GP Classification with Laplace Approximation on an overlapping dataset to better observe uncertainty, and visualize the results using Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate some overlapping data to demonstrate uncertainty\n",
    "X_train_np, y_train_np = generate_data(type=\"overlapping\", n_samples=100)\n",
    "\n",
    "# Convert training data to JAX arrays\n",
    "X_train_jax = jnp.array(X_train_np)\n",
    "y_train_jax = jnp.array(y_train_np)\n",
    "\n",
    "# Find the MAP estimate hat_f\n",
    "hat_f_jax, K_XX_jax = find_map_f_newton_jax(\n",
    "    X_train_jax, y_train_jax, rbf_kernel, length_scale=1.0\n",
    ")\n",
    "\n",
    "print(\"\\nMAP estimate (hat_f) first 5 elements:\")\n",
    "print(hat_f_jax[:5])\n",
    "\n",
    "# Prepare a grid of test points for visualization\n",
    "x1_grid = np.linspace(-4, 4, 100)\n",
    "x2_grid = np.linspace(-4, 4, 100)\n",
    "X1_mesh, X2_mesh = np.meshgrid(x1_grid, x2_grid)\n",
    "X_grid_np = np.vstack([X1_mesh.ravel(), X2_mesh.ravel()]).T\n",
    "X_grid_jax = jnp.array(X_grid_np)\n",
    "\n",
    "# Compute predictions using the Laplace Approximation with JAX\n",
    "(\n",
    "    mu_latent_grid_jax,\n",
    "    s2_latent_grid_jax,\n",
    "    prob_pred_plugin_grid_jax,\n",
    "    prob_pred_mackay_grid_jax,\n",
    ") = predict_laplace_gp_classification_jax(\n",
    "    X_grid_jax, X_train_jax, hat_f_jax, K_XX_jax, rbf_kernel, length_scale=1.0\n",
    ")\n",
    "\n",
    "# Convert JAX arrays back to numpy for Plotly\n",
    "prob_pred_plugin_mesh_np = np.array(prob_pred_plugin_grid_jax).reshape(X1_mesh.shape)\n",
    "prob_pred_mackay_mesh_np = np.array(prob_pred_mackay_grid_jax).reshape(X1_mesh.shape)\n",
    "s2_latent_mesh_np = np.array(s2_latent_grid_jax).reshape(X1_mesh.shape)\n",
    "\n",
    "# Plotting Predictive Probabilities (MacKay's Approximation) with Plotly\n",
    "fig_prob = go.Figure()\n",
    "fig_prob.add_trace(\n",
    "    go.Contour(\n",
    "        z=prob_pred_mackay_mesh_np,\n",
    "        x=x1_grid,\n",
    "        y=x2_grid,\n",
    "        colorscale=\"RdBu\",\n",
    "        opacity=0.6,\n",
    "        colorbar_title=\"Predicted Probability $P(y=1|x)$\",\n",
    "        line_smoothing=0.8,  # Smooth contours\n",
    "        contours_coloring=\"heatmap\",  # Color between contours\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add decision boundary (0.5 probability contour)\n",
    "fig_prob.add_trace(\n",
    "    go.Contour(\n",
    "        z=prob_pred_mackay_mesh_np,\n",
    "        x=x1_grid,\n",
    "        y=x2_grid,\n",
    "        showscale=False,\n",
    "        contours=dict(\n",
    "            start=0.5,\n",
    "            end=0.5,\n",
    "            size=0,\n",
    "            coloring=\"lines\",\n",
    "            showlabels=True,\n",
    "            labelfont=dict(size=12, color=\"black\"),\n",
    "        ),\n",
    "        line_color=\"black\",\n",
    "        line_width=2,\n",
    "        name=\"Decision Boundary ($P(y=1|x)=0.5$)\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add training data points\n",
    "fig_prob = plot_data_plotly(X_train_np, y_train_np, fig=fig_prob)\n",
    "\n",
    "fig_prob.update_layout(\n",
    "    title_text=\"GP Classification: Predictive Probabilities (MacKay) with JAX and Plotly\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"$x_1$\",\n",
    "    yaxis_title=\"$x_2$\",\n",
    "    height=600,\n",
    "    width=800,\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01,\n",
    "        bgcolor=\"rgba(255,255,255,0.7)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        font=dict(size=14),\n",
    "    ),\n",
    ")\n",
    "fig_prob.show()\n",
    "\n",
    "# Plotting Predictive Uncertainty (Variance of Latent Function) with Plotly\n",
    "fig_unc = go.Figure()\n",
    "fig_unc.add_trace(\n",
    "    go.Contour(\n",
    "        z=s2_latent_mesh_np,\n",
    "        x=x1_grid,\n",
    "        y=x2_grid,\n",
    "        colorscale=\"Viridis\",\n",
    "        opacity=0.7,\n",
    "        colorbar_title=\"Predictive Variance $s^2$\",\n",
    "        line_smoothing=0.8,\n",
    "        contours_coloring=\"heatmap\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add training data points\n",
    "fig_unc = plot_data_plotly(X_train_np, y_train_np, fig=fig_unc)\n",
    "\n",
    "fig_unc.update_layout(\n",
    "    title_text=\"GP Classification: Predictive Uncertainty (Latent Variance) with JAX and Plotly\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"$x_1$\",\n",
    "    yaxis_title=\"$x_2$\",\n",
    "    height=600,\n",
    "    width=800,\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01,\n",
    "        bgcolor=\"rgba(255,255,255,0.7)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        font=dict(size=14),\n",
    "    ),\n",
    ")\n",
    "fig_unc.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ab2da",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "### Interpreting Predictive Uncertainty in GP Classification\n",
    "\n",
    "The **predictive uncertainty** in Gaussian Process (GP) classification, as visualized by the variance of the latent function ($s^2$) in the contour plot, provides crucial insights into the model's confidence about its predictions at different locations in the input space.\n",
    "\n",
    "#### What Does Predictive Uncertainty Represent?\n",
    "\n",
    "- **High Uncertainty ($s^2$ is large):**\n",
    "    - The model is **less confident** about its prediction.\n",
    "    - This typically occurs in regions where:\n",
    "        - The classes overlap (i.e., both class -1 and +1 data points are nearby).\n",
    "        - There are **few or no training data points** (far from observed data).\n",
    "        - The decision boundary is located (the model is unsure which class to assign).\n",
    "- **Low Uncertainty ($s^2$ is small):**\n",
    "    - The model is **more confident** about its prediction.\n",
    "    - This is usually seen in regions densely populated by training points of a single class.\n",
    "\n",
    "#### What Do We See in the Contour Plot?\n",
    "\n",
    "- The **contour plot of $s^2$** (predictive variance) shows:\n",
    "    - **High uncertainty (bright/yellow regions in Viridis colormap)** near the **decision boundary** and in areas where the two classes are mixed or close together.\n",
    "    - **Low uncertainty (dark/purple regions)** in areas well inside a cluster of one class, far from the boundary.\n",
    "- **Away from the training data** (e.g., corners of the plot), uncertainty increases again because the model has little information there.\n",
    "\n",
    "#### Why Is This Important?\n",
    "\n",
    "- **Model Trust:** High uncertainty means the model \"knows what it doesn't know\"—it is cautious in ambiguous regions.\n",
    "- **Decision Making:** In applications, you might avoid making hard decisions in high-uncertainty regions, or seek more data there.\n",
    "- **Active Learning:** The model can suggest where new data would be most informative (regions of high uncertainty).\n",
    "\n",
    "#### Summary Table\n",
    "\n",
    "| Region in Input Space         | Predictive Variance $s^2$ | Model Confidence | Typical Location         |\n",
    "|------------------------------|---------------------------|------------------|-------------------------|\n",
    "| Near decision boundary       | High                      | Low              | Class overlap           |\n",
    "| Inside dense class cluster   | Low                       | High             | Well-separated regions  |\n",
    "| Far from any training points | High                      | Low              | Corners/outliers        |\n",
    "\n",
    "#### Visual Example\n",
    "\n",
    "- In your contour plot, **the highest uncertainty forms a band along the decision boundary**—this is where the model is most unsure about the class label.\n",
    "- **Uncertainty drops off** as you move into regions dominated by one class.\n",
    "\n",
    "> **In summary:**  \n",
    "> The predictive uncertainty tells you where the GP classifier is unsure about its predictions. The contour plot visually highlights these regions, guiding interpretation and further data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17356d4d",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "### Translating Uncertainty from Latent Space to Label Space in GP Classification\n",
    "\n",
    "In Gaussian Process (GP) classification, the **latent space** refers to the values of the latent function $f(x)$, which are modeled as a Gaussian process. The **label space** refers to the observed class labels $y \\in \\{-1, +1\\}$ (or $\\{0, 1\\}$).\n",
    "\n",
    "#### 1. Predictive Distribution in Latent Space\n",
    "\n",
    "After applying the Laplace approximation, the predictive distribution for the latent function at a new point $x_*$ is:\n",
    "\n",
    "$$\n",
    "q(f_{x_*} | y) = \\mathcal{N}(f_{x_*}; \\mu_{q(f_{x_*}|y)}, \\Sigma_{q(f_{x_*}|y)})\n",
    "$$\n",
    "\n",
    "- $\\mu_{q(f_{x_*}|y)}$: Mean of the latent function at $x_*$.\n",
    "- $\\Sigma_{q(f_{x_*}|y)}$: Variance (uncertainty) of the latent function at $x_*$.\n",
    "\n",
    "#### 2. From Latent Function to Label Probability\n",
    "\n",
    "The class label is determined by passing the latent function through a sigmoid (logistic) function:\n",
    "\n",
    "$$\n",
    "p(y_* = 1 \\mid x_*, \\text{data}) = \\mathbb{E}_{q(f_{x_*}|y)}[\\sigma(f_{x_*})]\n",
    "$$\n",
    "\n",
    "where $\\sigma(f) = \\frac{1}{1 + e^{-f}}$ is the logistic sigmoid.\n",
    "\n",
    "#### 3. Translating Uncertainty\n",
    "\n",
    "- **Latent Uncertainty ($s^2$):** The variance $\\Sigma_{q(f_{x_*}|y)}$ quantifies how uncertain we are about the value of $f_{x_*}$.\n",
    "- **Label Uncertainty:** The uncertainty in $f_{x_*}$ translates to uncertainty in the predicted class probability $p(y_* = 1 \\mid x_*)$.\n",
    "\n",
    "However, because the sigmoid is a nonlinear function, the mean of the sigmoid is **not** the sigmoid of the mean:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\sigma(f_{x_*})] \\neq \\sigma(\\mathbb{E}[f_{x_*}])\n",
    "$$\n",
    "\n",
    "#### 4. Practical Approximations\n",
    "\n",
    "- **Plug-in Approximation:**  \n",
    "    $$p(y_* = 1 \\mid x_*) \\approx \\sigma(\\mu_{q(f_{x_*}|y)})$$  \n",
    "    Ignores uncertainty in $f_{x_*}$.\n",
    "\n",
    "- **MacKay's Approximation:**  \n",
    "    $$p(y_* = 1 \\mid x_*) \\approx \\sigma\\left(\\frac{\\mu_{q(f_{x_*}|y)}}{\\sqrt{1 + \\frac{\\pi}{8} s^2}}\\right)$$  \n",
    "    This accounts for the variance $s^2 = \\Sigma_{q(f_{x_*}|y)}$ and gives a more accurate probability, especially when uncertainty is high.\n",
    "\n",
    "#### 5. Intuitive Summary\n",
    "\n",
    "- **High latent variance ($s^2$):** Predictive probability is \"squashed\" toward 0.5 (maximum uncertainty in label).\n",
    "- **Low latent variance:** Predictive probability is closer to 0 or 1 (high confidence in label).\n",
    "\n",
    "#### 6. Visual Example\n",
    "\n",
    "- In the contour plots, regions with high $s^2$ correspond to predicted probabilities near 0.5, indicating the model is unsure about the class label.\n",
    "- Regions with low $s^2$ correspond to probabilities near 0 or 1, indicating high confidence.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "Uncertainty in the latent function $f(x)$ is translated to label uncertainty by integrating the sigmoid over the Gaussian predictive distribution. Approximations like MacKay's formula allow us to efficiently compute this, ensuring that high latent uncertainty leads to label probabilities closer to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "**Exercise 1: Compare Plug-in vs. MacKay's Approximation**\n",
    "Plot the predictive probabilities using both `prob_pred_plugin_mesh_np` and `prob_pred_mackay_mesh_np` side-by-side using Plotly subplots. What differences do you observe, especially in regions of high uncertainty or near the decision boundary? Why might MacKay's approximation be preferred?\n",
    "\n",
    "**Exercise 2: Interpreting Predictive Uncertainty**\n",
    "Analyze the predictive uncertainty plot (`s2_latent_mesh_np`). Where is the uncertainty highest? How does this relate to the training data distribution and the decision boundary? What does high uncertainty imply in a classification context?\n",
    "\n",
    "**Exercise 3: Impact of Data Separability on Uncertainty**\n",
    "Change the `generate_data` type to `'separable'` and `'intermingled'`. Rerun the entire notebook. How do the predictive probabilities and uncertainties change for these different data distributions? Discuss the implications for model confidence.\n",
    "\n",
    "**Exercise 4: Effect of Kernel Length Scale on Predictions**\n",
    "Experiment with different `length_scale` values (e.g., 0.1, 0.5, 2.0) for the `rbf_kernel`. How does the `length_scale` influence the smoothness of the decision boundary and the distribution of predictive uncertainty? Relate this to the concept of kernel hyperparameters and their role in GP models.\n",
    "\n",
    "**Exercise 5 (Advanced): Implement Numerical Stability Improvement**\n",
    "Modify the `predict_laplace_gp_classification_jax` function to use the numerically more stable form for `Sigma_latent_pred` (using the $B$ matrix and matrix inversion lemma) as discussed in Section 5 and Slide 17. Verify that the results are consistent with the current implementation, but note the potential benefits for larger or more ill-conditioned datasets."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
