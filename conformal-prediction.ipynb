{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conformal_prediction_intro"
   },
   "source": [
    "# Conformal Prediction: Quantifying Uncertainty without Assumptions\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this in-depth exploration of **Conformal Prediction**, a powerful and increasingly popular framework in statistical learning for quantifying uncertainty in predictions. In traditional machine learning, models often output point predictions (e.g., a single value for regression, or a single class label for classification). While these predictions might be accurate on average, they rarely come with reliable guarantees about their uncertainty. Probabilistic models (like Gaussian Processes) provide uncertainty, but often rely on strong distributional assumptions (e.g., Gaussianity) or can be computationally intensive, especially for large datasets or complex models.\n",
    "\n",
    "Conformal prediction offers a compelling alternative: it provides **finite-sample coverage guarantees** for prediction sets (or intervals) without making any assumptions about the underlying data distribution. This \"distribution-free\" property makes it incredibly versatile and robust, allowing it to be applied to any underlying prediction algorithm, from simple linear models to complex deep neural networks.\n",
    "\n",
    "In this notebook, we will systematically delve into the core ideas, algorithms, and applications of conformal prediction, accompanied by practical Python code examples using `JAX` and `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conformal_prediction_goal"
   },
   "source": [
    "## 1. The Lofty Goal: Distribution-Free Finite-Sample Coverage\n",
    "\n",
    "The primary objective of conformal prediction is ambitious yet elegant. Let's formalize it:\n",
    "\n",
    "Suppose we have $n$ i.i.d. (independent and identically distributed) feature-response pairs $(X_i, Y_i) \\sim P$, for $i=1, \\ldots, n$, drawn from an unknown distribution $P$ over $\\mathcal{X} \\times \\mathcal{Y}$. Here, $\\mathcal{X}$ is the feature space (e.g., $\\mathbb{R}^d$) and $\\mathcal{Y}$ is the response space (e.g., $\\mathbb{R}$ for regression, or a set of discrete labels for classification).\n",
    "\n",
    "Given a nominal error level $\\alpha \\in (0, 1)$ (e.g., $\\alpha = 0.1$ for 90% coverage), our goal is to construct a **prediction set** $\\hat{C}_n(X_{n+1}) \\subseteq \\mathcal{Y}$ such that for a new, unseen i.i.d. pair $(X_{n+1}, Y_{n+1}) \\sim P$:\n",
    "\n",
    "$$\\mathbb{P}(Y_{n+1} \\in \\hat{C}_n(X_{n+1})) \\ge 1 - \\alpha$$\n",
    "\n",
    "This probability is taken over all our data: the $n$ training points and the new $(n+1)$-th test point.\n",
    "\n",
    "### Why is this \"Lofty\"?\n",
    "\n",
    "1.  **Distribution-Free**: The guarantee holds *without any assumptions* about the underlying distribution $P$. This is a powerful statement, as most statistical methods rely on knowing (or assuming) the data-generating process.\n",
    "2.  **Finite-Sample**: The guarantee is valid for *any finite sample size* $n$, not just asymptotically as $n \\to \\infty$. This is crucial for real-world applications where data might be limited.\n",
    "3.  **Model-Agnostic**: Conformal prediction can wrap *any* arbitrary prediction algorithm (often called the \"base predictor\" or \"black-box predictor\"). This means you can use your favorite complex model (e.g., a deep neural network) and still get valid uncertainty quantification.\n",
    "\n",
    "While a trivial prediction set (e.g., returning the entire response space $\\mathcal{Y}$ with probability $1-\\alpha$) could satisfy this, the true challenge is to achieve this guarantee with a **nontrivial** prediction set that adapts to the \"hardness\" of the problem. Ideally, the prediction set $\\hat{C}_n(X_{n+1})$ should be smaller when $Y_{n+1}$ is easier to predict from $X_{n+1}$.\n",
    "\n",
    "Remarkably, this goal is achievable, and the core ideas are surprisingly simple, rooted in the concept of **exchangeability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conformal_prediction_achievable_no_features"
   },
   "source": [
    "## 2. First Key Idea: Exchangeability and Ranks (No Features)\n",
    "\n",
    "Let's start with the simplest scenario: predicting a single real-valued response $Y_{n+1}$ when we have $n$ i.i.d. observations $Y_1, \\ldots, Y_n$, and no features $X$. Our goal is to find a one-sided prediction interval $\\hat{C}_n = (-\\infty, \\hat{q}_n]$ such that $\\mathbb{P}(Y_{n+1} \\le \\hat{q}_n) \\ge 1 - \\alpha$.\n",
    "\n",
    "A naive approach might be to set $\\hat{q}_n$ to be the $(1-\\alpha)$-th sample quantile of $Y_1, \\ldots, Y_n$. However, this only provides an *approximate* coverage that becomes exact asymptotically. Conformal prediction provides an exact finite-sample guarantee.\n",
    "\n",
    "### The Power of Exchangeability\n",
    "\n",
    "The key insight comes from the property of **exchangeability**. If $Y_1, \\ldots, Y_n, Y_{n+1}$ are i.i.d., then their joint distribution is invariant under any permutation of their indices. A direct consequence of this is:\n",
    "\n",
    "**The rank of $Y_{n+1}$ among the full set of $n+1$ observations $\\{Y_1, \\\\ldots, Y_n, Y_{n+1}\\}$ is uniformly distributed over $\\{1, 2, \\\\ldots, n+1\\}$.**\n",
    "\n",
    "This means that $Y_{n+1}$ is equally likely to be the smallest, second smallest, ..., or $(n+1)$-th smallest value in the combined set.\n",
    "\n",
    "From this, we can deduce:\n",
    "$$\\mathbb{P}(Y_{n+1} \\le \\text{the } k\\text{-th smallest of } \\{Y_1, \\ldots, Y_n, Y_{n+1}\\}) = \\frac{k}{n+1}$$\n",
    "where $k$ is the rank. To achieve at least $1-\\alpha$ coverage, we choose $k = \\lceil (1-\\alpha)(n+1) \\rceil$.\n",
    "\n",
    "The crucial step is then realizing that:\n",
    "$$\\mathbb{P}(Y_{n+1} \\le \\text{the } \\lceil (1-\\alpha)(n+1) \\rceil \\text{ smallest of } \\{Y_1, \\ldots, Y_n\\}) \\ge 1 - \\alpha$$\n",
    "This equivalence is profound because the quantity on the right-hand side, $\\hat{q}_n = \\text{the } \\lceil (1-\\alpha)(n+1) \\rceil \\text{ smallest of } \\{Y_1, \\ldots, Y_n\\}$, is **computable solely from the observed training data** $Y_1, \\ldots, Y_n$.\n",
    "\n",
    "Thus, by defining $\\hat{q}_n$ this way, we obtain a prediction interval $(-\\infty, \\hat{q}_n]$ that guarantees at least $1-\\alpha$ coverage for $Y_{n+1}$ in finite samples, without any distributional assumptions beyond i.i.d. (or the weaker condition of exchangeability).\n",
    "\n",
    "An equivalent formulation using the empirical quantile function is:\n",
    "$$\\hat{q}_n = \\text{Quantile}\\left(\\frac{\\lceil (1-\\alpha)(n+1) \\rceil}{n}; \\text{empirical distribution of } Y_1, \\ldots, Y_n\\right)$$\n",
    "This means we compute the sample quantile at an \"adjusted level\" $\\frac{\\lceil (1-\\alpha)(n+1) \\rceil}{n}$ instead of the naive $1-\\alpha$. This adjustment accounts for the finite sample size.\n",
    "\n",
    "If there are no ties in the data, the coverage can be sharpened to $\\mathbb{P}(Y_{n+1} \\le \\hat{q}_n) \\in [1-\\alpha, 1-\\alpha + \\frac{1}{n+1})$.\n",
    "\n",
    "### Code Example: Rank-Based Quantile (No Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conformal_no_features_code"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np  # For sorting and quantile function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def calculate_conformal_quantile_no_features(\n",
    "    Y_train: jnp.ndarray, alpha: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the conformal quantile for the no-features case.\n",
    "    This quantile defines a one-sided prediction interval (-inf, q_n].\n",
    "\n",
    "    Args:\n",
    "        Y_train: 1D array of observed response values (training data).\n",
    "        alpha: Nominal error level (e.g., 0.1 for 90% coverage).\n",
    "\n",
    "    Returns:\n",
    "        The conformal quantile q_n.\n",
    "    \"\"\"\n",
    "    n = Y_train.shape[0]\n",
    "\n",
    "    # Calculate the adjusted rank index.\n",
    "    # This is the rank (1-indexed) that Y_{n+1} should be less than or equal to,\n",
    "    # among the n+1 observations (Y_1, ..., Y_n, Y_{n+1}).\n",
    "    # We then find the value at this rank from Y_1, ..., Y_n.\n",
    "    adjusted_rank_index = jnp.ceil((1 - alpha) * (n + 1)).astype(int)\n",
    "\n",
    "    # Sort the training data to find the order statistic\n",
    "    Y_sorted = jnp.sort(Y_train)\n",
    "\n",
    "    # Handle edge cases for the rank index.\n",
    "    # If adjusted_rank_index is 0 or less, it means we want a very small quantile.\n",
    "    # If adjusted_rank_index is greater than n, it means we want a very large quantile.\n",
    "    if adjusted_rank_index <= 0:\n",
    "        # If alpha is close to 1, adjusted_rank_index could be <= 0.\n",
    "        # This implies the prediction set should be very small (e.g., -infinity for lower bound,\n",
    "        # or +infinity for upper bound). For a (-inf, q_n] interval, q_n would be very low.\n",
    "        print(\n",
    "            f\"Warning: Adjusted rank index ({adjusted_rank_index}) <= 0. Returning min observed value.\"\n",
    "        )\n",
    "        return Y_sorted[0]\n",
    "    elif adjusted_rank_index > n:\n",
    "        # If alpha is very small, adjusted_rank_index could be > n.\n",
    "        # For a (-inf, q_n] interval, q_n should be very high (effectively infinity).\n",
    "        print(\n",
    "            f\"Warning: Adjusted rank index ({adjusted_rank_index}) > n ({n}). Returning max observed value.\"\n",
    "        )\n",
    "        return Y_sorted[-1]\n",
    "    else:\n",
    "        # Return the (adjusted_rank_index)-th smallest value.\n",
    "        # JAX/Python indexing is 0-based, so we subtract 1.\n",
    "        return Y_sorted[adjusted_rank_index - 1]\n",
    "\n",
    "\n",
    "# --- Example Usage and Coverage Simulation ---\n",
    "key = random.PRNGKey(0)\n",
    "num_training_samples = 50  # n\n",
    "alpha_level = 0.1  # Desired coverage: 1 - alpha = 0.9 (90%)\n",
    "\n",
    "# Simulate i.i.d. training data from a standard normal distribution\n",
    "Y_train_data = random.normal(key, (num_training_samples,))\n",
    "print(f\"Training data (first 5 samples): {Y_train_data[:5]}\\n\")\n",
    "\n",
    "# Calculate the conformal quantile\n",
    "q_n_conformal = calculate_conformal_quantile_no_features(Y_train_data, alpha_level)\n",
    "print(f\"Calculated conformal quantile (q_n): {q_n_conformal:.4f}\")\n",
    "\n",
    "# For comparison, calculate the naive sample quantile (using 1-alpha directly)\n",
    "q_n_naive = jnp.quantile(Y_train_data, 1 - alpha_level)\n",
    "print(f\"Naive sample quantile (1-alpha): {q_n_naive:.4f}\\n\")\n",
    "\n",
    "# --- Simulate Coverage ---\n",
    "num_simulations = 10000\n",
    "coverage_count = 0\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    # Generate a new i.i.d. test observation Y_{n+1}\n",
    "    Y_n_plus_1 = random.normal(key, (1,))[0]  # Single new observation\n",
    "\n",
    "    # Check if Y_{n+1} falls into the prediction interval (-inf, q_n]\n",
    "    if Y_n_plus_1 <= q_n_conformal:\n",
    "        coverage_count += 1\n",
    "\n",
    "simulated_coverage = coverage_count / num_simulations\n",
    "print(f\"Simulated coverage for Y_n+1 <= q_n: {simulated_coverage:.4f}\")\n",
    "print(f\"Desired coverage (at least): {1 - alpha_level:.4f}\")\n",
    "print(\n",
    "    f\"Theoretical upper bound (approx): {1 - alpha_level + 1 / (num_training_samples + 1):.4f}\\n\"\n",
    ")\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(\n",
    "    Y_train_data,\n",
    "    bins=15,\n",
    "    density=True,\n",
    "    alpha=0.6,\n",
    "    color=\"skyblue\",\n",
    "    label=\"Training Data Distribution\",\n",
    ")\n",
    "plt.axvline(\n",
    "    q_n_conformal,\n",
    "    color=\"red\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=f\"Conformal Quantile (q_n={q_n_conformal:.2f})\",\n",
    ")\n",
    "plt.axvline(\n",
    "    q_n_naive,\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1,\n",
    "    label=f\"Naive Quantile (q_n={q_n_naive:.2f})\",\n",
    ")\n",
    "plt.title(\n",
    "    f\"Conformal Prediction (No Features) for n={num_training_samples}, $\\\\alpha$={alpha_level}\"\n",
    ")\n",
    "plt.xlabel(\"Y value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conformal_prediction_naive_regression"
   },
   "source": [
    "## 3. Naive Attempt to Lift to Regression Problems (Why it Fails)\n",
    "\n",
    "The success of the rank-based approach in the \"no features\" case naturally leads us to try and extend it to regression.\n",
    "\n",
    "Suppose we have training data $(X_i, Y_i)$ for $i=1, \\ldots, n$. We train a point predictor $\\hat{f}_n$ (e.g., linear regression, random forest, neural network) on this data.\n",
    "\n",
    "A naive attempt to form a prediction interval for a new point $(X_{n+1}, Y_{n+1})$ would be:\n",
    "\n",
    "1.  Compute absolute residuals on the training set: $R_i = |Y_i - \\hat{f}_n(X_i)|$ for $i=1, \\ldots, n$.\n",
    "2.  Find the conformal quantile $\\hat{q}_n = \\text{the } \\lceil (1-\\alpha)(n+1) \\rceil \\text{ smallest of } R_1, \\ldots, R_n$.\n",
    "3.  Form the prediction interval: $\\hat{C}_n(x) = [\\hat{f}_n(x) - \\hat{q}_n, \\hat{f}_n(x) + \\hat{q}_n]$.\n",
    "\n",
    "The hope is that $\\mathbb{P}(Y_{n+1} \\in \\hat{C}_n(X_{n+1})) \\ge 1-\\alpha$. This is equivalent to $\\mathbb{P}(R_{n+1} \\le \\hat{q}_n)$, where $R_{n+1} = |Y_{n+1} - \\hat{f}_n(X_{n+1})|$ is the residual for the new test point.\n",
    "\n",
    "### Why This Naive Approach Fails\n",
    "\n",
    "This approach fails because the crucial property of **exchangeability is broken**. The test residual $R_{n+1}$ is **not exchangeable with the training residuals $R_1, \\ldots, R_n$**.\n",
    "\n",
    "The reason is that $\\hat{f}_n$ was trained on $(X_1, Y_1), \\ldots, (X_n, Y_n)$. It has \"seen\" this data. When a model is trained on data, it often fits the training data very well, leading to *smaller* training residuals. However, it has *not* seen $(X_{n+1}, Y_{n+1})$. Thus, the test residual $R_{n+1}$ will generally be stochastically *larger* than the training residuals $R_1, \\ldots, R_n$.\n",
    "\n",
    "Because $\\hat{q}_n$ is computed from the (potentially artificially small) training residuals, it will be too small. Consequently, the prediction interval $\\hat{C}_n(x)$ will be too narrow, and the actual coverage probability will be **less than the desired $1-\\alpha$**. This is known as **undercoverage**.\n",
    "\n",
    "This highlights the need for a more sophisticated strategy that ensures the scores used for calibration are treated symmetrically with respect to the test point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conformal_prediction_split"
   },
   "source": [
    "## 4. Split Conformal Prediction: The Second Key Idea\n",
    "\n",
    "To restore the crucial exchangeability property and achieve valid coverage in regression, we introduce the **second key idea**: constructing scores symmetrically. **Split Conformal Prediction (Split CP)** is the most common and computationally efficient way to do this.\n",
    "\n",
    "The core idea is to divide the data into two parts: one for training the base predictor, and another for calibrating the prediction set.\n",
    "\n",
    "### The Procedure:\n",
    "\n",
    "1.  **Data Splitting**: Divide the original dataset $D = \\{(X_i, Y_i)\\}_{i=1}^n$ into two disjoint sets:\n",
    "    * $D_1$: The **proper training set** (e.g., 50% of the data). Used *only* to train the base predictor $\\hat{f}_{n_1}$.\n",
    "    * $D_2$: The **calibration set** (e.g., the remaining 50% of the data). Used *only* to compute the conformal quantile.\n",
    "    Let $n_1 = |D_1|$ and $n_2 = |D_2|$.\n",
    "\n",
    "2.  **Train Base Predictor**: Fit your chosen point predictor $\\hat{f}_{n_1}$ (e.g., `LinearRegression`, `RandomForestRegressor`, `NeuralNetwork`) exclusively on the data in $D_1$.\n",
    "\n",
    "3.  **Compute Calibration Scores**: For each data point $(X_i, Y_i)$ in the calibration set $D_2$, compute a **non-conformity score** (often an absolute residual):\n",
    "    $$R_i = |Y_i - \\hat{f}_{n_1}(X_i)| \\quad \\text{for } i \\in D_2$$\n",
    "    Crucially, $\\hat{f}_{n_1}$ has *not* seen these $D_2$ points during its training.\n",
    "\n",
    "4.  **Compute Conformal Quantile**: Find the conformal quantile $\\hat{q}_{n_2}$ from these calibration scores:\n",
    "    $$\\hat{q}_{n_2} = \\text{the } \\lceil (1-\\alpha)(n_2+1) \\rceil \\text{ smallest of } \\{R_i\\}_{i \\in D_2}$$\n",
    "\n",
    "5.  **Form Conformal Prediction Set**: For a new test input $x$, the prediction set is:\n",
    "    $$\\hat{C}_n(x) = [\\hat{f}_{n_1}(x) - \\hat{q}_{n_2}, \\hat{f}_{n_1}(x) + \\hat{q}_{n_2}]$$\n",
    "\n",
    "### The Guarantee\n",
    "\n",
    "The key guarantee of split conformal prediction is:\n",
    "$$\\mathbb{P}(Y_{n+1} \\in \\hat{C}_n(X_{n+1}) | (X_i, Y_i)_{i \\in D_1}) \\in [1-\\alpha, 1-\\alpha + \\frac{1}{n_2+1})$$\n",
    "The lower bound always holds, and the upper bound holds if scores are distinct.\n",
    "\n",
    "**Why it Works**: By conditioning on $D_1$ (making $\\hat{f}_{n_1}$ fixed), the calibration scores $\\{R_i\\}_{i \\in D_2}$ and the test score $R_{n+1} = |Y_{n+1} - \\hat{f}_{n_1}(X_{n+1})|$ become conditionally i.i.d. This restores the exchangeability property, allowing the rank-based argument to apply.\n",
    "\n",
    "### Remarks on Split Conformal Prediction\n",
    "\n",
    "* **Protection Against Overfitting**: Split CP inherently protects against the overfitting issue that plagues the naive approach. The calibration residuals are unbiased with respect to the test point.\n",
    "* **Computational Efficiency**: It's highly efficient. You train the base model once on $D_1$, compute residuals on $D_2$ once, and then prediction involves a single model inference and a lookup of the precomputed quantile.\n",
    "* **Constant-Width Bands**: When using absolute residuals, the prediction bands are constant-width across the input space. This is a limitation if the noise or uncertainty varies with $X$. We'll address this later.\n",
    "* **Quality of Base Predictor**: The better the base predictor $\\hat{f}_{n_1}$ is (in terms of point prediction accuracy), the tighter (smaller) the conformal prediction bands will be, while still maintaining the coverage guarantee. This means you can use powerful ML models and still get valid uncertainty.\n",
    "\n",
    "### Code Example: Split Conformal Prediction for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_conformal_regression_code"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression  # Our base predictor\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "# --- 1. Simulate Regression Data ---\n",
    "num_samples = 200\n",
    "# Generate data with some noise\n",
    "X_data_np = np.linspace(-5, 5, num_samples)[:, None]  # Features (1D)\n",
    "y_true = 2 * X_data_np.squeeze() + 3  # True linear relationship\n",
    "noise_std = 5  # Constant noise standard deviation\n",
    "y_data_np = y_true + np.random.normal(0, noise_std, num_samples)  # Add Gaussian noise\n",
    "\n",
    "X_data = jnp.array(X_data_np)\n",
    "y_data = jnp.array(y_data_np)\n",
    "\n",
    "print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "# --- 2. Split Data into Proper Training (D1) and Calibration (D2) Sets ---\n",
    "# We'll use a 50/50 split for simplicity\n",
    "X_D1, X_D2, y_D1, y_D2 = train_test_split(\n",
    "    X_data, y_data, test_size=0.5, random_state=key.get(0).tolist()[0]\n",
    ")\n",
    "n1 = X_D1.shape[0]\n",
    "n2 = X_D2.shape[0]\n",
    "print(f\"Proper training set size (n1): {n1}\")\n",
    "print(f\"Calibration set size (n2): {n2}\\n\")\n",
    "\n",
    "# --- 3. Fit Predictor on Proper Training Set (D1) ---\n",
    "base_predictor = LinearRegression()\n",
    "# Scikit-learn models typically expect numpy arrays\n",
    "base_predictor.fit(np.array(X_D1), np.array(y_D1))\n",
    "\n",
    "# --- 4. Compute Calibration Scores (Absolute Residuals) on D2 ---\n",
    "y_D2_pred = base_predictor.predict(np.array(X_D2))\n",
    "calibration_scores = jnp.abs(y_D2 - jnp.array(y_D2_pred))\n",
    "print(f\"Calibration scores (absolute residuals, first 5): {calibration_scores[:5]}\\n\")\n",
    "\n",
    "# --- 5. Compute Conformal Quantile from Calibration Scores ---\n",
    "alpha_level = 0.1  # Desired coverage: 1 - alpha = 0.9 (90%)\n",
    "\n",
    "# Calculate the adjusted rank index for n2 calibration points\n",
    "adjusted_rank_index = jnp.ceil((1 - alpha_level) * (n2 + 1)).astype(int)\n",
    "\n",
    "# Sort calibration scores\n",
    "sorted_calibration_scores = jnp.sort(calibration_scores)\n",
    "\n",
    "# Get the conformal quantile (0-indexed)\n",
    "# Handle edge cases for rank_index, though for alpha in (0,1) and n2 > 0, it's usually fine.\n",
    "if adjusted_rank_index <= 0:\n",
    "    conformal_quantile = sorted_calibration_scores[0]\n",
    "elif adjusted_rank_index > n2:\n",
    "    conformal_quantile = sorted_calibration_scores[-1]\n",
    "else:\n",
    "    conformal_quantile = sorted_calibration_scores[adjusted_rank_index - 1]\n",
    "\n",
    "print(f\"Conformal quantile (q_n2): {conformal_quantile:.4f}\\n\")\n",
    "\n",
    "# --- 6. Form Conformal Prediction Band ---\n",
    "# Generate a range of test points for plotting the prediction band\n",
    "X_test_plot = jnp.linspace(X_data.min() - 1, X_data.max() + 1, 100)[:, None]\n",
    "y_test_pred_mean = base_predictor.predict(np.array(X_test_plot))\n",
    "\n",
    "# Construct the prediction band: [mean - quantile, mean + quantile]\n",
    "lower_bound = y_test_pred_mean - conformal_quantile\n",
    "upper_bound = y_test_pred_mean + conformal_quantile\n",
    "\n",
    "# --- Plotting Results ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(\n",
    "    X_D1[:, 0], y_D1, color=\"blue\", label=\"Proper Training Data (D1)\", alpha=0.7, s=30\n",
    ")\n",
    "plt.scatter(\n",
    "    X_D2[:, 0], y_D2, color=\"green\", label=\"Calibration Data (D2)\", alpha=0.7, s=30\n",
    ")\n",
    "plt.plot(\n",
    "    X_test_plot[:, 0],\n",
    "    y_test_pred_mean,\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    label=\"Regression Mean ($\\\\hat{f}_{n_1}$)\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    X_test_plot[:, 0],\n",
    "    lower_bound,\n",
    "    upper_bound,\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    "    label=f\"Conformal Prediction Band (1-$\\\\alpha$={1 - alpha_level})\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Split Conformal Prediction for Regression (Constant Width)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Simulate Test Coverage to Verify Guarantee ---\n",
    "num_test_points_sim = 1000\n",
    "# Generate new i.i.d. test data\n",
    "X_test_sim_np = np.random.uniform(\n",
    "    X_data.min(), X_data.max(), size=(num_test_points_sim, 1)\n",
    ")\n",
    "y_test_sim_true = (\n",
    "    2 * X_test_sim_np.squeeze()\n",
    "    + 3\n",
    "    + np.random.normal(0, noise_std, num_test_points_sim)\n",
    ")\n",
    "\n",
    "X_test_sim = jnp.array(X_test_sim_np)\n",
    "y_test_sim = jnp.array(y_test_sim_true)\n",
    "\n",
    "y_test_sim_pred_mean = base_predictor.predict(np.array(X_test_sim))\n",
    "\n",
    "coverage_count_sim = 0\n",
    "for i in range(num_test_points_sim):\n",
    "    # Check if the true Y_test_sim falls within the predicted interval\n",
    "    if (y_test_sim[i] >= (y_test_sim_pred_mean[i] - conformal_quantile)) and (\n",
    "        y_test_sim[i] <= (y_test_sim_pred_mean[i] + conformal_quantile)\n",
    "    ):\n",
    "        coverage_count_sim += 1\n",
    "\n",
    "simulated_coverage = coverage_count_sim / num_test_points_sim\n",
    "print(f\"Simulated test coverage: {simulated_coverage:.4f}\")\n",
    "print(f\"Desired coverage (at least): {1 - alpha_level:.4f}\")\n",
    "print(f\"Theoretical upper bound (approx): {1 - alpha_level + 1 / (n2 + 1):.4f}\")\n",
    "\n",
    "# The simulated coverage should be close to or slightly above the desired 1-alpha,\n",
    "# demonstrating the finite-sample guarantee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conformal_prediction_score_functions"
   },
   "source": [
    "## 5. Generalizing Conformity Scores\n",
    "\n",
    "The power of conformal prediction lies in its flexibility regarding the choice of **conformity score function**. While we used absolute residuals $|Y_i - \\hat{f}_{n_1}(X_i)|$ in the regression example, any score function that quantifies \"non-conformity\" (how unusual a point is with respect to the model) can be used.\n",
    "\n",
    "Let $V(x, y)$ be a **negatively-oriented score function**, meaning smaller values of $V(x,y)$ indicate better conformity (e.g., smaller prediction errors). The procedure remains the same:\n",
    "\n",
    "1.  Compute calibration scores $R_i = V(X_i, Y_i)$ for $i \\in D_2$.\n",
    "2.  Find $\\hat{q}_{n_2} = \\text{the } \\lceil (1-\\alpha)(n_2+1) \\rceil \\text{ smallest of } \\{R_i\\}_{i \\in D_2}$.\n",
    "3.  The prediction set is $\\hat{C}_n(x) = \\{y : V(x, y) \\le \\hat{q}_{n_2}\\}$.\n",
    "\n",
    "If you have a **positively-oriented score function** (where larger values are better, e.g., predicted probability of the true class in classification), you can either:\n",
    "1.  Negate it: Use $-V(x,y)$ as the negatively-oriented score.\n",
    "2.  Adjust the quantile rule: Find $\\hat{q}_{n_2} = \\text{the } \\lfloor \\alpha(n_2+1) \\rfloor \\text{ smallest of } \\{R_i\\}_{i \\in D_2}$, and form the set $\\hat{C}_n(x) = \\{y : V(x, y) \\ge \\hat{q}_{n_2}\\}$.\n",
    "\n",
    "### Quantile and CDF Formulations\n",
    "\n",
    "The prediction set can be expressed in equivalent ways:\n",
    "\n",
    "* **Quantile Form**:\n",
    "    $$\\hat{C}_n(x) = \\left\\{y : V(x,y) \\le \\text{Quantile}\\left(\\frac{\\lceil (1-\\alpha)(n_2+1) \\rceil}{n_2}; \\text{empirical distribution of } \\{R_i\\}_{i \\in D_2}\\right)\\right\\}$$\n",
    "    This highlights that we're comparing the test score $V(x,y)$ against an adjusted quantile of the calibration scores.\n",
    "\n",
    "* **CDF Form**:\n",
    "    $$\\hat{C}_n(x) = \\left\\{y : \\frac{1}{n_2}\\sum_{i \\in D_2}1\\{R_i < V(x,y)\\} \\le \\frac{\\lceil (1-\\alpha)(n_2+1) \\rceil}{n_2}\\right\\}$$\n",
    "    This uses the empirical cumulative distribution function (CDF) of the calibration scores.\n",
    "\n",
    "### Auxiliary Randomization for Exact Coverage\n",
    "\n",
    "While the guarantee is $[1-\\alpha, 1-\\alpha + 1/(n_2+1))$, we can achieve **exact coverage** of precisely $1-\\alpha$ by introducing a small amount of auxiliary randomization. This is particularly useful when there are ties in the conformity scores.\n",
    "\n",
    "The idea is to randomize the empirical CDF. For a score $R_{n+1}$ and calibration scores $\\{R_i\\}_{i \\in D_2}$, we define a randomized p-value:\n",
    "$$p^*(R_{n+1}) = \\frac{1}{n_2+1} \\left( \\sum_{i \\in D_2} 1\\{R_i < R_{n+1}\\} + U \\cdot \\left( \\sum_{i \\in D_2} 1\\{R_i = R_{n+1}\\} + 1 \\right) \\right)$$\n",
    "where $U \\sim \\text{Unif}(0,1)$ is an independent random variable.\n",
    "The prediction set is then $\\{y : p^*(V(x,y)) \\ge \\alpha\\}$. This guarantees $\\mathbb{P}(Y_{n+1} \\in \\hat{C}_n^*(X_{n+1})) = 1-\\alpha$.\n",
    "\n",
    "### Code Example: Auxiliary Randomization (Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auxiliary_randomization_code"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np  # For sorting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def calculate_randomized_p_value(\n",
    "    test_score: float, calibration_scores: jnp.ndarray, key: random.PRNGKey\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates a randomized p-value for a test score against calibration scores.\n",
    "    This helps achieve exact coverage when ties are present.\n",
    "\n",
    "    Args:\n",
    "        test_score: The conformity score of the test point.\n",
    "        calibration_scores: 1D array of conformity scores from the calibration set.\n",
    "        key: JAX random key for generating uniform random number.\n",
    "\n",
    "    Returns:\n",
    "        The randomized p-value.\n",
    "    \"\"\"\n",
    "    n2 = calibration_scores.shape[0]\n",
    "\n",
    "    # Count scores strictly less than test_score\n",
    "    num_less = jnp.sum(calibration_scores < test_score)\n",
    "\n",
    "    # Count scores equal to test_score\n",
    "    num_equal = jnp.sum(calibration_scores == test_score)\n",
    "\n",
    "    # Generate a uniform random number\n",
    "    U = random.uniform(key)\n",
    "\n",
    "    # Calculate the randomized p-value\n",
    "    # The +1 in the denominator accounts for the test score itself\n",
    "    randomized_p_val = (num_less + U * (num_equal + 1)) / (n2 + 1)\n",
    "\n",
    "    return randomized_p_val\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "key = random.PRNGKey(0)\n",
    "num_calibration_samples = 10\n",
    "alpha_level = 0.1  # Desired coverage: 0.9\n",
    "\n",
    "# Simulate calibration scores (introduce some ties for demonstration)\n",
    "calibration_scores_data = jnp.array([1.2, 0.8, 1.5, 0.8, 1.0, 1.2, 0.9, 1.1, 1.0, 1.3])\n",
    "print(f\"Calibration scores: {calibration_scores_data}\\n\")\n",
    "\n",
    "# Simulate a test score\n",
    "test_score_data = 1.05\n",
    "print(f\"Test score: {test_score_data}\\n\")\n",
    "\n",
    "# Calculate the randomized p-value\n",
    "p_value = calculate_randomized_p_value(\n",
    "    test_score_data, calibration_scores_data, key.get(0)\n",
    ")\n",
    "print(f\"Randomized p-value for test score {test_score_data}: {p_value:.4f}\")\n",
    "\n",
    "# Determine if the test score is \"in\" the conformal set (for a negatively-oriented score)\n",
    "# The set includes y if p_value(V(x,y)) >= alpha\n",
    "# For a negatively-oriented score, we want V(x,y) <= quantile\n",
    "# This means p-value should be >= alpha (if p-value is 1 - rank / (n+1) )\n",
    "# Or, if p-value is rank / (n+1), we want p-value <= (1-alpha)\n",
    "# Let's stick to the definition: Y_n+1 is in C_n(X_n+1) if p-value(R_n+1) <= alpha\n",
    "# For a negatively-oriented score, smaller R is better, so smaller p-value is better.\n",
    "# So we want p-value <= alpha.\n",
    "# However, the slides use p(y) = (1/n) Sum 1{R_i >= R_n+1} >= b_alpha(n+1)c/n.\n",
    "# This means higher p(y) is better. So p(y) >= alpha.\n",
    "\n",
    "\n",
    "def calculate_randomized_p_value_slide_style(\n",
    "    test_score: float, calibration_scores: jnp.ndarray, key: random.PRNGKey\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates a randomized p-value where higher values are better (slide style).\n",
    "    This is 1 - (rank / (n+1)).\n",
    "    \"\"\"\n",
    "    n2 = calibration_scores.shape[0]\n",
    "\n",
    "    # Count scores strictly greater than test_score\n",
    "    num_greater = jnp.sum(calibration_scores > test_score)\n",
    "\n",
    "    # Count scores equal to test_score\n",
    "    num_equal = jnp.sum(calibration_scores == test_score)\n",
    "\n",
    "    U = random.uniform(key)\n",
    "\n",
    "    randomized_p_val = (num_greater + U * num_equal) / (n2 + 1)\n",
    "\n",
    "    return randomized_p_val\n",
    "\n",
    "\n",
    "p_value_slide_style = calculate_randomized_p_value_slide_style(\n",
    "    test_score_data, calibration_scores_data, key.get(1)\n",
    ")\n",
    "print(\n",
    "    f\"Randomized p-value (slide style) for test score {test_score_data}: {p_value_slide_style:.4f}\"\n",
    ")\n",
    "\n",
    "# The conformal set in this style is {y : p(y) >= alpha}\n",
    "is_in_set = p_value_slide_style >= alpha_level\n",
    "print(f\"Is test score {test_score_data} in the conformal set (p >= alpha)? {is_in_set}\")\n",
    "\n",
    "# Simulate coverage for this style\n",
    "num_simulations_exact = 10000\n",
    "coverage_count_exact = 0\n",
    "\n",
    "for i in range(num_simulations_exact):\n",
    "    # Generate new calibration scores (same as original for simplicity, but could be new)\n",
    "    # For true simulation, you'd resample both calibration and test.\n",
    "    # Here, we just simulate a new test score and check against fixed calibration.\n",
    "    new_test_score = random.uniform(\n",
    "        key.get(i + 2), minval=0.5, maxval=1.6\n",
    "    )  # Assume scores are in this range\n",
    "\n",
    "    p_val_for_new_test = calculate_randomized_p_value_slide_style(\n",
    "        new_test_score, calibration_scores_data, key.get(i + 2)\n",
    "    )\n",
    "\n",
    "    if p_val_for_new_test >= alpha_level:\n",
    "        coverage_count_exact += 1\n",
    "\n",
    "simulated_coverage_exact = coverage_count_exact / num_simulations_exact\n",
    "print(\n",
    "    f\"\\nSimulated exact coverage (should be close to alpha): {simulated_coverage_exact:.4f}\"\n",
    ")\n",
    "print(f\"Desired coverage (alpha): {alpha_level:.4f}\")\n",
    "\n",
    "# Note: The simulated coverage should be very close to alpha_level (the threshold for p-value),\n",
    "# demonstrating the exact coverage property for the p-value itself.\n",
    "# For the full prediction set, the coverage is 1-alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conformal_prediction_local_adaptivity"
   },
   "source": [
    "## 6. Improving Local Adaptivity\n",
    "\n",
    "As noted, standard split conformal prediction with absolute residuals produces constant-width prediction bands. This is suboptimal when the uncertainty (e.g., noise level) varies across the input space. We want **local adaptivity**: narrower bands where prediction is easy, and wider bands where it's hard. This can be achieved by changing the conformity score function.\n",
    "\n",
    "### 6.1 Studentized Residuals\n",
    "\n",
    "A simple way to introduce local adaptivity is to use **studentized residuals**, which normalize the absolute residual by an estimate of the local spread.\n",
    "\n",
    "**Procedure (Split Conformal with Studentized Residuals):**\n",
    "\n",
    "1.  **Train Mean Predictor $\\hat{f}_{n_1}$**: Fit your base predictor on $D_1$ to predict the mean response.\n",
    "2.  **Train Spread Predictor $\\hat{\\sigma}_{n_1}$**: Also on $D_1$, train a *separate* model (e.g., another regression model) to predict the local spread of the residuals. This model $\\hat{\\sigma}_{n_1}(x)$ could be trained to predict $|Y_i - \\hat{f}_{n_1}(X_i)|$ (the absolute residuals from $D_1$).\n",
    "3.  **Compute Studentized Calibration Scores**: For $i \\in D_2$, compute:\n",
    "    $$R_i = \\frac{|Y_i - \\hat{f}_{n_1}(X_i)|}{\\hat{\\sigma}_{n_1}(X_i)}$$\n",
    "4.  **Compute Conformal Quantile**: Find $\\hat{q}_{n_2}$ from these studentized residuals.\n",
    "5.  **Form Conformal Set**:\n",
    "    $$\\hat{C}_n(x) = [\\hat{f}_{n_1}(x) - \\hat{\\sigma}_{n_1}(x)\\hat{q}_{n_2}, \\hat{f}_{n_1}(x) + \\hat{\\sigma}_{n_1}(x)\\hat{q}_{n_2}]$$\n",
    "The width of the band now scales with $\\hat{\\sigma}_{n_1}(x)$, adapting to local uncertainty. The coverage guarantee remains valid.\n",
    "\n",
    "### 6.2 Conformalized Quantile Regression (CQR)\n",
    "\n",
    "CQR (Romano et al., 2019) is a more principled approach to local adaptivity. Instead of predicting the mean and then the spread, it directly predicts quantiles.\n",
    "\n",
    "**Procedure (Split Conformal with CQR):**\n",
    "\n",
    "1.  **Train Quantile Predictors**: On $D_1$, fit *two* quantile regression models:\n",
    "    * $\\hat{f}^{\\alpha/2}_{n_1}(x)$: Estimates the $\\alpha/2$ quantile of $Y|X=x$.\n",
    "    * $\\hat{f}^{1-\\alpha/2}_{n_1}(x)$: Estimates the $1-\\alpha/2$ quantile of $Y|X=x$.\n",
    "    These can be trained using specific quantile regression algorithms (e.g., LightGBM with quantile loss, or neural networks with pinball loss).\n",
    "2.  **Compute CQR Calibration Scores**: For $i \\in D_2$, compute:\n",
    "    $$R_i = \\max\\left\\{ \\hat{f}^{\\alpha/2}_{n_1}(X_i) - Y_i, Y_i - \\hat{f}^{1-\\alpha/2}_{n_1}(X_i) \\right\\}$$\n",
    "    This score measures how far the true $Y_i$ is outside the predicted central quantile interval.\n",
    "3.  **Compute Conformal Quantile**: Find $\\hat{q}_{n_2}$ from these scores.\n",
    "4.  **Form Conformal Set**:\n",
    "    $$\\hat{C}_n(x) = [\\hat{f}^{\\alpha/2}_{n_1}(x) - \\hat{q}_{n_2}, \\hat{f}^{1-\\alpha/2}_{n_1}(x) + \\hat{q}_{n_2}]$$\n",
    "CQR often provides superior local adaptivity, especially for non-Gaussian conditional distributions.\n",
    "\n",
    "### Code Example: Studentized Residuals for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "studentized_residuals_code"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor  # Used for spread predictor\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "# --- 1. Simulate Heteroscedastic Regression Data ---\n",
    "# Data where the noise level changes with X\n",
    "num_samples = 300\n",
    "X_data_np = np.linspace(-5, 5, num_samples)[:, None]\n",
    "y_true = np.sin(X_data_np * 2) * 5  # A non-linear true function\n",
    "noise_level = 0.5 + 0.5 * np.abs(X_data_np)  # Noise increases with |X|\n",
    "y_data_np = y_true + noise_level * np.random.normal(0, 1, size=(num_samples, 1))\n",
    "\n",
    "X_data = jnp.array(X_data_np)\n",
    "y_data = jnp.array(y_data_np).squeeze()  # Ensure y_data is 1D\n",
    "\n",
    "print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "# --- 2. Split Data into Proper Training (D1) and Calibration (D2) Sets ---\n",
    "X_D1, X_D2, y_D1, y_D2 = train_test_split(\n",
    "    X_data, y_data, test_size=0.5, random_state=key.get(0).tolist()[0]\n",
    ")\n",
    "n1 = X_D1.shape[0]\n",
    "n2 = X_D2.shape[0]\n",
    "print(f\"Proper training set size (n1): {n1}\")\n",
    "print(f\"Calibration set size (n2): {n2}\\n\")\n",
    "\n",
    "# --- 3. Fit Point Predictor on D1 (e.g., Linear Regression or RandomForest) ---\n",
    "# Using RandomForestRegressor for mean prediction to better capture non-linearity\n",
    "mean_predictor = RandomForestRegressor(\n",
    "    n_estimators=100, random_state=key.get(1).tolist()[0]\n",
    ")\n",
    "mean_predictor.fit(np.array(X_D1), np.array(y_D1))\n",
    "\n",
    "# --- 4. Fit Spread Predictor on D1 ---\n",
    "# Compute absolute residuals on D1 for training the spread predictor\n",
    "y_D1_pred_mean = mean_predictor.predict(np.array(X_D1))\n",
    "abs_residuals_D1 = jnp.abs(y_D1 - jnp.array(y_D1_pred_mean))\n",
    "\n",
    "# Train a model to predict these absolute residuals (as a proxy for local spread)\n",
    "spread_predictor = RandomForestRegressor(\n",
    "    n_estimators=100, random_state=key.get(2).tolist()[0]\n",
    ")\n",
    "spread_predictor.fit(np.array(X_D1), np.array(abs_residuals_D1))\n",
    "\n",
    "# --- 5. Compute Studentized Calibration Residuals on D2 ---\n",
    "y_D2_pred_mean = mean_predictor.predict(np.array(X_D2))\n",
    "y_D2_pred_spread = spread_predictor.predict(np.array(X_D2))\n",
    "\n",
    "# Add a small epsilon to spread predictions to avoid division by zero\n",
    "# Ensure spread predictions are positive and non-zero\n",
    "y_D2_pred_spread_stable = jnp.maximum(jnp.array(y_D2_pred_spread), 1e-6)\n",
    "\n",
    "studentized_calibration_scores = (\n",
    "    jnp.abs(y_D2 - jnp.array(y_D2_pred_mean)) / y_D2_pred_spread_stable\n",
    ")\n",
    "print(\n",
    "    f\"Studentized calibration scores (first 5): {studentized_calibration_scores[:5]}\\n\"\n",
    ")\n",
    "\n",
    "# --- 6. Compute Conformal Quantile from Studentized Calibration Scores ---\n",
    "alpha_level = 0.1  # Desired coverage: 1 - alpha = 0.9 (90%)\n",
    "adjusted_rank_index = jnp.ceil((1 - alpha_level) * (n2 + 1)).astype(int)\n",
    "sorted_studentized_calibration_scores = jnp.sort(studentized_calibration_scores)\n",
    "\n",
    "if adjusted_rank_index <= 0:\n",
    "    conformal_quantile_studentized = sorted_studentized_calibration_scores[0]\n",
    "elif adjusted_rank_index > n2:\n",
    "    conformal_quantile_studentized = sorted_studentized_calibration_scores[-1]\n",
    "else:\n",
    "    conformal_quantile_studentized = sorted_studentized_calibration_scores[\n",
    "        adjusted_rank_index - 1\n",
    "    ]\n",
    "\n",
    "print(\n",
    "    f\"Conformal quantile for studentized residuals: {conformal_quantile_studentized:.4f}\\n\"\n",
    ")\n",
    "\n",
    "# --- 7. Form Conformal Prediction Band ---\n",
    "# Generate a range of test points for plotting the prediction band\n",
    "X_test_plot = jnp.linspace(X_data.min() - 1, X_data.max() + 1, 200)[:, None]\n",
    "y_test_pred_mean = mean_predictor.predict(np.array(X_test_plot))\n",
    "y_test_pred_spread = spread_predictor.predict(np.array(X_test_plot))\n",
    "\n",
    "# Ensure spread predictions are positive and non-zero for test points too\n",
    "y_test_pred_spread_stable = jnp.maximum(jnp.array(y_test_pred_spread), 1e-6)\n",
    "\n",
    "# Construct the prediction band: [mean - spread * quantile, mean + spread * quantile]\n",
    "lower_bound_studentized = (\n",
    "    y_test_pred_mean - y_test_pred_spread_stable * conformal_quantile_studentized\n",
    ")\n",
    "upper_bound_studentized = (\n",
    "    y_test_pred_mean + y_test_pred_spread_stable * conformal_quantile_studentized\n",
    ")\n",
    "\n",
    "# --- Plotting Results ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(\n",
    "    X_D1[:, 0], y_D1, color=\"blue\", label=\"Proper Training Data (D1)\", alpha=0.7, s=30\n",
    ")\n",
    "plt.scatter(\n",
    "    X_D2[:, 0], y_D2, color=\"green\", label=\"Calibration Data (D2)\", alpha=0.7, s=30\n",
    ")\n",
    "plt.plot(\n",
    "    X_test_plot[:, 0],\n",
    "    y_test_pred_mean,\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    label=\"Regression Mean ($\\\\hat{f}_{n_1}$)\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    X_test_plot[:, 0],\n",
    "    lower_bound_studentized,\n",
    "    upper_bound_studentized,\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    "    label=f\"Studentized Conformal Band (1-$\\\\alpha$={1 - alpha_level})\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Split Conformal Prediction with Studentized Residuals (Adaptive Width)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Simulate Test Coverage to Verify Guarantee ---\n",
    "num_test_points_sim = 1000\n",
    "# Generate new i.i.d. test data with heteroscedastic noise\n",
    "X_test_sim_np = np.random.uniform(\n",
    "    X_data.min(), X_data.max(), size=(num_test_points_sim, 1)\n",
    ")\n",
    "y_true_sim = np.sin(X_test_sim_np * 2) * 5\n",
    "noise_level_sim = 0.5 + 0.5 * np.abs(X_test_sim_np)\n",
    "y_test_sim_true = y_true_sim + noise_level_sim * np.random.normal(\n",
    "    0, 1, size=(num_test_points_sim, 1)\n",
    ")\n",
    "\n",
    "X_test_sim = jnp.array(X_test_sim_np)\n",
    "y_test_sim = jnp.array(y_test_sim_true).squeeze()\n",
    "\n",
    "y_test_sim_pred_mean = mean_predictor.predict(np.array(X_test_sim))\n",
    "y_test_sim_pred_spread = spread_predictor.predict(np.array(X_test_sim))\n",
    "y_test_sim_pred_spread_stable = jnp.maximum(jnp.array(y_test_sim_pred_spread), 1e-6)\n",
    "\n",
    "coverage_count_sim_studentized = 0\n",
    "for i in range(num_test_points_sim):\n",
    "    lower = (\n",
    "        y_test_sim_pred_mean[i]\n",
    "        - y_test_sim_pred_spread_stable[i] * conformal_quantile_studentized\n",
    "    )\n",
    "    upper = (\n",
    "        y_test_sim_pred_mean[i]\n",
    "        + y_test_sim_pred_spread_stable[i] * conformal_quantile_studentized\n",
    "    )\n",
    "    if (y_test_sim[i] >= lower) and (y_test_sim[i] <= upper):\n",
    "        coverage_count_sim_studentized += 1\n",
    "\n",
    "simulated_coverage_studentized = coverage_count_sim_studentized / num_test_points_sim\n",
    "print(f\"Simulated test coverage (Studentized): {simulated_coverage_studentized:.4f}\")\n",
    "print(f\"Desired coverage (at least): {1 - alpha_level:.4f}\")\n",
    "print(f\"Theoretical upper bound (approx): {1 - alpha_level + 1 / (n2 + 1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conformal_prediction_full"
   },
   "source": [
    "## 7. Full Conformal Prediction: Guaranteed Coverage Without Splitting\n",
    "\n",
    "Full Conformal Prediction (often just \"conformal prediction\" in older literature) is a more general approach that achieves the coverage guarantee **without splitting the data**. This means all $n$ data points can be used for training the base predictor. However, this comes at a significant computational cost.\n",
    "\n",
    "The core idea remains to treat all data symmetrically, but in a more intricate way.\n",
    "\n",
    "### The Procedure:\n",
    "\n",
    "1.  **Query Point Augmentation**: For any fixed test input $x \\in \\mathcal{X}$, we want to determine if a trial response value $y \\in \\mathcal{Y}$ should be included in the prediction set $\\hat{C}_n(x)$. We consider $(x,y)$ as a hypothetical \"query point.\"\n",
    "\n",
    "2.  **Train on Augmented Data**: For each trial value $y$, we train our prediction algorithm on an **augmented dataset** consisting of the original $n$ training points *plus* the single query point $(x,y)$:\n",
    "    $$D_{\\text{aug}} = \\{(X_1, Y_1), \\ldots, (X_n, Y_n), (x,y)\\}$$\n",
    "    This yields a point predictor $\\hat{f}_{n,(x,y)}$ that has been trained on $n+1$ points.\n",
    "\n",
    "3.  **Define Residuals (Non-conformity Scores)**: Compute residuals for *all* $n+1$ points in the augmented dataset using this newly trained predictor $\\hat{f}_{n,(x,y)}$:\n",
    "    $$R_i^{(x,y)} = |Y_i - \\hat{f}_{n,(x,y)}(X_i)| \\quad \\text{for } i=1, \\ldots, n$$\n",
    "    $$R_{n+1}^{(x,y)} = |y - \\hat{f}_{n,(x,y)}(x)| \\quad \\text{(residual for the query point itself)}$$\n",
    "\n",
    "4.  **Form Conformal Set**: The prediction set $\\hat{C}_n(x)$ is the set of all trial values $y$ for which the residual of the query point $R_{n+1}^{(x,y)}$ is less than or equal to the $\\lceil (1-\\alpha)(n+1) \\rceil$-th smallest of *all* $n+1$ residuals $\\{R_1^{(x,y)}, \\ldots, R_n^{(x,y)}, R_{n+1}^{(x,y)}\\}$:\n",
    "    $$\\hat{C}_n(x) = \\left\\{y : R_{n+1}^{(x,y)} \\le \\text{the } \\lceil (1-\\alpha)(n+1) \\rceil \\text{ smallest of } \\{R_j^{(x,y)}\\}_{j=1}^{n+1} \\right\\}$$\n",
    "\n",
    "### The Guarantee\n",
    "\n",
    "The guarantee for full conformal prediction is:\n",
    "$$\\mathbb{P}(Y_{n+1} \\in \\hat{C}_n(X_{n+1})) \\in [1-\\alpha, 1-\\alpha + \\frac{1}{n+1})$$\n",
    "This holds because, when the true test point $(X_{n+1}, Y_{n+1})$ is plugged in as the query point $(x,y)$, the resulting $n+1$ residuals become exchangeable (assuming the base predictor is a symmetric function of its training data).\n",
    "\n",
    "### Remarks on Full Conformal Prediction\n",
    "\n",
    "* **Computational Expense**: This is the major drawback. For each test input $x$, and for each candidate $y$ in a grid of possible responses, the base predictor must be *re-trained* on $n+1$ data points. This makes it extremely computationally intensive, especially for large $n$ or complex base predictors.\n",
    "* **Use Cases**: Rarely used in practice for large-scale problems, except when the base predictor has a \"shortcut\" for efficient re-training (e.g., kernel methods).\n",
    "* **Theoretical Elegance**: Despite its computational cost, it's theoretically very elegant as it avoids data splitting and uses all available data for model training.\n",
    "\n",
    "### Code Example: Full Conformal Prediction (Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "full_conformal_regression_code"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression  # Our base predictor\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "# --- 1. Simulate Regression Data ---\n",
    "num_samples = 20  # Small N for conceptual illustration\n",
    "X_data_np = np.linspace(0, 10, num_samples)[:, None]\n",
    "y_true = 0.5 * X_data_np.squeeze() + 1\n",
    "noise_std = 1.0\n",
    "y_data_np = y_true + np.random.normal(0, noise_std, num_samples)\n",
    "\n",
    "X_data = jnp.array(X_data_np)\n",
    "y_data = jnp.array(y_data_np)\n",
    "\n",
    "print(f\"Original training samples (n): {num_samples}\\n\")\n",
    "\n",
    "\n",
    "# --- Full Conformal Prediction Function ---\n",
    "def full_conformal_predict(\n",
    "    X_train: jnp.ndarray,\n",
    "    y_train: jnp.ndarray,\n",
    "    X_test_point: jnp.ndarray,  # Single test feature point x\n",
    "    alpha: float,\n",
    "    y_candidate_grid: jnp.ndarray,  # Grid of candidate y values for the prediction set\n",
    "    base_predictor_class,  # e.g., LinearRegression\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Conceptual implementation of Full Conformal Prediction for a single test point.\n",
    "    NOTE: This is computationally very expensive and not practical for large N.\n",
    "\n",
    "    Args:\n",
    "        X_train: Original training features.\n",
    "        y_train: Original training responses.\n",
    "        X_test_point: The feature value (x) for which to build the prediction set.\n",
    "        alpha: Nominal error level.\n",
    "        y_candidate_grid: A grid of y values to check for inclusion in the prediction set.\n",
    "        base_predictor_class: The class of the base predictor (e.g., LinearRegression).\n",
    "\n",
    "    Returns:\n",
    "        A boolean array indicating which y_candidate_grid values are in the set.\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    is_in_prediction_set = jnp.zeros(y_candidate_grid.shape[0], dtype=bool)\n",
    "\n",
    "    print(\n",
    "        f\"  Checking {y_candidate_grid.shape[0]} candidate y values for X_test_point={X_test_point.item():.2f}...\"\n",
    "    )\n",
    "\n",
    "    # For each candidate y in the grid\n",
    "    for idx, y_candidate in enumerate(y_candidate_grid):\n",
    "        # 1. Augment the training data with the query point (X_test_point, y_candidate)\n",
    "        X_augmented = jnp.vstack([X_train, X_test_point])\n",
    "        y_augmented = jnp.concatenate([y_train, jnp.array([y_candidate])])\n",
    "\n",
    "        # 2. Train the base predictor on the augmented data\n",
    "        current_predictor = base_predictor_class()\n",
    "        current_predictor.fit(np.array(X_augmented), np.array(y_augmented))\n",
    "\n",
    "        # 3. Define residuals for all n+1 points\n",
    "        y_augmented_pred = current_predictor.predict(np.array(X_augmented))\n",
    "        all_residuals = jnp.abs(y_augmented - jnp.array(y_augmented_pred))\n",
    "\n",
    "        # The residual for the query point is the last one\n",
    "        R_n_plus_1_query = all_residuals[-1]\n",
    "\n",
    "        # 4. Form Conformal Set: Compare R_n_plus_1_query to the quantile of all residuals\n",
    "        # The rank index is based on n+1 total residuals\n",
    "        adjusted_rank_index = jnp.ceil((1 - alpha) * (n + 1)).astype(int)\n",
    "\n",
    "        # Sort all residuals\n",
    "        sorted_all_residuals = jnp.sort(all_residuals)\n",
    "\n",
    "        # Get the threshold quantile (0-indexed)\n",
    "        # Handle potential edge cases for rank_index\n",
    "        if adjusted_rank_index <= 0:\n",
    "            threshold_quantile = sorted_all_residuals[0]\n",
    "        elif adjusted_rank_index > n + 1:  # Max index is n\n",
    "            threshold_quantile = sorted_all_residuals[-1]\n",
    "        else:\n",
    "            threshold_quantile = sorted_all_residuals[adjusted_rank_index - 1]\n",
    "\n",
    "        # Check if the query point's residual is less than or equal to the threshold\n",
    "        if R_n_plus_1_query <= threshold_quantile:\n",
    "            is_in_prediction_set = is_in_prediction_set.at[idx].set(True)\n",
    "\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"    Processed {idx + 1}/{y_candidate_grid.shape[0]} candidates...\")\n",
    "\n",
    "    return is_in_prediction_set\n",
    "\n",
    "\n",
    "# --- Example Usage for Full Conformal Prediction ---\n",
    "alpha_level = 0.1  # 90% coverage\n",
    "X_test_point = jnp.array(\n",
    "    [[5.0]]\n",
    ")  # A single test feature point for which to build the set\n",
    "\n",
    "# Define a grid of candidate y values for the prediction set\n",
    "y_candidate_grid = jnp.linspace(\n",
    "    y_data.min() - 5, y_data.max() + 5, 200\n",
    ")  # Increased grid density for better visualization\n",
    "\n",
    "print(\"Starting Full Conformal Prediction for a single test point...\")\n",
    "is_in_set_flags = full_conformal_predict(\n",
    "    X_data, y_data, X_test_point, alpha_level, y_candidate_grid, LinearRegression\n",
    ")\n",
    "\n",
    "# Extract the prediction set\n",
    "full_conformal_set_y_values = y_candidate_grid[is_in_set_flags]\n",
    "\n",
    "print(f\"\\nFull Conformal Prediction Set for X={X_test_point.item():.2f}:\")\n",
    "if full_conformal_set_y_values.shape[0] > 0:\n",
    "    print(f\"  Min Y: {full_conformal_set_y_values.min():.2f}\")\n",
    "    print(f\"  Max Y: {full_conformal_set_y_values.max():.2f}\")\n",
    "    print(\n",
    "        f\"  Approximate Width: {full_conformal_set_y_values.max() - full_conformal_set_y_values.min():.2f}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"  Prediction set is empty.\")\n",
    "\n",
    "# --- Plotting Results ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_data[:, 0], y_data, color=\"blue\", label=\"Training Data\", alpha=0.7, s=30)\n",
    "plt.axvline(X_test_point.item(), color=\"purple\", linestyle=\"--\", label=\"Test Point X\")\n",
    "\n",
    "# Plot the prediction set as a vertical band at X_test_point\n",
    "if full_conformal_set_y_values.shape[0] > 0:\n",
    "    plt.vlines(\n",
    "        X_test_point.item(),\n",
    "        full_conformal_set_y_values.min(),\n",
    "        full_conformal_set_y_values.max(),\n",
    "        colors=\"red\",\n",
    "        linewidth=4,\n",
    "        label=f\"Full Conformal Set (1-$\\\\alpha$={1 - alpha_level})\",\n",
    "    )\n",
    "    # Also plot the individual points that are in the set\n",
    "    plt.scatter(\n",
    "        jnp.full_like(full_conformal_set_y_values, X_test_point.item()),\n",
    "        full_conformal_set_y_values,\n",
    "        color=\"red\",\n",
    "        marker=\".\",\n",
    "        s=10,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Full Conformal Prediction (Conceptual)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Note: Simulating coverage for full conformal is extremely computationally demanding\n",
    "# as it would require running this function for many (X_n+1, Y_n+1) pairs.\n",
    "# The theoretical guarantee is the main assurance here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conformal_prediction_classification"
   },
   "source": [
    "## 8. Conformal Classification\n",
    "\n",
    "Conformal prediction can also be applied to **classification problems**, where the output $Y$ is a discrete class label (e.g., $Y \\in \\{1, \\ldots, K\\}$). The core idea is the same, but the conformity score functions need to be adapted for discrete outputs.\n",
    "\n",
    "We assume we have a probabilistic classifier $\\hat{f}_{n_1}$ (trained on $D_1$) that estimates class probabilities: $\\hat{f}_{n_1}(x; k)$ is the estimated probability of class $k$ given input $x$.\n",
    "\n",
    "### 8.1 Likelihood Scores\n",
    "\n",
    "A straightforward approach uses the predicted probability of the *true class* as the conformity score. This is a **positively-oriented score** (higher is better).\n",
    "\n",
    "**Procedure (Split Conformal with Likelihood Scores):**\n",
    "\n",
    "1.  **Train Probabilistic Classifier**: Fit $\\hat{f}_{n_1}$ on $D_1$ (e.g., Logistic Regression, Softmax Neural Network).\n",
    "2.  **Compute Calibration Scores**: For $(X_i, Y_i) \\in D_2$, compute:\n",
    "    $$R_i = \\hat{f}_{n_1}(X_i; Y_i)$$\n",
    "    (The predicted probability of the true class $Y_i$).\n",
    "3.  **Compute Conformal Quantile**: Since $R_i$ is positively-oriented, we use the lower quantile:\n",
    "    $$\\hat{q}_{n_2} = \\text{the } \\lfloor \\alpha(n_2+1) \\rfloor \\text{ smallest of } \\{R_i\\}_{i \\in D_2}$$\n",
    "4.  **Form Conformal Prediction Set**: For a new input $x$, the prediction set $\\hat{C}_n(x)$ includes all classes $k$ whose predicted probability is greater than or equal to this quantile:\n",
    "    $$\\hat{C}_n(x) = \\{k : \\hat{f}_{n_1}(x; k) \\ge \\hat{q}_{n_2}\\}$$\n",
    "This set can contain one, multiple, or no classes.\n",
    "\n",
    "### 8.2 Cumulative Likelihood (Adaptive Prediction Sets - APS / RAPS)\n",
    "\n",
    "To achieve better local adaptivity and often smaller prediction sets, the **cumulative likelihood score** is used (Romano et al., 2020). This is a **negatively-oriented score**.\n",
    "\n",
    "**Procedure (Split Conformal with Cumulative Likelihood Scores):**\n",
    "\n",
    "1.  **Train Probabilistic Classifier**: Fit $\\hat{f}_{n_1}$ on $D_1$.\n",
    "2.  **Compute Calibration Scores**: For each $(X_i, Y_i) \\in D_2$:\n",
    "    * Sort the predicted probabilities $\\hat{f}_{n_1}(X_i; k)$ in decreasing order to get a permutation $\\pi_i$.\n",
    "    * Find the rank $k_i$ of the true class $Y_i$ in this sorted list (i.e., $\\pi_i(k_i) = Y_i$).\n",
    "    * Compute the cumulative probability up to the true class's rank:\n",
    "        $$R_i = \\sum_{j=1}^{k_i} \\hat{f}_{n_1}(X_i; \\pi_i(j))$$\n",
    "    This score represents the cumulative probability of all classes \"at least as likely\" as the true class.\n",
    "3.  **Compute Conformal Quantile**: Find $\\hat{q}_{n_2}$ from these negatively-oriented scores:\n",
    "    $$\\hat{q}_{n_2} = \\text{the } \\lceil (1-\\alpha)(n_2+1) \\rceil \\text{ smallest of } \\{R_i\\}_{i \\in D_2}$$\n",
    "4.  **Form Conformal Prediction Set**: For a new input $x$:\n",
    "    * Sort the predicted probabilities $\\hat{f}_{n_1}(x; k)$ in decreasing order to get $\\pi_x$.\n",
    "    * Find the smallest $k_x$ such that the cumulative probability of the top $k_x$ classes is less than or equal to $\\hat{q}_{n_2}$:\n",
    "        $$k_x = \\min \\left( k : \\sum_{j=1}^k \\hat{f}_{n_1}(x; \\pi_x(j)) \\le \\hat{q}_{n_2} \\right)$$\n",
    "    * The prediction set is the top $k_x$ most likely classes:\n",
    "        $$\\hat{C}_n(x) = \\{\\pi_x(1), \\ldots, \\pi_x(k_x)\\}$$\n",
    "This method allows for more adaptive prediction set sizes.\n",
    "\n",
    "### Code Example: Conformal Classification (Likelihood Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conformal_classification_likelihood_code"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression  # A probabilistic classifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "# --- 1. Simulate Classification Data ---\n",
    "num_samples = 200\n",
    "num_classes = 3  # For multi-class classification\n",
    "X_data_np, y_data_np = make_classification(\n",
    "    n_samples=num_samples,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    n_classes=num_classes,\n",
    "    random_state=key.get(0).tolist()[0],\n",
    ")\n",
    "X_data = jnp.array(X_data_np)\n",
    "y_data = jnp.array(y_data_np)\n",
    "\n",
    "print(f\"Total samples: {num_samples}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# --- 2. Split Data into Proper Training (D1) and Calibration (D2) Sets ---\n",
    "X_D1, X_D2, y_D1, y_D2 = train_test_split(\n",
    "    X_data, y_data, test_size=0.5, random_state=key.get(1).tolist()[0]\n",
    ")\n",
    "n1 = X_D1.shape[0]\n",
    "n2 = X_D2.shape[0]\n",
    "print(f\"Proper training set size (n1): {n1}\")\n",
    "print(f\"Calibration set size (n2): {n2}\\n\")\n",
    "\n",
    "# --- 3. Fit Probabilistic Classifier on Proper Training Set (D1) ---\n",
    "# LogisticRegression provides predict_proba() for class probabilities\n",
    "classifier = LogisticRegression(\n",
    "    random_state=key.get(2).tolist()[0], solver=\"lbfgs\", multi_class=\"multinomial\"\n",
    ")\n",
    "classifier.fit(np.array(X_D1), np.array(y_D1))\n",
    "\n",
    "# --- 4. Compute Calibration Scores (Likelihood Scores) on D2 ---\n",
    "# Get predicted probabilities for calibration set\n",
    "y_D2_pred_proba = classifier.predict_proba(np.array(X_D2))  # Shape (n2, num_classes)\n",
    "\n",
    "# Calibration score R_i = P(Y_i | X_i) for the true class Y_i\n",
    "# We select the probability corresponding to the true class label for each calibration point\n",
    "calibration_scores = y_D2_pred_proba[jnp.arange(n2), y_D2]\n",
    "print(\n",
    "    f\"Calibration scores (predicted likelihoods for true class, first 5): {calibration_scores[:5]}\\n\"\n",
    ")\n",
    "\n",
    "# --- 5. Compute Conformal Quantile from Calibration Scores ---\n",
    "alpha_level = 0.1  # Desired coverage: 1 - alpha = 0.9 (90%)\n",
    "\n",
    "# For positively-oriented scores (higher is better), the conformal quantile is the\n",
    "# floor(alpha * (n2 + 1))-th smallest score.\n",
    "# This corresponds to the (alpha * (n2 + 1))-th percentile.\n",
    "adjusted_rank_index = jnp.floor(alpha_level * (n2 + 1)).astype(int)\n",
    "\n",
    "sorted_calibration_scores = jnp.sort(calibration_scores)\n",
    "\n",
    "# Get the conformal quantile (0-indexed)\n",
    "# Handle edge cases: if adjusted_rank_index is 0, use the smallest score.\n",
    "# If adjusted_rank_index is n2 or more, use the largest score.\n",
    "if adjusted_rank_index < 0:\n",
    "    conformal_quantile_class = sorted_calibration_scores[0]\n",
    "elif adjusted_rank_index >= n2:\n",
    "    conformal_quantile_class = sorted_calibration_scores[-1]\n",
    "else:\n",
    "    conformal_quantile_class = sorted_calibration_scores[adjusted_rank_index]\n",
    "\n",
    "print(f\"Conformal quantile (q_n2) for classification: {conformal_quantile_class:.4f}\\n\")\n",
    "\n",
    "# --- 6. Form Conformal Prediction Set for a New Test Point ---\n",
    "# Let's pick a single test point for demonstration\n",
    "X_test_single = jnp.array([[0.5, 0.5]])  # Example test point\n",
    "# For checking coverage, assume true class is 1 for this point\n",
    "y_test_single_true_class = 1\n",
    "\n",
    "# Get predicted probabilities for the test point across all classes\n",
    "prob_test_single = classifier.predict_proba(np.array(X_test_single))[\n",
    "    0\n",
    "]  # Shape (num_classes,)\n",
    "print(f\"Predicted probabilities for X_test_single: {prob_test_single}\")\n",
    "\n",
    "# Construct the prediction set: {k : P(Y=k|X) >= q_n2}\n",
    "prediction_set = []\n",
    "for k in range(num_classes):\n",
    "    if prob_test_single[k] >= conformal_quantile_class:\n",
    "        prediction_set.append(k)\n",
    "\n",
    "print(f\"Conformal Prediction Set for X_test_single: {prediction_set}\")\n",
    "if y_test_single_true_class in prediction_set:\n",
    "    print(f\"True class {y_test_single_true_class} IS in the prediction set.\\n\")\n",
    "else:\n",
    "    print(f\"True class {y_test_single_true_class} IS NOT in the prediction set.\\n\")\n",
    "\n",
    "\n",
    "# --- Simulate Test Coverage to Verify Guarantee ---\n",
    "num_test_points_sim = 1000\n",
    "X_test_sim_np, y_test_sim_np = make_classification(\n",
    "    n_samples=num_test_points_sim,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    n_classes=num_classes,\n",
    "    random_state=key.get(3).tolist()[0],\n",
    ")\n",
    "X_test_sim = jnp.array(X_test_sim_np)\n",
    "y_test_sim = jnp.array(y_test_sim_np)\n",
    "\n",
    "y_test_sim_pred_proba = classifier.predict_proba(np.array(X_test_sim))\n",
    "\n",
    "coverage_count_sim_class = 0\n",
    "for i in range(num_test_points_sim):\n",
    "    predicted_probs_for_point = y_test_sim_pred_proba[i]\n",
    "    true_class_for_point = y_test_sim[i]\n",
    "\n",
    "    # Check if the true class's probability is above the conformal quantile\n",
    "    if predicted_probs_for_point[true_class_for_point] >= conformal_quantile_class:\n",
    "        coverage_count_sim_class += 1\n",
    "\n",
    "simulated_coverage_class = coverage_count_sim_class / num_test_points_sim\n",
    "print(f\"Simulated test coverage (Classification): {simulated_coverage_class:.4f}\")\n",
    "print(f\"Desired coverage (at least): {1 - alpha_level:.4f}\")\n",
    "print(f\"Theoretical upper bound (approx): {1 - alpha_level + 1 / (n2 + 1):.4f}\")\n",
    "\n",
    "# --- Plotting Decision Regions (Conceptual) ---\n",
    "# For classification, visualizing prediction sets (which can be multi-valued) is complex.\n",
    "# Here, we'll visualize the decision boundary for the most likely class, and the conformal threshold.\n",
    "x1_min, x1_max = X_data[:, 0].min() - 0.5, X_data[:, 0].max() + 0.5\n",
    "x2_min, x2_max = X_data[:, 1].min() - 0.5, X_data[:, 1].max() + 0.5\n",
    "xx1, xx2 = jnp.meshgrid(\n",
    "    jnp.linspace(x1_min, x1_max, 100), jnp.linspace(x2_min, x2_max, 100)\n",
    ")\n",
    "X_grid = jnp.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "# Predict probabilities on the grid for all classes\n",
    "prob_grid_all_classes = classifier.predict_proba(np.array(X_grid))\n",
    "# Get the probability of the most likely class for visualization\n",
    "max_prob_grid = jnp.max(prob_grid_all_classes, axis=1).reshape(xx1.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Contour plot of the maximum predicted probability\n",
    "plt.contourf(\n",
    "    xx1, xx2, max_prob_grid, levels=jnp.linspace(0, 1, 11), cmap=\"viridis\", alpha=0.6\n",
    ")\n",
    "plt.colorbar(label=\"Max Predicted Probability\")\n",
    "\n",
    "# Plot the training data points\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "markers = [\"o\", \"x\", \"s\"]\n",
    "labels = [f\"Class {k}\" for k in range(num_classes)]\n",
    "for k in range(num_classes):\n",
    "    plt.scatter(\n",
    "        X_data[y_data == k, 0],\n",
    "        X_data[y_data == k, 1],\n",
    "        color=colors[k],\n",
    "        marker=markers[k],\n",
    "        label=labels[k],\n",
    "        edgecolor=\"black\",\n",
    "        s=50,\n",
    "    )\n",
    "\n",
    "# Add a contour line for the conformal quantile threshold (for the most likely class)\n",
    "# This shows regions where even the most likely class might not meet the threshold.\n",
    "plt.contour(\n",
    "    xx1,\n",
    "    xx2,\n",
    "    max_prob_grid,\n",
    "    levels=[conformal_quantile_class],\n",
    "    colors=\"white\",\n",
    "    linewidths=2,\n",
    "    linestyles=\"--\",\n",
    "    label=\"Conformal Threshold\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Max Predicted Probability and Conformal Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conformal_prediction_classification_cumulative_conceptual"
   },
   "source": [
    "### Code Example: Conformal Classification (Cumulative Likelihood - Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conformal_classification_cumulative_code"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression  # A probabilistic classifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "# --- 1. Simulate Classification Data ---\n",
    "num_samples = 200\n",
    "num_classes = 3\n",
    "X_data_np, y_data_np = make_classification(\n",
    "    n_samples=num_samples,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    n_classes=num_classes,\n",
    "    random_state=key.get(0).tolist()[0],\n",
    ")\n",
    "X_data = jnp.array(X_data_np)\n",
    "y_data = jnp.array(y_data_np)\n",
    "\n",
    "# --- 2. Split Data ---\n",
    "X_D1, X_D2, y_D1, y_D2 = train_test_split(\n",
    "    X_data, y_data, test_size=0.5, random_state=key.get(1).tolist()[0]\n",
    ")\n",
    "n2 = X_D2.shape[0]\n",
    "\n",
    "# --- 3. Fit Probabilistic Classifier ---\n",
    "classifier = LogisticRegression(\n",
    "    random_state=key.get(2).tolist()[0], solver=\"lbfgs\", multi_class=\"multinomial\"\n",
    ")\n",
    "classifier.fit(np.array(X_D1), np.array(y_D1))\n",
    "\n",
    "# --- 4. Compute Calibration Scores (Cumulative Likelihood) on D2 ---\n",
    "y_D2_pred_proba = classifier.predict_proba(np.array(X_D2))  # Shape (n2, num_classes)\n",
    "\n",
    "cumulative_calibration_scores = jnp.zeros(n2)\n",
    "\n",
    "for i in range(n2):\n",
    "    true_class = y_D2[i]\n",
    "    predicted_probs = y_D2_pred_proba[i]\n",
    "\n",
    "    # Get the permutation that sorts probabilities in decreasing order\n",
    "    # argsort returns indices that would sort an array. We want descending order.\n",
    "    sorted_indices = jnp.argsort(predicted_probs)[::-1]\n",
    "\n",
    "    cumulative_sum = 0.0\n",
    "    for j, class_idx in enumerate(sorted_indices):\n",
    "        cumulative_sum += predicted_probs[class_idx]\n",
    "        if class_idx == true_class:\n",
    "            # This is the cumulative probability of all classes \"at least as likely\" as the true one\n",
    "            cumulative_calibration_scores = cumulative_calibration_scores.at[i].set(\n",
    "                cumulative_sum\n",
    "            )\n",
    "            break\n",
    "print(\n",
    "    f\"Cumulative likelihood calibration scores (first 5): {cumulative_calibration_scores[:5]}\\n\"\n",
    ")\n",
    "\n",
    "# --- 5. Compute Conformal Quantile ---\n",
    "alpha_level = 0.1  # Desired coverage: 1 - alpha = 0.9 (90%)\n",
    "\n",
    "# For negatively-oriented scores (higher is worse), we use ceil((1-alpha)*(n2+1)) smallest.\n",
    "adjusted_rank_index = jnp.ceil((1 - alpha_level) * (n2 + 1)).astype(int)\n",
    "sorted_cumulative_scores = jnp.sort(cumulative_calibration_scores)\n",
    "\n",
    "if adjusted_rank_index <= 0:\n",
    "    conformal_quantile_cumulative = sorted_cumulative_scores[0]\n",
    "elif adjusted_rank_index > n2:\n",
    "    conformal_quantile_cumulative = sorted_cumulative_scores[-1]\n",
    "else:\n",
    "    conformal_quantile_cumulative = sorted_cumulative_scores[adjusted_rank_index - 1]\n",
    "\n",
    "print(\n",
    "    f\"Conformal quantile (cumulative likelihood): {conformal_quantile_cumulative:.4f}\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- 6. Form Conformal Prediction Set for a New Test Point ---\n",
    "def get_conformal_set_cumulative(\n",
    "    X_point: jnp.ndarray, classifier_model, conformal_q: float, num_classes: int\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Forms the conformal prediction set using cumulative likelihood.\n",
    "    \"\"\"\n",
    "    predicted_probs = classifier_model.predict_proba(np.array(X_point))[0]\n",
    "\n",
    "    # Get the permutation that sorts probabilities in decreasing order\n",
    "    sorted_indices = jnp.argsort(predicted_probs)[::-1]\n",
    "\n",
    "    prediction_set_classes = []\n",
    "    cumulative_prob_sum = 0.0\n",
    "\n",
    "    for class_idx in sorted_indices:\n",
    "        cumulative_prob_sum += predicted_probs[class_idx]\n",
    "        prediction_set_classes.append(class_idx.item())  # Add to set\n",
    "\n",
    "        # Stop when cumulative probability exceeds the conformal quantile\n",
    "        if cumulative_prob_sum > conformal_q:\n",
    "            break\n",
    "\n",
    "    return prediction_set_classes\n",
    "\n",
    "\n",
    "# Example test point\n",
    "X_test_single = jnp.array([[0.5, 0.5]])\n",
    "y_test_single_true_class = 1  # For checking\n",
    "\n",
    "prediction_set_cumulative = get_conformal_set_cumulative(\n",
    "    X_test_single, classifier, conformal_quantile_cumulative, num_classes\n",
    ")\n",
    "print(\n",
    "    f\"Conformal Prediction Set (Cumulative Likelihood) for X_test_single: {prediction_set_cumulative}\"\n",
    ")\n",
    "if y_test_single_true_class in prediction_set_cumulative:\n",
    "    print(f\"True class {y_test_single_true_class} IS in the prediction set.\\n\")\n",
    "else:\n",
    "    print(f\"True class {y_test_single_true_class} IS NOT in the prediction set.\\n\")\n",
    "\n",
    "# --- Simulate Test Coverage ---\n",
    "num_test_points_sim = 1000\n",
    "X_test_sim_np, y_test_sim_np = make_classification(\n",
    "    n_samples=num_test_points_sim,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    n_classes=num_classes,\n",
    "    random_state=key.get(3).tolist()[0],\n",
    ")\n",
    "X_test_sim = jnp.array(X_test_sim_np)\n",
    "y_test_sim = jnp.array(y_test_sim_np)\n",
    "\n",
    "coverage_count_sim_cumulative = 0\n",
    "for i in range(num_test_points_sim):\n",
    "    X_point = X_test_sim[i : i + 1]  # Slice to keep 2D shape\n",
    "    true_class = y_test_sim[i]\n",
    "\n",
    "    current_prediction_set = get_conformal_set_cumulative(\n",
    "        X_point, classifier, conformal_quantile_cumulative, num_classes\n",
    "    )\n",
    "\n",
    "    if true_class in current_prediction_set:\n",
    "        coverage_count_sim_cumulative += 1\n",
    "\n",
    "simulated_coverage_cumulative = coverage_count_sim_cumulative / num_test_points_sim\n",
    "print(\n",
    "    f\"Simulated test coverage (Cumulative Likelihood): {simulated_coverage_cumulative:.4f}\"\n",
    ")\n",
    "print(f\"Desired coverage (at least): {1 - alpha_level:.4f}\")\n",
    "print(f\"Theoretical upper bound (approx): {1 - alpha_level + 1 / (n2 + 1):.4f}\")\n",
    "\n",
    "# --- Plotting (Conceptual) ---\n",
    "# Visualizing adaptive set sizes is harder for multi-class.\n",
    "# We can plot the decision boundary for the most likely class, similar to before.\n",
    "x1_min, x1_max = X_data[:, 0].min() - 0.5, X_data[:, 0].max() + 0.5\n",
    "x2_min, x2_max = X_data[:, 1].min() - 0.5, X_data[:, 1].max() + 0.5\n",
    "xx1, xx2 = jnp.meshgrid(\n",
    "    jnp.linspace(x1_min, x1_max, 100), jnp.linspace(x2_min, x2_max, 100)\n",
    ")\n",
    "X_grid = jnp.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "prob_grid_all_classes = classifier.predict_proba(np.array(X_grid))\n",
    "# For visualization, let's plot the probability of the most likely class\n",
    "max_prob_grid = jnp.max(prob_grid_all_classes, axis=1).reshape(xx1.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(\n",
    "    xx1, xx2, max_prob_grid, levels=jnp.linspace(0, 1, 11), cmap=\"viridis\", alpha=0.6\n",
    ")\n",
    "plt.colorbar(label=\"Max Predicted Probability\")\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "markers = [\"o\", \"x\", \"s\"]\n",
    "labels = [f\"Class {k}\" for k in range(num_classes)]\n",
    "for k in range(num_classes):\n",
    "    plt.scatter(\n",
    "        X_data[y_data == k, 0],\n",
    "        X_data[y_data == k, 1],\n",
    "        color=colors[k],\n",
    "        marker=markers[k],\n",
    "        label=labels[k],\n",
    "        edgecolor=\"black\",\n",
    "        s=50,\n",
    "    )\n",
    "\n",
    "# Add a contour line for the conformal quantile threshold (for the most likely class)\n",
    "# This is a simplification, as the actual set depends on cumulative probabilities.\n",
    "plt.contour(\n",
    "    xx1,\n",
    "    xx2,\n",
    "    max_prob_grid,\n",
    "    levels=[conformal_quantile_cumulative],\n",
    "    colors=\"white\",\n",
    "    linewidths=2,\n",
    "    linestyles=\"--\",\n",
    "    label=\"Conformal Threshold\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Max Predicted Probability and Cumulative Conformal Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conformal_prediction_conclusion"
   },
   "source": [
    "## Conclusion: The Power of Conformal Prediction\n",
    "\n",
    "Conformal prediction offers a unique and powerful approach to uncertainty quantification in machine learning. Its ability to provide **distribution-free, finite-sample coverage guarantees** for prediction sets makes it an invaluable tool, especially in high-stakes applications where reliability is paramount.\n",
    "\n",
    "Key takeaways from this notebook:\n",
    "\n",
    "* **Rigorous Guarantees**: Unlike many methods that rely on asymptotic approximations or strong distributional assumptions, conformal prediction provides a hard guarantee on the coverage of its prediction sets.\n",
    "* **Model Agnostic**: It can be wrapped around any existing machine learning model, allowing you to leverage the predictive power of complex algorithms while adding a layer of statistical validity.\n",
    "* **Exchangeability is Key**: The core principle relies on the exchangeability of conformity scores, which is achieved through clever data splitting (Split Conformal) or symmetric re-training (Full Conformal).\n",
    "* **Computational Efficiency (Split CP)**: Split conformal prediction is highly practical and computationally efficient, making it suitable for large datasets.\n",
    "* **Local Adaptivity**: By choosing appropriate conformity scores (e.g., studentized residuals, CQR, cumulative likelihood), prediction sets can adapt their size to the local difficulty of prediction, providing more informative uncertainty estimates.\n",
    "* **Versatile Applications**: Applicable to both regression (continuous outputs) and classification (discrete outputs), yielding prediction intervals and prediction sets, respectively.\n",
    "\n",
    "While challenges like the impossibility of exact X-conditional coverage exist, conformal prediction provides a robust and theoretically sound framework for building trustworthy machine learning systems."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP5G65t64m+14+14+14+14",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
