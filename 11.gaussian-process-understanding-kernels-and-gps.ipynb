{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1feb908",
   "metadata": {},
   "source": [
    "# Understanding Kernels and Gaussian Processes: A Deeper Dive\n",
    "\n",
    "Welcome back to our **Gaussian Processes (GPs)** blog series! So far, we've introduced GPs as distributions over functions, explored various kernel functions, and seen how to apply them to real-world data like the Mauna Loa CO₂ measurements.\n",
    "\n",
    "In this post, we'll peel back another layer of abstraction to truly understand what Gaussian Processes and kernels are. We'll address some fundamental questions that often arise when first learning about GPs:\n",
    "\n",
    "---\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "- **Are Gaussian Processes truly probability distributions over functions?**  \n",
    "    What kind of functions do they define?\n",
    "\n",
    "- **What exactly are kernels?**  \n",
    "    Can we think of them as \"infinitely large matrices\"?\n",
    "\n",
    "- **What's the connection between GPs and \"kernel machines\"?**  \n",
    "    How do concepts from other machine learning courses relate to GPs?\n",
    "\n",
    "- **If GPs can use infinitely many features, can they learn any function?**\n",
    "\n",
    "---\n",
    "\n",
    "This lecture will provide deeper theoretical insights, connecting GPs to concepts from functional analysis and linear algebra. We'll break down each question, build intuition with examples, and use mathematical tools to clarify these powerful ideas.\n",
    "\n",
    "Stay tuned as we explore the mathematical foundations and practical implications of kernels and Gaussian Processes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529a8f7",
   "metadata": {},
   "source": [
    "## Recap: What We've Learned So Far & Key Questions\n",
    "\n",
    "Let's briefly summarize the main ideas from our previous discussions:\n",
    "\n",
    "- **Inference with Gaussians:**  \n",
    "    Inference in models involving linear relationships between Gaussian random variables can be performed using linear algebra. This enables us to compute posterior distributions analytically, making Gaussian models both elegant and practical.\n",
    "\n",
    "- **Features for Non-linearity:**  \n",
    "    By applying feature mappings $\\phi(x)$, we can extend linear models to capture complex, non-linear relationships. This allows us to model a wide variety of real-valued functions across different domains.\n",
    "\n",
    "- **Feature Learning (Type-II Maximum Likelihood):**  \n",
    "    We can optimize feature representations—or kernel hyperparameters—by maximizing the marginal likelihood. This approach, also known as Type-II Maximum Likelihood or empirical Bayes, helps us learn the most suitable features for our data.\n",
    "\n",
    "- **Gaussian Process Models:**  \n",
    "    Gaussian Processes (GPs) enable us to work with models that effectively use infinitely many features, all within finite computation time. The core components of a GP are its mean function and a positive definite covariance function (also called a Mercer kernel).\n",
    "\n",
    "---\n",
    "\n",
    "These foundational ideas naturally lead to several important questions:\n",
    "\n",
    "- **Are Gaussian Processes a probability measure?**  \n",
    "    Over what space? Do they truly define \"random functions\"?\n",
    "\n",
    "- **What are kernels?**  \n",
    "    Can we think of them as \"infinitely large matrices\"?\n",
    "\n",
    "- **How do kernel machines relate to GPs?**  \n",
    "    In statistical machine learning, kernel machines are a common concept. What is their connection to Gaussian Processes?\n",
    "\n",
    "- **If GPs or kernel machines use infinitely many features, can they learn every function?**\n",
    "\n",
    "---\n",
    "\n",
    "Understanding these questions will deepen our intuition for Gaussian Processes and kernels, and clarify their role in modern machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f6e59",
   "metadata": {},
   "source": [
    "## Goals for Today: Three Key Insights\n",
    "\n",
    "Today's lecture will focus on three crucial insights into Gaussian Processes (GPs) and kernels:\n",
    "\n",
    "1. **GPs as Probability Distributions over Functions**  \n",
    "    - We'll confirm that Gaussian Processes truly define a *probability distribution over functions*.\n",
    "    - We'll discuss how the associated function space is very general, and why understanding its structure requires a closer look at the kernel.\n",
    "\n",
    "2. **Connection to Frequentist Kernel Methods**  \n",
    "    - The covariance function in a GP is a *kernel*, which opens a direct connection to frequentist kernel methods.\n",
    "    - By comparing these approaches, we can uncover both the relationships and the differences between frequentist and Bayesian perspectives.\n",
    "\n",
    "3. **Bayesian-Frequentist Interplay & Limitations**  \n",
    "    - Bayesian analysis can be supported by frequentist tools, and vice versa.\n",
    "    - However, it's important not to assume these approaches are always equivalent.\n",
    "    - Nonparametric models, like GPs, are extremely powerful—but not omnipotent. The size of a model class does not necessarily guarantee fast convergence or perfect learning.\n",
    "\n",
    "---\n",
    "\n",
    "By the end of this lecture, you'll have a deeper understanding of:\n",
    "\n",
    "- How GPs define distributions over functions,\n",
    "- The mathematical and conceptual connections between Bayesian and frequentist kernel methods,\n",
    "- And the practical limitations and subtleties of nonparametric models in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d9465",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## What is a Gaussian Process? A More Careful Definition\n",
    "\n",
    "Let's start with a more formal and careful definition of a **Gaussian Process (GP)** as a stochastic process.\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition: Gaussian Process**\n",
    "\n",
    "A **Gaussian Process** $f$ with index set $X$ is a family of $\\mathbb{R}$-valued random variables $\\omega \\mapsto f(x, \\omega)$ for $x \\in X$ on a common probability space $(\\Omega, \\mathcal{F}, P)$ such that **every finite collection** $f(x_1, \\cdot), \\ldots, f(x_n, \\cdot)$ of these random variables follows a multivariate Gaussian distribution.\n",
    "\n",
    "We often simplify the notation to $f(x) := f(x, \\cdot)$.\n",
    "\n",
    "**Key points:**\n",
    "- For each input $x$, $f(x)$ is a random variable.\n",
    "- A GP is a collection of such random variables, indexed by $x \\in X$.\n",
    "- The defining property: **any finite subset** of these random variables is jointly Gaussian.\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition: Mean and Covariance Function of a GP**\n",
    "\n",
    "Let $f$ be a Gaussian process.\n",
    "\n",
    "- The **mean function** $\\mu: X \\to \\mathbb{R}$ is defined as:\n",
    "    $$\n",
    "    \\mu(x) = \\mathbb{E}[f(x)]\n",
    "    $$\n",
    "\n",
    "- The **covariance function** (or kernel) $k: X \\times X \\to \\mathbb{R}$ is defined as:\n",
    "    $$\n",
    "    k(x_1, x_2) = \\operatorname{Cov}[f(x_1), f(x_2)] = \\mathbb{E}\\left[(f(x_1) - \\mu(x_1))(f(x_2) - \\mu(x_2))\\right]\n",
    "    $$\n",
    "\n",
    "Every Gaussian process has a unique mean and covariance function. Conversely, a mean function and a **positive definite kernel** define a unique Gaussian process. This is often written as:\n",
    "$$\n",
    "f \\sim \\mathcal{GP}(\\mu, k)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition: What Does a GP Sample Look Like?**\n",
    "\n",
    "A **sample path** $f(x, \\omega)$ from a GP can be thought of as:\n",
    "$$\n",
    "f(x, \\omega) = \\mu_x + \\text{Cholesky}(k_{XX}) \\, \\omega\n",
    "$$\n",
    "where:\n",
    "- $k_{XX}$ is the covariance matrix evaluated at a finite set of points $X = \\{x_1, \\ldots, x_n\\}$,\n",
    "- $\\omega$ is a vector of standard normal random variables.\n",
    "\n",
    "This means that, for any finite set of input points, drawing a sample from a GP is equivalent to drawing from a multivariate normal distribution with the specified mean and covariance.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "- A Gaussian Process is a distribution over functions, defined by its mean and covariance functions.\n",
    "- It generalizes the multivariate normal distribution to infinite (possibly uncountable) index sets.\n",
    "- GPs are a powerful tool for modeling distributions over functions in a principled, probabilistic way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da38167",
   "metadata": {},
   "source": [
    "## Covariance Functions and Kernels: Every Covariance Function is a Kernel\n",
    "\n",
    "We've defined a **kernel** as a function that produces symmetric, positive semidefinite matrices. Now, let's formally prove that **every covariance function is indeed a positive definite kernel**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Lemma:**  \n",
    "Every covariance function $k$ is a positive-definite kernel.\n",
    "\n",
    "---\n",
    "\n",
    "### **Proof**\n",
    "\n",
    "Let $v \\in \\mathbb{R}^n$ be an arbitrary vector, and let $X = \\{x_i\\}_{i=1}^n \\subset \\mathcal{X}$ be any finite set of points from the index set.\n",
    "\n",
    "We need to show that the matrix $K_{XX}$ with entries  \n",
    "$$\n",
    "[K_{XX}]_{ij} = k(x_i, x_j) = \\operatorname{Cov}[f(x_i), f(x_j)]\n",
    "$$  \n",
    "is positive semidefinite, i.e.,  \n",
    "$$\n",
    "v^\\top K_{XX} v \\geq 0.\n",
    "$$\n",
    "\n",
    "Let $m(x_i)$ denote the mean function evaluated at $x_i$.\n",
    "\n",
    "Now, consider:\n",
    "$$\n",
    "v^\\top K_{XX} v = \\sum_{i=1}^n \\sum_{j=1}^n v_i v_j \\, \\mathbb{E}\\left[(f(x_i) - m(x_i))(f(x_j) - m(x_j))\\right]\n",
    "$$\n",
    "\n",
    "Since expectation is a linear operator, we can move it outside the sums:\n",
    "$$\n",
    "= \\mathbb{E}\\left[\\sum_{i=1}^n \\sum_{j=1}^n v_i v_j (f(x_i) - m(x_i))(f(x_j) - m(x_j))\\right]\n",
    "$$\n",
    "\n",
    "Observe that the terms inside the expectation can be rewritten as a square:\n",
    "$$\n",
    "= \\mathbb{E}\\left[\\left(\\sum_{i=1}^n v_i (f(x_i) - m(x_i))\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "Let $Z = \\sum_{i=1}^n v_i (f(x_i) - m(x_i))$. $Z$ is a random variable.\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "v^\\top K_{XX} v = \\mathbb{E}[Z^2]\n",
    "$$\n",
    "\n",
    "Since $Z^2$ is always non-negative, its expectation $\\mathbb{E}[Z^2]$ must also be non-negative:\n",
    "$$\n",
    "v^\\top K_{XX} v \\geq 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "This proves that every covariance function is a positive-definite kernel. $\\boxed{}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60badfe9",
   "metadata": {},
   "source": [
    "## Covariance Functions and Kernels: Every Kernel Can Be a Covariance Function of a GP\n",
    "\n",
    "We've just shown that **every covariance function is a kernel**. But the converse is also true and equally important:\n",
    "\n",
    "> **Every positive-definite kernel can serve as the covariance function of some Gaussian Process.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Lemma**\n",
    "\n",
    "For every function $m: X \\to \\mathbb{R}$ and every positive-definite kernel $k: X \\times X \\to \\mathbb{R}$, there exists a Gaussian process $f$ with mean function $m$ and covariance function $k$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Is This True?**\n",
    "\n",
    "The proof of this lemma is more advanced and relies on a fundamental result from probability theory called the **Kolmogorov Extension Theorem**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Kolmogorov Extension Theorem (Simplified Statement)**\n",
    "\n",
    "- Let $I$ be a non-empty index set.\n",
    "- Suppose we are given a collection of consistent finite-dimensional probability distributions.\n",
    "- Then, these finite-dimensional distributions uniquely define a probability measure on the infinite-dimensional product space.\n",
    "\n",
    "---\n",
    "\n",
    "#### **How Does This Apply to Gaussian Processes?**\n",
    "\n",
    "- The **index set** $I$ is our input space $X$.\n",
    "- The **finite-dimensional distributions** are the multivariate Gaussian distributions\n",
    "    $$\n",
    "    p(f_X) = \\mathcal{N}(f_X; m_X, K_{XX})\n",
    "    $$\n",
    "    for any finite set of points $X$.\n",
    "- **Consistency** means that these distributions \"fit together\" properly: for example, if you marginalize $p(f_{x_1}, f_{x_2})$ over $f_{x_2}$, you get $p(f_{x_1})$. The properties of Gaussian distributions and positive-definite kernels guarantee this consistency.\n",
    "\n",
    "---\n",
    "\n",
    "### **What Does This Mean in Practice?**\n",
    "\n",
    "- If you specify **any** valid mean function $m$ and **any** positive-definite kernel $k$, you are guaranteed that there exists a unique Gaussian process $f \\sim \\mathcal{GP}(m, k)$.\n",
    "- This GP will generate finite-dimensional distributions that are exactly the multivariate Gaussians you expect, for any finite collection of input points.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- **Every covariance function is a kernel** (previous lemma).\n",
    "- **Every positive-definite kernel is a valid covariance function for some GP** (this lemma).\n",
    "- The Kolmogorov Extension Theorem ensures that our finite-dimensional Gaussian distributions \"glue together\" to define a true probability distribution over functions.\n",
    "\n",
    "This is why we can confidently write:\n",
    "$$\n",
    "f \\sim \\mathcal{GP}(m, k)\n",
    "$$\n",
    "and know that this uniquely specifies a probability distribution over functions, fully determined by $m$ and $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf0aa8",
   "metadata": {},
   "source": [
    "## Are GPs Probability Distributions over Functions?  \n",
    "### Sample Paths and Random Functions\n",
    "\n",
    "Yes, **Gaussian Processes (GPs)** are truly probability distributions over functions. Let's break down what this means and why it's important.\n",
    "\n",
    "---\n",
    "\n",
    "### **What Is a Sample Path?**\n",
    "\n",
    "- For a fixed outcome $\\omega \\in \\Omega$ from the underlying probability space, the function $f(\\cdot, \\omega): X \\to \\mathbb{R}$ is called a **sample path** (or realization) of the GP $f$.\n",
    "- When we plot samples from a GP, we are visualizing these sample paths—each one is a possible function that could be drawn from the GP.\n",
    "\n",
    "---\n",
    "\n",
    "### **Random Functions: The Mapping $\\omega \\mapsto f(\\cdot, \\omega)$**\n",
    "\n",
    "- The mapping $\\omega \\mapsto f(\\cdot, \\omega)$ is itself a **random variable**, but its values are functions rather than numbers or vectors.\n",
    "- More precisely, this mapping transforms an outcome $\\omega$ into a function, and it maps into a measurable space of functions (for example, $\\mathbb{R}^X$, the space of all real-valued functions on $X$).\n",
    "- This is a direct consequence of the **Kolmogorov Extension Theorem**, which ensures that the collection of finite-dimensional distributions from a GP \"glue together\" to define a probability measure over functions.\n",
    "\n",
    "---\n",
    "\n",
    "### **What Does This Mean?**\n",
    "\n",
    "- **GPs can truly be interpreted as random functions.**  \n",
    "    Each draw from a GP is a function, not just a finite-dimensional vector.\n",
    "- The space $\\mathbb{R}^X$ (all real-valued functions on $X$) is extremely large and includes many \"wild\" or \"ill-behaved\" functions (e.g., functions that are nowhere continuous).\n",
    "- However, most GPs used in practice (such as those with Squared Exponential, Matérn, or Wiener kernels) produce sample paths that are at least continuous, and often much smoother.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Does the Kernel Matter?**\n",
    "\n",
    "- The **kernel** (covariance function) of a GP determines the properties of its sample paths:\n",
    "        - **Smoothness:** Does the GP produce smooth, continuous, or even differentiable functions?\n",
    "        - **Roughness:** Can the GP model abrupt changes or only gradual variations?\n",
    "- For example:\n",
    "        - The **Squared Exponential kernel** produces sample paths that are infinitely differentiable (very smooth).\n",
    "        - The **Matérn kernel** can be tuned to produce sample paths with different degrees of smoothness.\n",
    "        - The **Wiener process** (Brownian motion) kernel produces continuous but nowhere differentiable sample paths.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- **Gaussian Processes are probability distributions over functions.**\n",
    "- Each sample from a GP is a function (a sample path), not just a vector.\n",
    "- The **kernel** controls the \"type\" of functions you get—its properties dictate the smoothness, continuity, and other characteristics of the sample paths.\n",
    "- To understand what kinds of functions a GP can model, always look at the kernel!\n",
    "\n",
    "---\n",
    "\n",
    "**In practice:**  \n",
    "When you use a GP for regression or modeling, you are placing a prior over the space of functions, and the kernel encodes your assumptions about what those functions should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "# --- Re-using kernel definitions from previous lectures for completeness ---\n",
    "# Squared Exponential (RBF) Kernel\n",
    "def squared_exponential_kernel(\n",
    "    x1: jnp.ndarray, x2: jnp.ndarray, sigma: float = 1.0, lengthscale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the Squared Exponential (RBF) kernel matrix.\n",
    "    \"\"\"\n",
    "    x1 = jnp.atleast_2d(x1)\n",
    "    x2 = jnp.atleast_2d(x2)\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    K = sigma**2 * jnp.exp(-0.5 * sq_dist / lengthscale**2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# --- Function to Sample from a GP (assuming zero mean for simplicity) ---\n",
    "def sample_from_gp(\n",
    "    X: jnp.ndarray,\n",
    "    kernel_func: Callable,\n",
    "    num_samples: int = 1,\n",
    "    key: random.PRNGKey = random.PRNGKey(0),\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Draws samples from a Gaussian Process with zero mean.\n",
    "\n",
    "    Args:\n",
    "        X: Input points to sample at. Shape (N, D).\n",
    "        kernel_func: Covariance function (kernel).\n",
    "        num_samples: Number of samples to draw.\n",
    "        key: JAX PRNG key for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        An array of samples. Shape (num_samples, N).\n",
    "    \"\"\"\n",
    "    # Ensure X is at least 2D\n",
    "    X = jnp.atleast_2d(X)\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    K_XX = kernel_func(X, X)\n",
    "\n",
    "    # Add a small jitter for numerical stability in Cholesky decomposition\n",
    "    jitter = 1e-6 * jnp.eye(X.shape[0])\n",
    "    K_XX += jitter\n",
    "\n",
    "    # Compute the Cholesky decomposition of the covariance matrix\n",
    "    L = jnp.linalg.cholesky(K_XX, lower=True)\n",
    "\n",
    "    # Generate random standard normal variables\n",
    "    z = random.normal(key, shape=(num_samples, X.shape[0]))\n",
    "\n",
    "    # Compute the samples: mean + L @ z^T (mean is zero here)\n",
    "    # L is (N, N), z.T is (N, num_samples). Result is (N, num_samples).\n",
    "    # Transpose to get (num_samples, N)\n",
    "    samples = jnp.dot(L, z.T).T\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# --- Example: Sampling from a GP with RBF Kernel ---\n",
    "key = random.PRNGKey(123)\n",
    "\n",
    "# Generate input points\n",
    "X_test = jnp.linspace(-5.0, 5.0, 100)[:, None]  # Test inputs (needs to be 2D)\n",
    "\n",
    "# Define the RBF kernel with specific hyperparameters\n",
    "rbf_sigma = 1.0\n",
    "rbf_lengthscale = 1.0\n",
    "rbf_k = lambda x1, x2: squared_exponential_kernel(\n",
    "    x1, x2, sigma=rbf_sigma, lengthscale=rbf_lengthscale\n",
    ")\n",
    "\n",
    "# Sample functions\n",
    "num_samples = 5\n",
    "gp_samples = sample_from_gp(X_test, rbf_k, num_samples=num_samples, key=key)\n",
    "\n",
    "# Plot the samples\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(num_samples):\n",
    "    plt.plot(X_test[:, 0], gp_samples[i, :], alpha=0.7)\n",
    "\n",
    "# Add the mean function (which is zero here)\n",
    "plt.plot(\n",
    "    X_test[:, 0],\n",
    "    jnp.zeros_like(X_test[:, 0]),\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Mean Function\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.title(\"Samples from a Gaussian Process (RBF Kernel)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae66a7",
   "metadata": {},
   "source": [
    "## Kernels as \"Infinite Matrices\": Introduction\n",
    "\n",
    "So far, we've established several foundational facts:\n",
    "\n",
    "- **GPs are probability distributions on function spaces.**  \n",
    "    However, the probability space defined by a GP is only weakly specified by its general construction and lacks much useful structure. To truly understand the nature of the sample space (the set of possible functions), we must study the kernel in detail.\n",
    "\n",
    "- **Every covariance function is a kernel, and every kernel is a covariance function.**  \n",
    "    This deep equivalence means that the mathematical object we call a \"kernel\" is not just a computational trick—it's the very heart of how GPs define distributions over functions.\n",
    "\n",
    "---\n",
    "\n",
    "Now, let's tackle a key conceptual question:\n",
    "\n",
    "> **What are kernels? Can we think of them as \"infinitely large matrices\"?**\n",
    "\n",
    "In the next sections, we'll explore this idea, building intuition for how kernels generalize the notion of covariance matrices to infinite-dimensional spaces, and why this perspective is so powerful for understanding Gaussian Processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92ee54",
   "metadata": {},
   "source": [
    "## Quick Linear-Algebra Refresher: Positive Definite Matrices\n",
    "\n",
    "Before we dive into kernels as \"infinite matrices,\" let's briefly review some key concepts from linear algebra, focusing on symmetric positive-definite matrices.\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition: Eigenvalue and Eigenvector**\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{n \\times n}$ be a matrix.  \n",
    "A scalar $\\lambda \\in \\mathbb{C}$ and a vector $v \\in \\mathbb{C}^n$ are called an **eigenvalue** and corresponding **eigenvector** of $A$ if:\n",
    "\n",
    "$$\n",
    "A v = \\lambda v\n",
    "$$\n",
    "\n",
    "Or, written in components:\n",
    "\n",
    "$$\n",
    "[Av]_i = \\sum_{j=1}^n [A]_{ij} [v]_j = \\lambda [v]_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Theorem: Spectral Theorem for Symmetric Positive-Definite Matrices**\n",
    "\n",
    "- If $A$ is a **symmetric matrix** ($A = A^\\top$), then:\n",
    "    - All eigenvalues $\\lambda_a$ are **real**.\n",
    "    - The eigenvectors $\\{v_a\\}$ form an **orthonormal basis** for $\\mathbb{R}^n$.\n",
    "- If $A$ is also **positive definite** (i.e., $x^\\top A x > 0$ for all $x \\neq 0$), then **all eigenvalues are positive**: $\\lambda_a > 0$ for all $a = 1, \\ldots, n$.\n",
    "\n",
    "Any symmetric positive-definite matrix $A$ can be written as a **Gramian** (outer product) of its eigenvectors:\n",
    "\n",
    "$$\n",
    "[A]_{ij} = \\sum_{a=1}^n \\lambda_a [v_a]_i [v_a]_j\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\lambda_a$ are the eigenvalues,\n",
    "- $v_a$ are the corresponding orthonormal eigenvectors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Is This Important?**\n",
    "\n",
    "- This decomposition is fundamental for understanding the structure of covariance matrices and kernels.\n",
    "- It allows us to interpret a positive-definite matrix as a weighted sum of rank-one matrices (outer products of eigenvectors).\n",
    "- This perspective will help us generalize to the infinite-dimensional case, which is essential for understanding kernels in Gaussian Processes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9787ee",
   "metadata": {},
   "source": [
    "## Kernels as Inner Products: Mercer's Theorem\n",
    "\n",
    "To understand kernels at a deeper level, let's extend the familiar concept of eigenvalues and eigenvectors from finite matrices to the world of functions and kernels. This leads us to the powerful idea of **eigenfunctions** and **Mercer's Theorem**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Eigenfunctions and Eigenvalues of a Kernel**\n",
    "\n",
    "Given a kernel function $k : X \\times X \\to \\mathbb{R}$ and a measure $\\nu$ on $X$, an **eigenfunction** $\\phi : X \\to \\mathbb{R}$ and an **eigenvalue** $\\lambda \\in \\mathbb{C}$ satisfy the following integral equation:\n",
    "\n",
    "$$\n",
    "\\int k(x, \\tilde{x}) \\, \\phi(\\tilde{x}) \\, d\\nu(\\tilde{x}) = \\lambda \\, \\phi(x)\n",
    "$$\n",
    "\n",
    "- This is the functional analogue of the matrix equation $A v = \\lambda v$.\n",
    "- Here, the kernel $k$ plays the role of the matrix $A$, and the function $\\phi$ is analogous to the eigenvector $v$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mercer's Theorem (1909)**\n",
    "\n",
    "Let $(X, \\nu)$ be a finite measure space, and let $k : X \\times X \\to \\mathbb{R}$ be a continuous (Mercer) kernel. Then:\n",
    "\n",
    "- There exists a countable set of eigenvalues and eigenfunctions $\\{ (\\lambda_i, \\phi_i) \\}_{i \\in I}$ with respect to $\\nu$ such that:\n",
    "    - $I$ is countable.\n",
    "    - All $\\lambda_i$ are real and non-negative.\n",
    "    - The eigenfunctions $\\phi_i$ can be chosen to be orthonormal in $L^2(X, \\nu)$:\n",
    "      $$\n",
    "      \\int \\phi_i(x) \\phi_j(x) d\\nu(x) = \\delta_{ij}\n",
    "      $$\n",
    "    - The kernel admits the following **spectral decomposition** (series converges absolutely and uniformly $\\nu^2$-almost everywhere):\n",
    "      $$\n",
    "      k(a, b) = \\sum_{i \\in I} \\lambda_i \\, \\phi_i(a) \\, \\phi_i(b) \\qquad \\forall a, b \\in X\n",
    "      $$\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Is Mercer's Theorem Important?**\n",
    "\n",
    "- **Analogy to Finite Matrices:**  \n",
    "  Just as any symmetric positive-definite matrix can be decomposed into a sum of outer products of its eigenvectors, a Mercer kernel can be decomposed into an (infinite) sum of products of its eigenfunctions, weighted by their eigenvalues.\n",
    "- **Feature Space Interpretation:**  \n",
    "  This decomposition shows that kernels implicitly define an infinite-dimensional feature space, where each eigenfunction acts as a \"feature\" and the eigenvalues determine their importance.\n",
    "- **Foundation for Kernel Methods:**  \n",
    "  Mercer's Theorem provides the mathematical foundation for kernel methods in machine learning, including Support Vector Machines and Gaussian Processes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- **Mercer's Theorem** bridges the gap between finite-dimensional linear algebra and infinite-dimensional function spaces.\n",
    "- It tells us that every continuous, positive-definite kernel can be viewed as an \"infinite Gram matrix\"—an inner product in a (possibly infinite-dimensional) feature space.\n",
    "- This perspective is crucial for understanding the power and flexibility of kernel-based models, especially Gaussian Processes.\n",
    "\n",
    "---\n",
    "\n",
    "**In essence:**  \n",
    "Mercer's Theorem reveals that kernels are not just similarity measures—they are inner products in a rich, often infinite-dimensional, space of features defined by the kernel's eigenfunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cfc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# --- Re-using kernel definition ---\n",
    "# Squared Exponential (RBF) Kernel\n",
    "def squared_exponential_kernel(\n",
    "    x1: jnp.ndarray, x2: jnp.ndarray, sigma: float = 1.0, lengthscale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the Squared Exponential (RBF) kernel matrix.\n",
    "    \"\"\"\n",
    "    x1 = jnp.atleast_2d(x1)\n",
    "    x2 = jnp.atleast_2d(x2)\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    K = sigma**2 * jnp.exp(-0.5 * sq_dist / lengthscale**2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# --- Example: Eigenvalues and Eigenvectors of a Finite Kernel Matrix ---\n",
    "# This is an analogy to Mercer's Theorem for infinite-dimensional kernels.\n",
    "# For a finite set of points, the kernel matrix is a symmetric positive definite matrix,\n",
    "# and thus has real, non-negative eigenvalues and orthogonal eigenvectors.\n",
    "\n",
    "# Define a finite set of input points\n",
    "X_finite = jnp.linspace(-3.0, 3.0, 20)[:, None]  # 20 points\n",
    "\n",
    "# Compute the kernel matrix for these points\n",
    "K_finite = squared_exponential_kernel(X_finite, X_finite, sigma=1.0, lengthscale=1.0)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "# jnp.linalg.eigh returns eigenvalues in ascending order\n",
    "eigenvalues, eigenvectors = jnp.linalg.eigh(K_finite)\n",
    "\n",
    "# Sort eigenvalues in descending order and reorder eigenvectors accordingly\n",
    "idx = eigenvalues.argsort()[::-1]  # Get indices for descending order\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]  # Reorder columns of eigenvectors matrix\n",
    "\n",
    "print(f\"Top 5 Eigenvalues:\\n{eigenvalues[:5]}\")\n",
    "print(\n",
    "    f\"Smallest 5 Eigenvalues:\\n{eigenvalues[-5:]}\"\n",
    ")  # Should be positive/close to zero\n",
    "\n",
    "# Plot the first few eigenvectors (analogous to eigenfunctions)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(min(5, len(eigenvalues))):  # Plot up to 5 eigenvectors\n",
    "    # Scale eigenvectors by sqrt of eigenvalues for visualization,\n",
    "    # as per Karhunen-Loève expansion form (lambda_i^1/2 * phi_i(x))\n",
    "    plt.plot(\n",
    "        X_finite[:, 0],\n",
    "        eigenvectors[:, i] * jnp.sqrt(eigenvalues[i]),\n",
    "        label=f\"Eigenvector {i + 1} (scaled by $\\\\sqrt{{\\\\lambda_{i + 1}}}$)\",\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\n",
    "    \"First Few Eigenvectors of a Finite RBF Kernel Matrix (Analogy to Eigenfunctions)\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Verify Reconstruction (Mercer's Theorem Analogy) ---\n",
    "# We can reconstruct the kernel matrix from its eigenvalues and eigenvectors\n",
    "# K_reconstructed = Sum_{i} lambda_i * v_i * v_i^T\n",
    "K_reconstructed = jnp.dot(eigenvectors * eigenvalues, eigenvectors.T)\n",
    "\n",
    "# Check if the reconstruction is close to the original matrix\n",
    "print(\n",
    "    f\"\\nMax absolute difference between original and reconstructed K: {jnp.max(jnp.abs(K_finite - K_reconstructed)):.2e}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772aa932",
   "metadata": {},
   "source": [
    "## Are Kernels Infinitely Large Positive Definite Matrices? Kind of...\n",
    "\n",
    "In the sense of **Mercer’s theorem**, you can *loosely* think of a kernel \\( k : X \\times X \\to \\mathbb{R} \\), evaluated at \\( k(a, b) \\) for \\( a, b \\in X \\), as the \"element\" of an **infinitely large** matrix \\( k_{ab} \\). The spectral decomposition of the kernel is:\n",
    "\n",
    "$$\n",
    "k(a, b) = \\sum_{i \\in I} \\lambda_i \\, \\phi_i(a) \\, \\phi_i(b)\n",
    "$$\n",
    "\n",
    "This can also be written more compactly using a **feature map**:\n",
    "\n",
    "$$\n",
    "\\Phi(x) = [\\sqrt{\\lambda_1} \\, \\phi_1(x), \\sqrt{\\lambda_2} \\, \\phi_2(x), \\ldots]^T\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "k(a, b) = \\langle \\Phi(a), \\Phi(b) \\rangle\n",
    "$$\n",
    "\n",
    "where $ \\langle \\cdot, \\cdot \\rangle $ denotes the inner product in this (possibly infinite-dimensional) feature space.\n",
    "\n",
    "> **Note:**  \n",
    "> This interpretation depends on the measure \\( \\nu : X \\to \\mathbb{R} \\) used in the integral equation for eigenfunctions. In practice, it is often *not* straightforward to find the eigenfunctions analytically for arbitrary kernels.\n",
    "\n",
    "---\n",
    "\n",
    "### Better Questions to Ask\n",
    "\n",
    "- **Do the eigenfunctions span a space like the eigenvectors of a matrix?**\n",
    "- **What is that space? Is it the sample space of a GP?**\n",
    "\n",
    "These questions naturally lead us to the concept of **Reproducing Kernel Hilbert Spaces (RKHS)**, which provide a rigorous mathematical framework for understanding the function spaces associated with kernels.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "- Kernels can be viewed as \"infinite matrices\" via their spectral decomposition.\n",
    "- The feature map \\( \\Phi(x) \\) embeds inputs into a (possibly infinite-dimensional) space where the kernel acts as an inner product.\n",
    "- Understanding the space spanned by the eigenfunctions leads to the powerful theory of RKHS, which we will explore next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fa6fd",
   "metadata": {},
   "source": [
    "## Transition to RKHS and Kernel Machines\n",
    "\n",
    "So far, we've established several key insights:\n",
    "\n",
    "- **Gaussian Processes (GPs) are probability distributions over function spaces.**\n",
    "- **Every covariance function is a kernel, and every kernel is a covariance function.**\n",
    "- **Kernels have eigenfunctions, just as matrices have eigenvectors.**  \n",
    "    This means we can think of kernels as a kind of \"infinite matrix\" that spans a space of functions.\n",
    "\n",
    "---\n",
    "\n",
    "Now, let's explore two fundamental questions:\n",
    "\n",
    "1. **What is the space spanned by the eigenfunctions of a kernel?**  \n",
    "     What does it mean for a kernel to define a \"space of functions,\" and how is this space constructed?\n",
    "\n",
    "2. **What is the connection to \"kernel machines\"?**  \n",
    "     How do these ideas relate to kernel-based algorithms in machine learning, such as Support Vector Machines (SVMs) and Gaussian Processes?\n",
    "\n",
    "---\n",
    "\n",
    "In the next sections, we'll introduce the concept of the **Reproducing Kernel Hilbert Space (RKHS)**, which provides a rigorous mathematical framework for understanding the function spaces associated with kernels. We'll also see how this connects to practical machine learning methods known as kernel machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8cbdc4",
   "metadata": {},
   "source": [
    "## Gaussian Processes, By Any Other Name\n",
    "\n",
    "Gaussian Process (GP) regression is one of the most deeply studied and widely used models in statistics and machine learning. Interestingly, it has appeared under various names across different fields, reflecting its fundamental nature and broad applicability.\n",
    "\n",
    "### Equivalent and Closely Related Names\n",
    "\n",
    "- **Kriging**  \n",
    "    - Originates from geostatistics and spatial analysis.\n",
    "    - Used for spatial interpolation and prediction of unknown values based on observed data.\n",
    "\n",
    "- **Kernel Ridge Regression**  \n",
    "    - A regularized regression method in machine learning.\n",
    "    - Closely connected to GP regression through the use of kernels and feature spaces (we will explore this connection in detail).\n",
    "\n",
    "- **Wiener–Kolmogorov Prediction**  \n",
    "    - Emerges from time series analysis and signal processing.\n",
    "    - Focuses on optimal linear prediction of future values in a stochastic process.\n",
    "\n",
    "- **Linear Least-Squares Regression (in a generalized feature space)**  \n",
    "    - The classical regression method, extended to infinite-dimensional feature spaces via kernels.\n",
    "\n",
    "---\n",
    "\n",
    "These different names reflect the various historical contexts and research communities that independently discovered or developed aspects of what we now unify under the umbrella of **Gaussian Processes**.\n",
    "\n",
    "> **Key Takeaway:**  \n",
    "> Whether you encounter the term Kriging, kernel ridge regression, or Wiener–Kolmogorov prediction, you are often dealing with the same underlying mathematical ideas as Gaussian Process regression—just viewed through different disciplinary lenses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997967e",
   "metadata": {},
   "source": [
    "## The Gaussian Posterior Mean as a Least-Squares Estimate (Kernel Ridge Regression)\n",
    "\n",
    "One of the most profound connections in Gaussian Process (GP) theory is the equivalence between the **GP posterior mean** and the solution to a regularized least-squares problem, known as **Kernel Ridge Regression (KRR)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Setup: GP Regression with Gaussian Likelihood**\n",
    "\n",
    "- **Prior:**  \n",
    "    $$ p(f) = \\mathcal{GP}(f; 0, k) $$\n",
    "    where $k$ is the kernel (covariance function).\n",
    "\n",
    "- **Likelihood:**  \n",
    "    $$ p(y \\mid f) = \\mathcal{N}(y; f_X, \\sigma^2 I) $$\n",
    "    where $f_X$ denotes the vector of function values at the observed inputs $X$.\n",
    "\n",
    "- **Posterior:**  \n",
    "    The posterior over function values at $X$ is:\n",
    "    $$\n",
    "    p(f_X \\mid y) = \\frac{p(y \\mid f_X) p(f_X)}{p(y)}\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "### **Posterior Mean as a Solution to Regularized Least Squares**\n",
    "\n",
    "The **posterior mean** at a new input $x$ is:\n",
    "$$\n",
    "m(x) = \\mathbb{E}_{p(f \\mid y)}[f(x)]\n",
    "$$\n",
    "\n",
    "It turns out that $m(x)$ is the function in the **Reproducing Kernel Hilbert Space (RKHS)** $\\mathcal{H}_k$ that minimizes the following regularized $\\ell_2$ loss:\n",
    "$$\n",
    "L(f) = \\frac{1}{2\\sigma^2} \\| y - f_X \\|^2 + \\frac{1}{2} \\| f \\|_{\\mathcal{H}_k}^2\n",
    "$$\n",
    "\n",
    "- $\\| f \\|_{\\mathcal{H}_k}^2$ is the RKHS norm of $f$, acting as a regularizer.\n",
    "- Minimizing $L(f)$ is exactly the objective of **Kernel Ridge Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Explicit Formula for the Posterior Mean**\n",
    "\n",
    "The solution (posterior mean) is given by:\n",
    "$$\n",
    "m(x) = k_{xX} (k_{XX} + \\sigma^2 I)^{-1} y\n",
    "$$\n",
    "\n",
    "- $k_{xX}$: kernel vector between test point $x$ and training points $X$\n",
    "- $k_{XX}$: kernel matrix on training points\n",
    "- $\\sigma^2$: noise variance\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**\n",
    "\n",
    "- The **Bayesian posterior mean** of a GP is **identical** to the solution of a specific regularized least-squares problem in the frequentist framework.\n",
    "- This highlights a deep connection between Bayesian and frequentist approaches:  \n",
    "    **Many seemingly different models are fundamentally linked.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Historical Context**\n",
    "\n",
    "The connection to least-squares and regularization dates back over 200 years to mathematicians like **Adrien-Marie Legendre** and **Carl-Friedrich Gauss**, who developed the method of least squares. Today, this classical idea underpins modern kernel methods and Gaussian Process regression.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "The GP posterior mean is not just a Bayesian prediction—it is also the optimal solution to a regularized least-squares problem in an infinite-dimensional feature space defined by the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92beec",
   "metadata": {},
   "source": [
    "# Reproducing Kernel Hilbert Spaces (RKHS): Two Definitions\n",
    "\n",
    "The **eigenfunctions** of a kernel span a very important space of functions called the **Reproducing Kernel Hilbert Space (RKHS)**. This space is central to understanding what a Gaussian Process (GP) can \"learn\" or represent.\n",
    "\n",
    "---\n",
    "\n",
    "## Definition: Reproducing Kernel Hilbert Space (RKHS)\n",
    "\n",
    "Let $\\mathcal{H} = (X, \\langle \\cdot, \\cdot \\rangle_{\\mathcal{H}})$ be a Hilbert space of functions $f : X \\to \\mathbb{R}$.  \n",
    "$\\mathcal{H}$ is called a **reproducing kernel Hilbert space** if there exists a kernel $k : X \\times X \\to \\mathbb{R}$ such that:\n",
    "\n",
    "1. **Kernel Functions Belong to the Space:**  \n",
    "    For all $x \\in X$, the function $k(\\cdot, x) \\in \\mathcal{H}$.\n",
    "\n",
    "2. **Reproducing Property:**  \n",
    "    For all $f \\in \\mathcal{H}$ and all $x \\in X$,\n",
    "    $$\n",
    "    \\langle f(\\cdot), k(\\cdot, x) \\rangle_{\\mathcal{H}} = f(x)\n",
    "    $$\n",
    "    This means that evaluating a function at $x$ is equivalent to taking its inner product with the kernel function $k(\\cdot, x)$.\n",
    "\n",
    "---\n",
    "\n",
    "## Theorem (Aronszajn, 1950)\n",
    "\n",
    "> For every positive definite kernel $k$ on $X$, there exists a **unique** RKHS $\\mathcal{H}_k$ associated with $k$.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition\n",
    "\n",
    "- The kernel $k(\\cdot, x)$ acts like a **generalized identity function** or a \"delta function\" in the RKHS.\n",
    "- It allows us to \"reproduce\" the value of any function in the space by taking an inner product.\n",
    "- The RKHS is the smallest Hilbert space of functions containing all finite linear combinations of kernel functions $k(\\cdot, x)$, with the inner product defined so that the reproducing property holds.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Is RKHS Important?\n",
    "\n",
    "- The RKHS associated with a kernel $k$ describes the set of functions that can be \"expressed\" or \"learned\" by kernel methods (including GPs and SVMs).\n",
    "- The **smoothness** and **complexity** of functions in the RKHS are controlled by the choice of kernel.\n",
    "- In practice, the RKHS provides a rigorous mathematical framework for understanding the power and limitations of kernel-based learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: RKHS for the RBF Kernel\n",
    "\n",
    "- For the popular **RBF (Squared Exponential) kernel**, the associated RKHS consists of very smooth functions.\n",
    "- The more \"wiggly\" the kernel (e.g., Matérn kernels with low smoothness), the larger and rougher the RKHS.\n",
    "\n",
    "---\n",
    "\n",
    "> **Summary:**  \n",
    "> The RKHS is the function space \"spanned\" by the kernel's eigenfunctions. It is the natural habitat for kernel methods and provides deep insight into what kinds of functions a GP or kernel machine can represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94034ee",
   "metadata": {},
   "source": [
    "## What is the RKHS? (1) The Space of Possible Posterior Mean Functionsons\n",
    "\n",
    "A key insight in kernel methods and Gaussian Processes (GPs) is that the **Reproducing Kernel Hilbert Space (RKHS)** associated with a kernel $k$ is precisely the space of all possible posterior mean functions that can be obtained from a GP with that kernel.f all possible posterior mean functions that can be obtained from a GP with that kernel.\n",
    "\n",
    "---\n",
    "\n",
    "### Theorem: Reproducing Kernel Map Representation\n",
    "\n",
    "Let $X$, $\\nu$, and $\\{(\\phi_i, \\lambda_i)\\}_{i \\in I}$ be as defined in Mercer's Theorem. Let $\\{x_i\\}_{i \\in I} \\subset X$ be a countable collection of points in $X$. Then, the RKHS $\\mathcal{H}_k$ can be written as the space of linear combinations of kernel functions:Let $X$, $\\nu$, and $\\{(\\phi_i, \\lambda_i)\\}_{i \\in I}$ be as defined in Mercer's Theorem. Let $\\{x_i\\}_{i \\in I} \\subset X$ be a countable collection of points in $X$. Then, the RKHS $\\mathcal{H}_k$ can be written as the space of linear combinations of kernel functions:\n",
    "\n",
    "$$\n",
    "\\mathcal{H}_k = \\left\\{ f(x) := \\sum_{i \\in I} \\tilde{\\alpha}_i\\, k(x_i, x) \\right\\}\\mathcal{H}_k = \\left\\{ f(x) := \\sum_{i \\in I} \\tilde{\\alpha}_i\\, k(x_i, x) \\right\\}\n",
    "$$\n",
    "\n",
    "with the inner productwith the inner product\n",
    "\n",
    "$$\n",
    "\\langle f, g \\rangle_{\\mathcal{H}_k} := \\sum_{i \\in I} \\tilde{\\alpha}_i \\tilde{\\beta}_i\\, k(x_i, x_i)\n",
    "$$\n",
    "\n",
    "where $f(x) = \\sum_{i} \\tilde{\\alpha}_i k(x_i, x)$ and $g(x) = \\sum_{i} \\tilde{\\beta}_i k(x_i, x)$.\n",
    "\n",
    "---\n",
    "\n",
    "### GP Posterior Mean Functions Live in the RKHS\n",
    "\n",
    "Consider a Gaussian process prior $p(f) = \\mathcal{GP}(0, k)$ with likelihood $p(y \\mid f, X) = \\mathcal{N}(y; f_X, \\sigma^2 I)$. The posterior mean function is given by:\n",
    "\n",
    "$$\n",
    "\\mu(x) = k_{xX} (k_{XX} + \\sigma^2 I)^{-1} y\n",
    "$$\n",
    "\n",
    "Let $w = (k_{XX} + \\sigma^2 I)^{-1} y$. Then,\n",
    "\n",
    "$$\n",
    "\\mu(x) = k_{xX} w = \\sum_{i=1}^n w_i\\, k(x, x_i)\n",
    "$$\n",
    "\n",
    "This shows that the posterior mean function is a **finite linear combination of kernel functions** centered at the training data points.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Does This Matter?\n",
    "\n",
    "- **The RKHS is the space of all possible posterior mean functions of GP regression.**\n",
    "- To understand what a GP can \"learn,\" we must analyze the RKHS associated with its kernel.\n",
    "- The **posterior mean** (the GP's best estimate of the function) always lives in this space, regardless of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The RKHS provides a rigorous mathematical framework for understanding the expressiveness and limitations of kernel-based learning algorithms.\n",
    "- The choice of kernel determines the \"shape\" and \"smoothness\" of functions in the RKHS, and thus what the GP can represent.\n",
    "- This connection is fundamental to the\n",
    "\n",
    "---\n",
    "\n",
    "> **Summary:**  \n",
    "> The RKHS associated with a kernel $k$ is the natural habitat for the posterior mean functions of GP regression. Understanding the RKHS is crucial for understanding what your GP model can and cannot learn.\n",
    "> The RKHS associated with a kernel $k$ is the natural habitat for the posterior mean functions of GP regression. Understanding the RKHS is crucial for understanding what your GP model can and cannot learn.\n",
    "> **Summary:**  \n",
    "\n",
    "---\n",
    " statistical learning theory of RKHSs and underpins much of modern machine learning with kernels.\n",
    "$$\n",
    "\\langle f, g \\rangle_{\\mathcal{H}_k} := \\sum_{i \\in I} \\tilde{\\alpha}_i \\tilde{\\beta}_i\\, k(x_i, x_i)\n",
    "$$\n",
    "\n",
    "where $f(x) = \\sum_{i} \\tilde{\\alpha}_i k(x_i, x)$ and $g(x) = \\sum_{i} \\tilde{\\beta}_i k(x_i, x)$.\n",
    "\n",
    "---\n",
    "\n",
    "### GP Posterior Mean Functions Live in the RKHS\n",
    "\n",
    "Consider a Gaussian process prior $p(f) = \\mathcal{GP}(0, k)$ with likelihood $p(y \\mid f, X) = \\mathcal{N}(y; f_X, \\sigma^2 I)$. The posterior mean function is given by:\n",
    "\n",
    "$$\n",
    "\\mu(x) = k_{xX} (k_{XX} + \\sigma^2 I)^{-1} y\n",
    "$$\n",
    "\n",
    "Let $w = (k_{XX} + \\sigma^2 I)^{-1} y$. Then,\n",
    "\n",
    "$$\n",
    "\\mu(x) = k_{xX} w = \\sum_{i=1}^n w_i\\, k(x, x_i)\n",
    "$$\n",
    "\n",
    "This shows that the posterior mean function is a **finite linear combination of kernel functions** centered at the training data points.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Does This Matter?\n",
    "\n",
    "- **The RKHS is the space of all possible posterior mean functions of GP regression.**\n",
    "- To understand what a GP can \"learn,\" we must analyze the RKHS associated with its kernel.\n",
    "- The **posterior mean** (the GP's best estimate of the function) always lives in this space, regardless of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The RKHS provides a rigorous mathematical framework for understanding the expressiveness and limitations of kernel-based learning algorithms.\n",
    "- The choice of kernel determines the \"shape\" and \"smoothness\" of functions in the RKHS, and thus what the GP can represent.\n",
    "- This connection is fundamental to the statistical learning theory of RKHSs and underpins much of modern machine learning with kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.scipy.linalg import solve  # Use JAX's solve for numerical stability\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "# --- Re-using kernel definitions from previous lectures for completeness ---\n",
    "def squared_exponential_kernel(\n",
    "    x1: jnp.ndarray, x2: jnp.ndarray, sigma: float = 1.0, lengthscale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the Squared Exponential (RBF) kernel matrix.\n",
    "    \"\"\"\n",
    "    x1 = jnp.atleast_2d(x1)\n",
    "    x2 = jnp.atleast_2d(x2)\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    K = sigma**2 * jnp.exp(-0.5 * sq_dist / lengthscale**2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# GP Prediction Function (as used in previous lectures)\n",
    "def gp_predict(\n",
    "    X_train: jnp.ndarray,\n",
    "    y_train: jnp.ndarray,\n",
    "    X_test: jnp.ndarray,\n",
    "    mean_func: Callable[[jnp.ndarray], jnp.ndarray],\n",
    "    kernel_func: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray],\n",
    "    noise_variance: float = 1e-6,\n",
    ") -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Performs Gaussian Process regression prediction.\n",
    "    \"\"\"\n",
    "    X_train = jnp.atleast_2d(X_train)\n",
    "    X_test = jnp.atleast_2d(X_test)\n",
    "    y_train = jnp.atleast_1d(y_train)\n",
    "\n",
    "    K_train_train = kernel_func(X_train, X_train) + noise_variance * jnp.eye(\n",
    "        X_train.shape[0]\n",
    "    )\n",
    "    K_test_train = kernel_func(X_test, X_train)\n",
    "    K_test_test = kernel_func(X_test, X_test)\n",
    "\n",
    "    K_train_train_inv_y_diff = solve(K_train_train, y_train - mean_func(X_train))\n",
    "    mu_pred = mean_func(X_test) + jnp.dot(K_test_train, K_train_train_inv_y_diff)\n",
    "\n",
    "    K_train_train_inv_K_test_train_T = solve(K_train_train, K_test_train.T)\n",
    "    Sigma_pred = K_test_test - jnp.dot(K_test_train, K_train_train_inv_K_test_train_T)\n",
    "\n",
    "    return mu_pred, Sigma_pred\n",
    "\n",
    "\n",
    "# --- Example: GP Regression to illustrate Posterior Mean (which lies in RKHS) ---\n",
    "key = random.PRNGKey(456)\n",
    "\n",
    "# Generate synthetic data\n",
    "X_train = jnp.sort(random.uniform(key, shape=(10, 1), minval=-4, maxval=4))\n",
    "noise = random.normal(key, shape=X_train.shape) * 0.3\n",
    "y_train = jnp.sin(X_train) + noise\n",
    "\n",
    "# Define mean and kernel functions\n",
    "zero_mean_func = lambda x: jnp.zeros(x.shape[0])\n",
    "rbf_kernel_for_pred = lambda x1, x2: squared_exponential_kernel(\n",
    "    x1, x2, sigma=1.0, lengthscale=1.0\n",
    ")\n",
    "obs_noise_variance = 0.1**2\n",
    "\n",
    "# Generate test points\n",
    "X_test = jnp.linspace(-5, 5, 100)[:, None]\n",
    "\n",
    "# Perform GP prediction\n",
    "mu_pred, Sigma_pred = gp_predict(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    zero_mean_func,\n",
    "    rbf_kernel_for_pred,\n",
    "    noise_variance=obs_noise_variance,\n",
    ")\n",
    "\n",
    "predictive_std = jnp.sqrt(jnp.diag(Sigma_pred))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[:, 0], y_train, label=\"Training Data\", zorder=2)\n",
    "plt.plot(X_test[:, 0], mu_pred, label=\"GP Posterior Mean (in RKHS)\", color=\"red\")\n",
    "plt.fill_between(\n",
    "    X_test[:, 0],\n",
    "    mu_pred - 2 * predictive_std,\n",
    "    mu_pred + 2 * predictive_std,\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    "    label=\"95% Confidence Interval\",\n",
    ")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Gaussian Process Regression: Posterior Mean in RKHS\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# The posterior mean can be expressed as a linear combination of kernel functions\n",
    "# centered at the training data points.\n",
    "# This explicitly shows its membership in the RKHS.\n",
    "# The weights 'alpha' are computed during the GP prediction:\n",
    "# alpha = (K_train_train + noise_variance * I)^-1 * (y_train - mean_func(X_train))\n",
    "K_train_train_noisy = rbf_kernel_for_pred(\n",
    "    X_train, X_train\n",
    ") + obs_noise_variance * jnp.eye(X_train.shape[0])\n",
    "alpha_weights = solve(K_train_train_noisy, y_train - zero_mean_func(X_train))\n",
    "\n",
    "# Reconstruct posterior mean using alpha_weights and kernel functions\n",
    "mu_pred_reconstructed = jnp.dot(\n",
    "    rbf_kernel_for_pred(X_test, X_train), alpha_weights\n",
    ") + zero_mean_func(X_test)\n",
    "\n",
    "print(\n",
    "    f\"\\nMax absolute difference between direct mu_pred and reconstructed mu_pred: {jnp.max(jnp.abs(mu_pred - mu_pred_reconstructed)):.2e}\"\n",
    ")\n",
    "print(\n",
    "    \"This confirms the posterior mean is a linear combination of kernel functions, residing in the RKHS.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16d3ca",
   "metadata": {},
   "source": [
    "# What is the Meaning of Uncertainty?  \n",
    "## Frequentist Interpretation of Posterior Variance\n",
    "\n",
    "In **Bayesian Gaussian Processes (GPs)**, the posterior variance  \n",
    "$$\n",
    "v(x) = \\operatorname{Cov}[f(x) \\mid y]\n",
    "$$  \n",
    "quantifies our uncertainty about the function value at a point $x$ given the observed data $y$. Interestingly, this Bayesian measure of uncertainty has a strong **Frequentist interpretation**.\n",
    "\n",
    "---\n",
    "\n",
    "### Theorem: Posterior Variance as a Worst-Case Error in the RKHS\n",
    "\n",
    "Assume a GP prior $p(f) = \\mathcal{GP}(f; 0, k)$ and **noise-free observations** $p(y \\mid f) = \\delta(y - f_X)$ (where $\\delta$ is the Dirac delta function, implying exact observations). The GP posterior variance (the expected squared error) is:\n",
    "$$\n",
    "v(x) := \\mathbb{E}_{p(f \\mid y)}\\left[(f(x) - m(x))^2\\right] = k_{xx} - k_{xX} K_{XX}^{-1} k_{Xx}\n",
    "$$\n",
    "\n",
    "This variance is also a **worst-case bound** on the divergence between the posterior mean $m(x)$ and any function $f$ in the RKHS with norm at most $1$:\n",
    "$$\n",
    "v(x) = \\sup_{f \\in \\mathcal{H}_k,\\, \\|f\\|_{\\mathcal{H}_k} \\leq 1} (m(x) - f(x))^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Proof Sketch\n",
    "\n",
    "1. **Start with the supremum:**\n",
    "    $$\n",
    "    \\sup_{f \\in \\mathcal{H},\\, \\|f\\| \\leq 1} (m(x) - f(x))^2\n",
    "    $$\n",
    "    where $m(x) = \\sum_i f(x_i) [K_{XX}^{-1} k(X, x)]_i$.\n",
    "\n",
    "2. **Use the reproducing property:**  \n",
    "    For any $f \\in \\mathcal{H}_k$,\n",
    "    $$\n",
    "    f(x) = \\langle f(\\cdot), k(\\cdot, x) \\rangle_{\\mathcal{H}}\n",
    "    $$\n",
    "\n",
    "3. **Rewrite the difference as an inner product:**\n",
    "    $$\n",
    "    m(x) - f(x) = \\left\\langle \\sum_i [K_{XX}^{-1} k(X, x)]_i k(\\cdot, x_i) - k(\\cdot, x),\\, f(\\cdot) \\right\\rangle_{\\mathcal{H}}\n",
    "    $$\n",
    "\n",
    "4. **Apply Cauchy-Schwarz inequality:**  \n",
    "    $$\n",
    "    | \\langle a, b \\rangle | \\leq \\|a\\| \\|b\\| \\implies \\sup_{\\|f\\| \\leq 1} \\langle a, f \\rangle^2 = \\|a\\|^2\n",
    "    $$\n",
    "\n",
    "5. **Compute the RKHS norm:**\n",
    "    $$\n",
    "    \\left\\| \\sum_i [K_{XX}^{-1} k(X, x)]_i k(\\cdot, x_i) - k(\\cdot, x) \\right\\|_{\\mathcal{H}}^2\n",
    "    $$\n",
    "\n",
    "6. **Expand using the RKHS inner product:**\n",
    "    $$\n",
    "    = \\sum_{ij} [K_{XX}^{-1} k(X, x)]_i [K_{XX}^{-1} k(X, x)]_j k(x_i, x_j)\n",
    "    - 2 \\sum_i [K_{XX}^{-1} k(X, x)]_i k(x, x_i)\n",
    "    + k(x, x)\n",
    "    $$\n",
    "\n",
    "    This simplifies to:\n",
    "    $$\n",
    "    = k_{xx} - k_{xX} K_{XX}^{-1} k_{Xx}\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- The **Bayesian posterior variance** at $x$ is equal to the **maximum possible squared difference** between the posterior mean $m(x)$ and any function $f$ in the RKHS with $\\|f\\|_{\\mathcal{H}_k} \\leq 1$.\n",
    "- In essence, it tells us *how far off* our posterior mean could be from any \"well-behaved\" function in the RKHS, even in the best-case scenario of noise-free observations.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Is This Powerful?\n",
    "\n",
    "- The GP's **expected squared error** (a Bayesian quantity) is equivalent to the RKHS's **worst-case squared error** for functions of bounded norm (a Frequentist quantity).\n",
    "- This suggests that Bayesians, in a sense, \"expect the worst\" when it comes to uncertainty, as their posterior variance provides a robust bound.\n",
    "\n",
    "> **Note:** The posterior variance $v(x)$ itself is generally **not** an element of $\\mathcal{H}_k$.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Bayesian posterior variance** quantifies uncertainty about $f(x)$ after observing data.\n",
    "- **Frequentist interpretation:** It is the worst-case squared error between the posterior mean and any function in the RKHS of bounded norm.\n",
    "- This duality bridges Bayesian and Frequentist perspectives, deepening our understanding of uncertainty in kernel methods and GPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f39fa",
   "metadata": {},
   "source": [
    "# Are Bayesian and Frequentist Analysis the Same Thing?\n",
    "\n",
    "Let's recap the key connections we've uncovered so far:\n",
    "\n",
    "- **GPs are probability distributions on function spaces.**\n",
    "- **Every covariance function is a kernel, and every kernel is a covariance function.**\n",
    "- **Kernels have eigenfunctions, just as matrices have eigenvectors.**  \n",
    "    This means we can think of kernels as a kind of \"infinite matrix\" that spans a space of functions.\n",
    "- **That space is the Reproducing Kernel Hilbert Space (RKHS).**  \n",
    "    The RKHS is identical to the space of all possible posterior mean functions of GP regression.\n",
    "- **The posterior covariance function (the Bayesian average squared error) is a worst-case squared error in the RKHS.**\n",
    "\n",
    "---\n",
    "\n",
    "These deep connections might make it tempting to conclude that **Bayesian and Frequentist analyses are essentially the same**—at least for Gaussian Processes. However, it's important to be cautious:\n",
    "\n",
    "- While there are strong equivalences for specific quantities (like the posterior mean and variance in GPs), the **philosophical foundations**, **interpretation of probability**, and the **treatment of uncertainty** often differ significantly between the two approaches.\n",
    "- For example, the Bayesian framework interprets probability as a degree of belief, while the Frequentist approach treats probability as a long-run frequency of events.\n",
    "- The way uncertainty is quantified and interpreted can also diverge, especially outside the special case of GPs.\n",
    "\n",
    "---\n",
    "\n",
    "> **Key Point:**  \n",
    "> The mathematical overlap between Bayesian and Frequentist perspectives in GPs is profound, but the two frameworks are not identical in general. Their interpretations and practical implications can differ, especially when we move beyond the quantities where their answers coincide.\n",
    "\n",
    "---\n",
    "\n",
    "In the next sections, we'll highlight a crucial difference regarding the **sample paths** of a Gaussian Process, and see where the Bayesian and Frequentist perspectives start to diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ab9e7",
   "metadata": {},
   "source": [
    "## What is the RKHS? (2) Representation in Terms of Eigenfunctions\n",
    "\n",
    "The **Reproducing Kernel Hilbert Space (RKHS)** can also be characterized using the eigenfunctions of its kernel, providing a powerful and intuitive perspective on the structure of this function space.\n",
    "\n",
    "---\n",
    "\n",
    "### **Theorem (Mercer Representation)**\n",
    "\n",
    "Let $X$ be a compact metric space, $k$ a continuous kernel on $X$, and $\\nu$ a finite Borel measure whose support is $X$. Let $\\{ (\\phi_i, \\lambda_i) \\}_{i \\in I}$ be the eigenfunctions and eigenvalues of $k$ with respect to $\\nu$. Then, the RKHS $\\mathcal{H}_k$ is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{H}_k = \\left\\{ f(x) := \\sum_{i \\in I} \\alpha_i \\lambda_i^{1/2} \\phi_i(x) \\;\\; \\Bigg| \\;\\; \\|f\\|^2_{\\mathcal{H}_k} := \\sum_{i \\in I} \\alpha_i^2 < \\infty \\right\\}\n",
    "$$\n",
    "\n",
    "with the inner product\n",
    "\n",
    "$$\n",
    "\\langle f, g \\rangle_{\\mathcal{H}_k} := \\sum_{i \\in I} \\alpha_i \\beta_i\n",
    "$$\n",
    "\n",
    "where $f(x) = \\sum_{i \\in I} \\alpha_i \\lambda_i^{1/2} \\phi_i(x)$ and $g(x) = \\sum_{i \\in I} \\beta_i \\lambda_i^{1/2} \\phi_i(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "- **Infinite Linear Combinations:**  \n",
    "    Every function in the RKHS can be written as an (infinite) linear combination of the kernel's eigenfunctions, weighted by $\\lambda_i^{1/2}$ and coefficients $\\alpha_i$.\n",
    "- **Square-Summable Coefficients:**  \n",
    "    The coefficients $\\alpha_i$ must satisfy $\\sum_{i \\in I} \\alpha_i^2 < \\infty$. This ensures that the function has finite \"complexity\" or \"smoothness\" as measured by the RKHS norm.\n",
    "- **Inner Product Structure:**  \n",
    "    The inner product in the RKHS is simply the dot product of the coefficient sequences $(\\alpha_i)$ and $(\\beta_i)$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Is This Useful?**\n",
    "\n",
    "- **Feature Space View:**  \n",
    "    This representation makes it clear that the RKHS is a space of functions built from the kernel's eigenfunctions, analogous to how a vector space is built from its basis vectors.\n",
    "- **Smoothness Control:**  \n",
    "    The decay of the eigenvalues $\\lambda_i$ controls the smoothness and richness of the RKHS. Faster decay means smoother functions.\n",
    "- **Practical Computation:**  \n",
    "    In practice, for finite datasets, we often approximate functions in the RKHS using only the leading eigenfunctions (those with largest eigenvalues).\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- The RKHS associated with a kernel $k$ consists of all functions that can be expressed as square-summable linear combinations of the kernel's eigenfunctions.\n",
    "- This spectral view provides deep insight into the expressiveness and limitations of kernel methods, including Gaussian Processes and Support Vector Machines.\n",
    "\n",
    "---\n",
    "\n",
    "> **Key Takeaway:**  \n",
    "> The RKHS is the \"natural habitat\" for kernel-based learning algorithms. Understanding its structure through eigenfunctions and eigenvalues is essential for grasping what kinds of functions your model can represent and learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6761cc2c",
   "metadata": {},
   "source": [
    "# What About the Samples? GP Samples Are *Not* in the RKHS!\n",
    "\n",
    "This is a subtle but important point that often surprises beginners:  \n",
    "**While the posterior mean of a Gaussian Process (GP) always lives in the RKHS, the sample paths (i.e., individual functions drawn from the GP) almost never do—especially when the RKHS is infinite-dimensional.**\n",
    "\n",
    "---\n",
    "\n",
    "## Why Aren't GP Samples in the RKHS?\n",
    "\n",
    "To understand this, let's introduce the **Karhunen–Loève Expansion**, which provides a spectral decomposition for stochastic processes:\n",
    "\n",
    "### **Theorem (Karhunen–Loève Expansion, Simplified)**\n",
    "\n",
    "Let $X$ be a compact metric space, $k : X \\times X \\to \\mathbb{R}$ a continuous kernel, $\\nu$ a finite Borel measure whose support is $X$, and $\\{ (\\phi_i, \\lambda_i) \\}_{i \\in I}$ the eigenfunctions and eigenvalues of $k$ (from Mercer's theorem).  \n",
    "Let $\\{ z_i \\}_{i \\in I}$ be i.i.d. standard normal random variables ($z_i \\sim \\mathcal{N}(0, 1)$).\n",
    "\n",
    "Then, a sample path $f$ from the GP can be written as:\n",
    "$$\n",
    "f(x) = \\sum_{i \\in I} z_i \\lambda_i^{1/2} \\phi_i(x) \\sim \\mathcal{GP}(0, k)\n",
    "$$\n",
    "\n",
    "- Each sample path is an **infinite sum** of eigenfunctions, weighted by random Gaussian coefficients and the square roots of the eigenvalues.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Don't These Sample Paths Belong to the RKHS?**\n",
    "\n",
    "Recall that the RKHS norm of a function $f$ (with expansion $f(x) = \\sum_{i \\in I} \\alpha_i \\lambda_i^{1/2} \\phi_i(x)$) is:\n",
    "$$\n",
    "\\| f \\|_{\\mathcal{H}_k}^2 = \\sum_{i \\in I} \\alpha_i^2\n",
    "$$\n",
    "\n",
    "For a GP sample path, the coefficients are random: $\\alpha_i = z_i$.  \n",
    "So, the expected RKHS norm squared is:\n",
    "$$\n",
    "\\mathbb{E}\\left[ \\| f \\|_{\\mathcal{H}_k}^2 \\right] = \\mathbb{E}\\left[ \\sum_{i \\in I} z_i^2 \\right] = \\sum_{i \\in I} \\mathbb{E}[z_i^2] = \\sum_{i \\in I} 1\n",
    "$$\n",
    "\n",
    "- **If $I$ is infinite, this sum diverges:** $\\sum_{i \\in I} 1 = \\infty$.\n",
    "- Therefore, with probability 1, a GP sample path has **infinite RKHS norm** and does **not** belong to the RKHS.\n",
    "\n",
    "---\n",
    "\n",
    "## **Corollary (Wahba, 1990; see also Kanagawa et al., Thm. 4.9)**\n",
    "\n",
    "> If the set of eigenfunctions is infinite, then for $f \\sim \\mathcal{GP}(0, k)$, almost surely $f \\notin \\mathcal{H}_k$.\n",
    "\n",
    "---\n",
    "\n",
    "## **Intuitive Summary**\n",
    "\n",
    "- **Posterior mean functions** of a GP always live in the RKHS.\n",
    "- **Sample paths** from a GP are \"rougher\" or \"more complex\" than any function in the RKHS, and almost never belong to it when the RKHS is infinite-dimensional.\n",
    "- This distinction is crucial for understanding the expressiveness and limitations of GPs and kernel methods.\n",
    "\n",
    "---\n",
    "\n",
    "> **Key Takeaway:**  \n",
    "> The RKHS is the space of \"well-behaved\" functions that kernel methods can learn as mean predictions, but the random functions sampled from a GP prior are almost always much wilder!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46545dc",
   "metadata": {},
   "source": [
    "# What About the Samples? Sample Spaces of Gaussian Processes\n",
    "\n",
    "Understanding the **sample space** of a Gaussian Process (GP) is subtle and foundational for kernel methods and probabilistic modeling. Here, we clarify what kinds of functions GP samples are, and how this relates to the kernel and the RKHS.\n",
    "\n",
    "---\n",
    "\n",
    "## Function Spaces Containing GP Samples\n",
    "\n",
    "- The **precise sample space** of a GP is often difficult to characterize exactly.\n",
    "- Instead, we typically identify *function spaces* that are large enough to contain (almost all) GP sample paths.\n",
    "\n",
    "### Why Does This Matter?\n",
    "\n",
    "- If you know your target function has certain properties (e.g., continuity, differentiability), you should **choose a kernel** whose GP sample paths have matching properties. This can dramatically improve learning efficiency.\n",
    "\n",
    "### Common Function Spaces for GP Samples\n",
    "\n",
    "- $\\mathbb{R}^X$: The space of all real-valued functions on $X$ (too large for practical use).\n",
    "- **Banach space $C(X)$**: The space of continuous functions on $X$.\n",
    "- **Banach space $C^k(X)$**: The space of $k$-times continuously differentiable functions (useful for modeling derivatives, e.g., in Bayesian optimization).\n",
    "- **Sobolev spaces $W_2^k(X)$**: Spaces of functions with square-integrable derivatives up to order $k$ (important in PDE inference and functional analysis).\n",
    "\n",
    "---\n",
    "\n",
    "## GP Samples Are *Not* in the RKHS! But Almost...\n",
    "\n",
    "- **Key fact:** While the **posterior mean** of a GP always lies in the RKHS associated with the kernel, **sample paths drawn from the GP almost never do** (when the RKHS is infinite-dimensional).\n",
    "- However, GP samples do belong to a slightly larger space—a \"completion\" of the RKHS.\n",
    "\n",
    "---\n",
    "\n",
    "## Theorem: GP Samples in Interpolated RKHS Spaces\n",
    "\n",
    "**(Kanagawa, 2018; restricted from Steinwart, 2017; generalizing Driscoll, 1973)**\n",
    "\n",
    "Let $\\mathcal{H}_k$ be the RKHS of kernel $k$, and $0 < \\theta \\leq 1$. Define the $\\theta$-power of the RKHS as:\n",
    "\n",
    "$$\n",
    "\\mathcal{H}_k^\\theta = \\left\\{ f(x) := \\sum_{i \\in I} \\alpha_i \\lambda_i^{\\theta/2} \\phi_i(x) \\;\\; \\Bigg| \\;\\; \\|f\\|^2_{\\mathcal{H}_k^\\theta} := \\sum_{i \\in I} \\alpha_i^2 < \\infty \\right\\}\n",
    "$$\n",
    "\n",
    "with inner product\n",
    "\n",
    "$$\n",
    "\\langle f, g \\rangle_{\\mathcal{H}_k^\\theta} := \\sum_{i \\in I} \\alpha_i \\beta_i\n",
    "$$\n",
    "\n",
    "where $f(x) = \\sum_{i} \\alpha_i \\lambda_i^{\\theta/2} \\phi_i(x)$ and $g(x) = \\sum_{i} \\beta_i \\lambda_i^{\\theta/2} \\phi_i(x)$.\n",
    "\n",
    "If\n",
    "\n",
    "$$\n",
    "\\sum_{i \\in I} \\lambda_i^{1-\\theta} < \\infty,\n",
    "$$\n",
    "\n",
    "then for $f \\sim \\mathcal{GP}(0, k)$, we have $f \\in \\mathcal{H}_k^\\theta$ with probability 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "- **GP samples are \"almost\" in the RKHS**: They live in a slightly larger space, $\\mathcal{H}_k^\\theta$, for any $\\theta < 1$ (provided the eigenvalues decay fast enough).\n",
    "- The faster the eigenvalues $\\lambda_i$ decay, the closer the GP samples are to being in the RKHS.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- The choice of kernel determines not only the RKHS (the space of possible posterior means), but also the broader function space where GP samples live.\n",
    "- **GP samples are typically rougher and more complex than any function in the RKHS**, but they are still \"well-behaved\" in a precise mathematical sense.\n",
    "- Understanding these spaces helps you select kernels that encode the right assumptions for your modeling task, leading to better learning and generalization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22182a46",
   "metadata": {},
   "source": [
    "# Summary: Understanding Kernels and Gaussian Processes (GPs)\n",
    "\n",
    "This lecture provided a deeper, more theoretical understanding of **Gaussian Processes (GPs)** and their underlying **kernels**. Here are the key takeaways, organized for clarity and accessibility:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. GPs as Distributions over Functions\n",
    "\n",
    "- **Gaussian Processes** are truly probability distributions over spaces of functions.\n",
    "- However, the probability space defined by a GP is only *weakly specified* by its general construction and lacks much useful structure.\n",
    "- To understand what kinds of functions a GP can generate, we must study the **kernel** in detail.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Kernel–Covariance Equivalence\n",
    "\n",
    "- **Every covariance function is a kernel**, and **every kernel can serve as the covariance function of a GP**.\n",
    "- This fundamental equivalence means that specifying a valid kernel is sufficient to define a GP.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Kernels as \"Infinite Matrices\"\n",
    "\n",
    "- Kernels have **eigenfunctions**, just as matrices have eigenvectors.\n",
    "- You can think of a kernel as a kind of *infinite matrix* that spans a space of functions.\n",
    "- This perspective is formalized by **Mercer's Theorem**, which connects kernels to infinite-dimensional feature spaces.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Reproducing Kernel Hilbert Space (RKHS)\n",
    "\n",
    "- The space spanned by the kernel's eigenfunctions is called the **Reproducing Kernel Hilbert Space (RKHS)**.\n",
    "- The RKHS is **identical to the space of all possible posterior mean functions** in GP regression.\n",
    "- This directly connects GPs to frequentist kernel methods, such as **Kernel Ridge Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Posterior Variance as Worst-Case Error\n",
    "\n",
    "- The **posterior covariance function** (the Bayesian average squared error) is also a **worst-case squared error in the RKHS**.\n",
    "- This reveals a deep connection between Bayesian uncertainty and frequentist error bounds.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. GP Samples vs. RKHS\n",
    "\n",
    "- **GP samples do not lie in the RKHS**. While the posterior mean always lives in the RKHS, the functions sampled from a GP are generally \"rougher\" and more complex.\n",
    "- To study GP samples, we often use a *power* of the RKHS (a slightly larger function space), but not the RKHS itself.\n",
    "- This highlights a key distinction:  \n",
    "    - **What a GP can generate (samples) is generally rougher than what it can learn or represent (posterior means).**\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "A thorough understanding of the mathematical properties of kernels and RKHSs is essential for the effective application and interpretation of Gaussian Processes.  \n",
    "While GPs are powerful and flexible tools, their true capabilities and limitations are best understood through the lens of kernel theory and functional analysis.\n",
    "\n",
    "---\n",
    "\n",
    "> **Key Takeaway:**  \n",
    "> Mastering kernels and RKHSs unlocks a deeper intuition for GPs, bridging Bayesian and frequentist perspectives, and empowering you to design better models for real-world data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
