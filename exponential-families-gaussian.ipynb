{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d5fb579",
   "metadata": {},
   "source": [
    "# Lecture 05: Exponential Families\n",
    "\n",
    "Based on the lecture slides by Philipp Hennig (SS 2023).\n",
    "\n",
    "This notebook explores the concept of Exponential Families, their properties related to sufficient statistics, conjugate priors, and maximum likelihood estimation, with coding examples primarily using JAX.\n",
    "\n",
    "## The Skeleton of ML and Conjugate Priors\n",
    "\n",
    "The slides introduce probabilistic inference as a key component of ML.\n",
    "$$p(w|x) = \\frac{p(x|w) p(w)}{\\int p(x|w) p(w) dw}$$\n",
    "For i.i.d. data $x = \\{x_1, \\dots, x_n\\}$, this becomes:\n",
    "$$p(w|x) = \\frac{\\prod_{i=1}^n p(x_i|w) p(w)}{\\int \\prod_{i=1}^n p(x_i|w) p(w) dw}$$\n",
    "\n",
    "This general form can be complex. The concept of **Conjugate Priors** simplifies Bayesian inference by ensuring the posterior has the same functional form as the prior, with parameters updated based on **sufficient statistics** $\\phi(x)$ of the data.\n",
    "\n",
    "$$p(w|x) \\propto l(x; w) g(w; \\theta) = g(w; \\theta + \\phi(x))$$\n",
    "\n",
    "The power of conjugate priors is that the complex data likelihood $\\prod_{i=1}^n p(x_i|w)$ combines with the prior $p(w)$ in a way that the data's influence is entirely captured by the sufficient statistics $\\phi(x)$ and the number of data points $n$.\n",
    "\n",
    "## Exponential Families: Definition\n",
    "\n",
    "Exponential Families are a class of probability distributions for which conjugate priors naturally exist.\n",
    "\n",
    "A probability distribution for a random variable $X$ is in the exponential family if its probability density/mass function can be written in the form:\n",
    "$$p_w(x) = h(x) \\exp[\\phi(x)^T w - \\log Z(w)]$$\n",
    "or equivalently\n",
    "$$p_w(x) = \\frac{h(x)}{Z(w)} \\exp[\\phi(x)^T w]$$\n",
    "\n",
    "Where:\n",
    "- $h(x)$: the **base measure**. A non-negative function depending only on $x$.\n",
    "- $\\phi(x)$: the **sufficient statistics**. A vector function of the data $x$. It summarizes all the information from the data relevant to the natural parameters $w$.\n",
    "- $w$: the **natural parameters**. A vector of parameters for the distribution.\n",
    "- $Z(w)$: the **partition function**. A normalization constant ensuring the distribution integrates/sums to 1. It depends on $w$. $\\log Z(w)$ is the log-partition function.\n",
    "\n",
    "The slides also mention canonical parameters $\\theta$, where $w = \\eta(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09ed4d",
   "metadata": {},
   "source": [
    "\n",
    "### Example: The Univariate Gaussian Distribution as an Exponential Family\n",
    "\n",
    "Let's express the probability density function (PDF) of a univariate Gaussian $N(x; \\mu, \\sigma^2)$ in the exponential family form.\n",
    "\n",
    "The standard PDF is:\n",
    "$$p(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "We need to rearrange the exponent to isolate terms that are a linear function of some parameters multiplied by some function of $x$.\n",
    "$$-\\frac{(x - \\mu)^2}{2\\sigma^2} = -\\frac{x^2 - 2\\mu x + \\mu^2}{2\\sigma^2} = \\left(\\frac{\\mu}{\\sigma^2}\\right) x + \\left(-\\frac{1}{2\\sigma^2}\\right) x^2 - \\frac{\\mu^2}{2\\sigma^2}$$\n",
    "\n",
    "Now, let's match this to the exponential family form $h(x) \\exp[\\phi(x)^T w - \\log Z(w)]$:\n",
    "\n",
    "- **Sufficient Statistics:** These are the parts depending only on $x$ that are multiplied by the parameters in the exponent. Based on the rearranged exponent, we can choose:\n",
    "  $$\\phi(x) = \\begin{bmatrix} x \\\\ x^2 \\end{bmatrix}$$\n",
    "  *(Note: The slide uses $\\phi(x) = [x, -x^2/2]^T$. Let's use the slide's definition for consistency as it directly maps to the natural parameters shown there.)*\n",
    "  $$\\phi(x) = \\begin{bmatrix} x \\\\ -x^2/2 \\end{bmatrix}$$\n",
    "\n",
    "- **Natural Parameters:** These are the parameters that multiply the sufficient statistics in the exponent. Matching the terms:\n",
    "  $$w = \\begin{bmatrix} \\mu/\\sigma^2 \\\\ 1/\\sigma^2 \\end{bmatrix}$$\n",
    "\n",
    "- **Base Measure:** $h(x)$ is the part depending only on $x$ outside the main exponential term. In the standard Gaussian PDF, there's no explicit $h(x)$ term outside the $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ and the main exponential. We can effectively set $h(x) = 1$ and absorb the $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ into the normalization constant $Z(w)$.\n",
    "\n",
    "- **Log-Partition Function:** $\\log Z(w)$ must absorb terms depending only on the parameters $(\\mu, \\sigma^2)$, or equivalently, $w$. From the original PDF and the rearrangement, the terms that depend only on parameters are $-\\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)$ and $-\\frac{\\mu^2}{2\\sigma^2}$.\n",
    "  $$-\\left(-\\log\\left(\\sqrt{2\\pi\\sigma^2}\\right)\\right) - \\frac{\\mu^2}{2\\sigma^2} = \\log(\\sqrt{2\\pi\\sigma^2}) - \\frac{\\mu^2}{2\\sigma^2}$$\n",
    "  So, $\\log Z(w) = \\frac{\\mu^2}{2\\sigma^2} + \\log(\\sqrt{2\\pi\\sigma^2})$.\n",
    "  Now, express this in terms of $w_1 = \\mu/\\sigma^2$ and $w_2 = 1/\\sigma^2$. This means $\\mu = w_1/w_2$ and $\\sigma^2 = 1/w_2$.\n",
    "  $$\\log Z(w) = \\frac{(w_1/w_2)^2}{2(1/w_2)} + \\log\\left(\\sqrt{\\frac{2\\pi}{w_2}}\\right) = \\frac{w_1^2}{2w_2} + \\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(w_2)$$\n",
    "  Often, the constant term $\\frac{1}{2}\\log(2\\pi)$ is omitted in the definition of $\\log Z(w)$ as it cancels out when normalizing, but it's needed for the PDF to integrate to 1. Let's include it for completeness when comparing to the standard PDF.\n",
    "\n",
    "Let's implement the sufficient statistics and log-partition function for the univariate Gaussian using JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a6bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Natural Parameters (mu=1, sigma^2=0.5): w = [2. 2.]\n",
      "Log Partition Function for w: 1.5723649263381958\n",
      "Log PDF at x=1.0 with w=[2. 2.] (EF form): -0.5723649263381958\n",
      "Standard Log PDF at x=1.0 for N(1, 0.5): -0.5723649263381958\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import logsumexp  # Useful for log-likelihood calculations\n",
    "import jax.scipy.stats as jss  # For standard distributions\n",
    "import optax\n",
    "\n",
    "\n",
    "# Define the sufficient statistics function for a single data point x\n",
    "def gaussian_sufficient_statistics(x):\n",
    "    \"\"\"\n",
    "    Sufficient statistics for the univariate Gaussian, following the slide's definition.\n",
    "    phi(x) = [x, -x^2/2]\n",
    "    \"\"\"\n",
    "    return jnp.array([x, -0.5 * x**2])\n",
    "\n",
    "\n",
    "# Define the log-partition function for the univariate Gaussian in terms of natural parameters\n",
    "def gaussian_log_partition_function(w):\n",
    "    \"\"\"\n",
    "    Log-partition function for the univariate Gaussian in terms of natural parameters.\n",
    "    w = [w1, w2], where w1 = mu/sigma^2, w2 = 1/sigma^2\n",
    "    log Z(w) = 0.5 * (w1^2 / w2) - 0.5 * log(w2) + 0.5 * log(2*pi)\n",
    "    We include the constant here to match the standard log PDF.\n",
    "    \"\"\"\n",
    "    w1, w2 = w\n",
    "    # Ensure w2 is positive (corresponds to sigma^2 > 0)\n",
    "    # Adding a small epsilon can improve numerical stability if w2 can be close to zero.\n",
    "    # For this example, we assume w2 > 0.\n",
    "    log_Z = 0.5 * (w1**2 / w2) - 0.5 * jnp.log(w2) + 0.5 * jnp.log(2 * jnp.pi)\n",
    "    return log_Z\n",
    "\n",
    "\n",
    "# Define the log PDF of the Gaussian in Exponential Family form\n",
    "# log p_w(x) = log h(x) + phi(x)^T w - log Z(w)\n",
    "# Assuming h(x) = 1, so log h(x) = 0\n",
    "def gaussian_log_pdf_ef(x, w):\n",
    "    \"\"\"\n",
    "    Log PDF of the univariate Gaussian in Exponential Family form.\n",
    "    Assumes h(x) = 1.\n",
    "    \"\"\"\n",
    "    phi_x = gaussian_sufficient_statistics(x)\n",
    "    log_Z_w = gaussian_log_partition_function(w)\n",
    "    return jnp.dot(phi_x, w) - log_Z_w\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Let's pick some natural parameters corresponding to N(x; mu=1, sigma^2=0.5)\n",
    "# mu = 1.0, sigma^2 = 0.5\n",
    "# w1 = mu/sigma^2 = 1.0 / 0.5 = 2.0\n",
    "# w2 = 1/sigma^2 = 1 / 0.5 = 2.0\n",
    "w_example = jnp.array([2.0, 2.0])\n",
    "\n",
    "# Calculate log Z for these parameters\n",
    "log_Z_example = gaussian_log_partition_function(w_example)\n",
    "print(f\"Example Natural Parameters (mu=1, sigma^2=0.5): w = {w_example}\")\n",
    "print(f\"Log Partition Function for w: {log_Z_example}\")\n",
    "\n",
    "# Evaluate the log PDF for a data point x=1.0 using the EF form\n",
    "x_example = 1.0\n",
    "log_pdf_example_ef = gaussian_log_pdf_ef(x_example, w_example)\n",
    "print(f\"Log PDF at x={x_example} with w={w_example} (EF form): {log_pdf_example_ef}\")\n",
    "\n",
    "# For comparison, the standard log PDF of N(x; mu=1, sigma^2=0.5) at x=1.0\n",
    "log_pdf_standard = jss.norm.logpdf(x_example, loc=1.0, scale=jnp.sqrt(0.5))\n",
    "print(f\"Standard Log PDF at x={x_example} for N(1, 0.5): {log_pdf_standard}\")\n",
    "\n",
    "# The two log PDF values should match closely, confirming the EF representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1f6b5",
   "metadata": {},
   "source": [
    "## Sufficient Statistics and Data Reduction\n",
    "A key property of exponential families is that for i.i.d. data $x = \\{x_1, \\dots, x_n\\}$, the joint likelihood is:\n",
    "\n",
    "$$\n",
    "p_w(x_1, \\dots, x_n \\mid w) = \\prod_{i=1}^n p_w(x_i \\mid w) = \\prod_{i=1}^n \\left[ h(x_i) \\exp\\left( \\phi(x_i)^T w - \\log Z(w) \\right) \\right] = \\left( \\prod_{i=1}^n h(x_i) \\right) \\exp\\left( \\left( \\sum_{i=1}^n \\phi(x_i) \\right)^T w - n \\log Z(w) \\right)\n",
    "$$\n",
    "\n",
    "This shows that the joint distribution depends on the data only through the sum of the sufficient statistics $\\sum_{i=1}^n \\phi(x_i)$ and the number of data points $n$. This is the **data reduction property**: instead of needing the full dataset, you only need to compute and store the sum of sufficient statistics.\n",
    "\n",
    "For the univariate Gaussian, the sum of sufficient statistics is\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\begin{bmatrix} x_i \\\\ -\\frac{1}{2} x_i^2 \\end{bmatrix} = \\begin{bmatrix} \\sum x_i \\\\ -\\frac{1}{2} \\sum x_i^2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This means for any number of data points, we only need the sum of the data points and the sum of their squares to compute the sufficient statistics for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a922085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100 data points from N(mu=5.0, sigma=2.0)\n",
      "First 5 data points: [8.245284  9.0505295 4.132811  4.8427653 5.352182 ]\n",
      "\n",
      "Sum of sufficient statistics for the dataset: [  521.8606 -1538.9651]\n",
      "Direct sum of x_i: 521.860595703125\n",
      "Direct sum of -0.5 * x_i^2: -1538.965087890625\n"
     ]
    }
   ],
   "source": [
    "# Generate some synthetic univariate Gaussian data\n",
    "key = jax.random.PRNGKey(0)\n",
    "mu_true = 5.0\n",
    "sigma_true = 2.0\n",
    "n_samples = 100\n",
    "synthetic_data = jax.random.normal(key, (n_samples,)) * sigma_true + mu_true\n",
    "\n",
    "print(f\"Generated {n_samples} data points from N(mu={mu_true}, sigma={sigma_true})\")\n",
    "print(f\"First 5 data points: {synthetic_data[:5]}\")\n",
    "\n",
    "# Calculate the sufficient statistics for each data point\n",
    "phi_data = jax.vmap(gaussian_sufficient_statistics)(synthetic_data)\n",
    "\n",
    "# Calculate the sum of sufficient statistics for the dataset\n",
    "sum_phi_x = jnp.sum(phi_data, axis=0)\n",
    "\n",
    "print(f\"\\nSum of sufficient statistics for the dataset: {sum_phi_x}\")\n",
    "\n",
    "# For univariate Gaussian, sum_phi_x = [sum(x_i), sum(-0.5 * x_i^2)]\n",
    "# Let's verify this against direct sums:\n",
    "direct_sum_x = jnp.sum(synthetic_data)\n",
    "direct_sum_neg_half_x_sq = jnp.sum(-0.5 * synthetic_data**2)\n",
    "\n",
    "print(f\"Direct sum of x_i: {direct_sum_x}\")\n",
    "print(f\"Direct sum of -0.5 * x_i^2: {direct_sum_neg_half_x_sq}\")\n",
    "# The values in sum_phi_x should match the direct sums.\n",
    "# This confirms that the sum of sufficient statistics captures the necessary information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045338b",
   "metadata": {},
   "source": [
    "## Conjugate Prior Parameter Update\n",
    "The slides state that if the likelihood $p_w(x \\mid w)$ is in an exponential family, its conjugate prior $p_\\alpha(w \\mid \\alpha, \\nu)$ is also in a related exponential family form. The key result is the simple update rule for the prior's parameters $(\\alpha, \\nu)$ to obtain the posterior's parameters $(\\alpha', \\nu')$ after observing data $x = \\{x_1, \\ldots, x_n\\}$:\n",
    "\n",
    "$$\n",
    "\\alpha' = \\alpha + \\sum_{i=1}^n \\phi(x_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nu' = \\nu + n\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ is a vector parameter with the same dimension as $\\phi(x)$, and $\\nu$ is a scalar parameter representing the \"prior number of observations\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe7f5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prior parameters: alpha_0 = [0.1 0.1], nu_0 = 1.0\n",
      "Observed data: n = 100, sum_phi_x = [  521.8606 -1538.9651]\n",
      "Posterior parameters: alpha_posterior = [  521.9606 -1538.8651], nu_posterior = 101.0\n"
     ]
    }
   ],
   "source": [
    "# Conceptual Conjugate Prior Update\n",
    "# For the Gaussian likelihood, the conjugate prior over (mu, sigma^2) is the Normal-Gamma distribution.\n",
    "# Its parameters are related to the (alpha, nu) parameters of the EF conjugate prior form.\n",
    "# Let's assume some initial prior parameters (alpha_0, nu_0)\n",
    "# For simplicity, let's use values that might represent a weak prior.\n",
    "alpha_0 = jnp.array([0.1, 0.1])  # Prior \"pseudo-sufficient statistics\"\n",
    "nu_0 = 1.0  # Prior \"pseudo-observation count\"\n",
    "\n",
    "print(f\"Initial prior parameters: alpha_0 = {alpha_0}, nu_0 = {nu_0}\")\n",
    "\n",
    "# We observed the 'n_samples' synthetic data points with sum of sufficient statistics 'sum_phi_x'\n",
    "\n",
    "# The updated posterior parameters are:\n",
    "alpha_posterior = alpha_0 + sum_phi_x\n",
    "nu_posterior = nu_0 + n_samples\n",
    "\n",
    "print(f\"Observed data: n = {n_samples}, sum_phi_x = {sum_phi_x}\")\n",
    "print(\n",
    "    f\"Posterior parameters: alpha_posterior = {alpha_posterior}, nu_posterior = {nu_posterior}\"\n",
    ")\n",
    "\n",
    "# These posterior parameters (alpha_posterior, nu_posterior) completely define the Normal-Gamma posterior distribution over the natural parameters w (which relate to mu and sigma^2).\n",
    "# The complexity of the data has been reduced to a simple addition to the prior parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743e54e",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE) in Exponential Families\n",
    "Exponential Families simplify Maximum Likelihood Estimation. The log-likelihood for i.i.d. data is:\n",
    "\n",
    "$$\n",
    "\\log p_w(x_1, \\ldots, x_n \\mid w) = \\left(\\sum_{i=1}^n \\phi(x_i)\\right)^T w - n \\log Z(w) + \\sum_{i=1}^n \\log h(x_i)\n",
    "$$\n",
    "\n",
    "To find the MLE $\\hat{w}$, we take the gradient with respect to $w$ and set it to zero:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\log p(x \\mid w) = \\sum_{i=1}^n \\phi(x_i) - n \\nabla_w \\log Z(w) = 0\n",
    "$$\n",
    "\n",
    "This gives the crucial property:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\log Z(w) = \\frac{1}{n} \\sum_{i=1}^n \\phi(x_i)\n",
    "$$\n",
    "\n",
    "The gradient of the log-partition function with respect to the natural parameters is equal to the empirical average of the sufficient statistics from the data at the MLE $\\hat{w}$.\n",
    "\n",
    "The MLE for $w$ is found by solving this equation.\n",
    "\n",
    "**Example: MLE for the Gaussian**\n",
    "\n",
    "For the univariate Gaussian, we found\n",
    "\n",
    "$$\n",
    "\\log Z(w) = \\frac{w_1^2}{2w_2} - \\frac{1}{2} \\log(w_2) + \\text{const}.\n",
    "$$\n",
    "\n",
    "The gradient with respect to $w = [w_1, w_2]^T$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\log Z(w) = \\left[\n",
    "    \\frac{\\partial}{\\partial w_1},\\ \\frac{\\partial}{\\partial w_2}\n",
    "\\right] \\left( \\frac{w_1^2}{2w_2} - \\frac{1}{2} \\log w_2 \\right)\n",
    "= \\left[ \\frac{w_1}{w_2},\\ -\\frac{w_1^2}{2w_2^2} - \\frac{1}{2w_2} \\right]\n",
    "$$\n",
    "\n",
    "(Ignoring the constant term as its gradient is zero.)\n",
    "\n",
    "The empirical mean of the sufficient statistics is\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\phi(x_i) = \\frac{1}{n} \\sum_{i=1}^n \\begin{bmatrix} x_i \\\\ -\\frac{1}{2} x_i^2 \\end{bmatrix}\n",
    "= \\begin{bmatrix} \\bar{x} \\\\ -\\frac{1}{2} \\overline{x^2} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Setting $\\nabla_w \\log Z(\\hat{w}) = \\frac{1}{n} \\sum \\phi(x_i)$:\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{w}_1}{\\hat{w}_2} = \\bar{x} \\\\\n",
    "-\\frac{\\hat{w}_1^2}{2\\hat{w}_2^2} - \\frac{1}{2\\hat{w}_2} = -\\frac{1}{2} \\overline{x^2}\n",
    "$$\n",
    "\n",
    "Solving these equations for $\\hat{w}_1$ and $\\hat{w}_2$ in terms of $\\bar{x}$ and $\\overline{x^2}$ gives the MLE for the natural parameters. As shown in the slides, this leads to\n",
    "\n",
    "$$\n",
    "\\hat{w}_1 = \\frac{\\bar{x}}{\\overline{x^2} - \\bar{x}^2}, \\qquad \\hat{w}_2 = \\frac{1}{\\overline{x^2} - \\bar{x}^2}\n",
    "$$\n",
    "\n",
    "which correspond to the standard MLEs for $\\mu$ and $\\sigma^2$.\n",
    "\n",
    "We can use JAX's automatic differentiation to compute the gradient of our `gaussian_log_partition_function` and verify this property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c330284a",
   "metadata": {},
   "source": [
    "Certainly! Here’s an explanation of the analytic solution for Maximum Likelihood Estimation (MLE) of the univariate Gaussian in its standard (normal) form:\n",
    "\n",
    "---\n",
    "\n",
    "### Analytic Solution of Gaussian MLE (Normal Form)\n",
    "\n",
    "Suppose you have $n$ i.i.d. data points $x_1, x_2, \\ldots, x_n$ drawn from a Gaussian (normal) distribution with unknown mean $\\mu$ and variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "p(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "#### Log-Likelihood\n",
    "\n",
    "The log-likelihood for the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd80f8",
   "metadata": {},
   "source": [
    "### Moment Matching vs. Direct Maximization: Convergence Speed and Stability\n",
    "\n",
    "**Moment Matching (Solving $\\nabla_w \\log Z(w) = \\text{target}$):**\n",
    "- **Pros:**  \n",
    "    - For exponential families with simple $\\log Z(w)$, moment matching can be very efficient, especially if an analytic solution exists (e.g., Gaussian).\n",
    "    - The optimization objective is often well-behaved (quadratic for Gaussian), leading to fast and stable convergence.\n",
    "- **Cons:**  \n",
    "    - For complex or high-dimensional models, the root-finding problem may be non-convex or require careful initialization.\n",
    "    - Requires computing the gradient of $\\log Z(w)$, which may be expensive for some distributions.\n",
    "\n",
    "**Direct Maximization (Maximizing Log Likelihood):**\n",
    "- **Pros:**  \n",
    "    - More general: works for any model where you can write the likelihood, not just exponential families.\n",
    "    - The negative log-likelihood is often convex for exponential families, leading to stable optimization.\n",
    "- **Cons:**  \n",
    "    - May converge slower if the objective is poorly scaled or if the optimizer is not well-tuned.\n",
    "    - Still requires evaluating $\\log Z(w)$, which can be costly for some models.\n",
    "\n",
    "**In Practice:**\n",
    "- For exponential families, **both methods are equivalent** and typically converge quickly and stably if gradients are implemented correctly.\n",
    "- **Moment matching** can be slightly faster if the gradient of $\\log Z(w)$ is easy to compute and the target is well-scaled.\n",
    "- **Direct maximization** is more flexible and often preferred in practice, especially when extending to non-exponential family models.\n",
    "\n",
    "**Summary Table:**\n",
    "\n",
    "| Method             | Speed      | Stability | Generality |\n",
    "|--------------------|------------|-----------|------------|\n",
    "| Moment Matching    | Fast (EF)  | High      | Limited    |\n",
    "| Direct Maximization| Fast       | High      | General    |\n",
    "\n",
    "For the univariate Gaussian example in this notebook, **both methods are fast and stable**. For more complex models, direct maximization is usually preferred for its flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8705a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Natural Parameters: w = [1.25 0.25]\n",
      "Gradient of Log Z evaluated at true w: [  5.  -14.5]\n",
      "Expected value of Sufficient Statistics at true w: [  5.  -14.5]\n",
      "\n",
      "Empirical Mean of Sufficient Statistics from synthetic data: [  5.218606 -15.389651]\n",
      "MLE Natural Parameters estimated from empirical stats: w_hat_mle = [1.4719148  0.28205132]\n",
      "Gradient of Log Z evaluated at w_hat_mle: [  5.218606 -15.389652]\n"
     ]
    }
   ],
   "source": [
    "# Compute the gradient of the log-partition function using JAX's autograd\n",
    "grad_log_Z = jax.grad(\n",
    "    gaussian_log_partition_function\n",
    ")  # Let's evaluate the gradient at the natural parameters corresponding to the true mean and variance\n",
    "\n",
    "# mu_true = 5.0, sigma_true = 2.0, sigma_true^2 = 4.0\n",
    "# w1_true = mu_true / sigma_true^2 = 5.0 / 4.0 = 1.25\n",
    "# w2_true = 1 / sigma_true^2 = 1 / 4.0 = 0.25\n",
    "w_true = jnp.array([1.25, 0.25])\n",
    "\n",
    "gradient_at_true_w = grad_log_Z(w_true)\n",
    "\n",
    "print(f\"True Natural Parameters: w = {w_true}\")\n",
    "print(f\"Gradient of Log Z evaluated at true w: {gradient_at_true_w}\")\n",
    "\n",
    "# According to theory, this gradient should be equal to the expected value of the sufficient statistics\n",
    "# under the distribution N(mu_true, sigma_true^2).\n",
    "# E[phi(x)] = E[[x, -x^2/2]] = [E[x], -0.5 * E[x^2]]\n",
    "# E[x] = mu_true = 5.0\n",
    "# E[x^2] = Var(x) + (E[x])^2 = sigma_true^2 + mu_true^2 = 4.0 + 5.0**2 = 4.0 + 25.0 = 29.0\n",
    "# E[-x^2/2] = -0.5 * E[x^2] = -0.5 * 29.0 = -14.5\n",
    "\n",
    "expected_phi_true_w = jnp.array([mu_true, -0.5 * (sigma_true**2 + mu_true**2)])\n",
    "print(f\"Expected value of Sufficient Statistics at true w: {expected_phi_true_w}\")\n",
    "\n",
    "# The gradient evaluated at the true natural parameters should match the expected value of the sufficient statistics.\n",
    "\n",
    "# Now, let's find the empirical mean of sufficient statistics from our synthetic data\n",
    "empirical_mean_phi = sum_phi_x / n_samples\n",
    "print(\n",
    "    f\"\\nEmpirical Mean of Sufficient Statistics from synthetic data: {empirical_mean_phi}\"\n",
    ")\n",
    "\n",
    "# The MLE property states that grad_log_Z(w_hat_mle) = empirical_mean_phi.\n",
    "# We can find the w_hat_mle that satisfies this equation.\n",
    "# For the Gaussian, we derived the analytical form of w_hat_mle from empirical stats:\n",
    "mean_x = empirical_mean_phi[0]\n",
    "# Recover mean(x^2) from mean(-0.5 * x^2)\n",
    "mean_x_sq = -2 * empirical_mean_phi[1]\n",
    "\n",
    "# Calculate MLE natural parameters from empirical moments\n",
    "# w2_hat_mle = 1 / (mean(x^2) - mean(x)^2)\n",
    "# w1_hat_mle = mean(x) * w2_hat_mle\n",
    "w2_hat_mle = 1.0 / (mean_x_sq - mean_x**2)\n",
    "w1_hat_mle = mean_x * w2_hat_mle\n",
    "w_hat_mle = jnp.array([w1_hat_mle, w2_hat_mle])\n",
    "\n",
    "print(f\"MLE Natural Parameters estimated from empirical stats: w_hat_mle = {w_hat_mle}\")\n",
    "\n",
    "# Let's verify if the gradient of log Z at w_hat_mle is close to the empirical mean of sufficient statistics\n",
    "gradient_at_mle_w = grad_log_Z(w_hat_mle)\n",
    "print(f\"Gradient of Log Z evaluated at w_hat_mle: {gradient_at_mle_w}\")\n",
    "\n",
    "# These two values (empirical_mean_phi and gradient_at_mle_w) should be very close,\n",
    "# confirming the MLE property for Exponential Families computationally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da791e",
   "metadata": {},
   "source": [
    "### Analytic MLE Solution for the Univariate Gaussian (Normal Form)\n",
    "\n",
    "Given a set of independent and identically distributed (i.i.d.) samples $\\{x_1, x_2, \\dots, x_n\\}$ drawn from a univariate Gaussian distribution:\n",
    "\n",
    "$$\n",
    "x_i \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "The probability density function (PDF) of the normal distribution is:\n",
    "\n",
    "$$\n",
    "p(x_i \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "## Log-Likelihood Function\n",
    "\n",
    "The likelihood of the data is the product of the individual probabilities:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2) = \\prod_{i=1}^n p(x_i \\mid \\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Taking the natural logarithm gives the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\log L(\\mu, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "## MLE for $\\mu$ and $\\sigma^2$\n",
    "\n",
    "To find the MLEs, we take partial derivatives of the log-likelihood with respect to $\\mu$ and $\\sigma^2$, and set them to zero.\n",
    "\n",
    "### Estimate for $\\mu$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu} \\log L = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu) = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\mu$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$\n",
    "### Estimate for $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\sigma^2} \\log L = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^n (x_i - \\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{\\mu})^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a99f4d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE mu: 5.218605995178223\n",
      "MLE sigma^2: 3.5454607009887695\n",
      "MLE sigma: 1.882939338684082\n",
      "Natural parameters from (mu, sigma^2): w = [1.4719119 0.2820508]\n",
      "w_hat_mle from exponential family: [1.4719148  0.28205132]\n",
      "Difference (w_from_mle - w_hat_mle): [-2.861023e-06 -5.364418e-07]\n"
     ]
    }
   ],
   "source": [
    "# Compute MLE estimates for mu and sigma^2 from the synthetic data\n",
    "mu_mle = jnp.mean(synthetic_data)\n",
    "sigma2_mle = jnp.mean((synthetic_data - mu_mle) ** 2)\n",
    "sigma_mle = jnp.sqrt(sigma2_mle)\n",
    "\n",
    "print(f\"MLE mu: {mu_mle}\")\n",
    "print(f\"MLE sigma^2: {sigma2_mle}\")\n",
    "print(f\"MLE sigma: {sigma_mle}\")\n",
    "\n",
    "# Compute natural parameters from these MLE estimates\n",
    "w1_from_mle = mu_mle / sigma2_mle\n",
    "w2_from_mle = 1.0 / sigma2_mle\n",
    "w_from_mle = jnp.array([w1_from_mle, w2_from_mle])\n",
    "\n",
    "print(f\"Natural parameters from (mu, sigma^2): w = {w_from_mle}\")\n",
    "\n",
    "# Compare to w_hat_mle computed via exponential family moment matching\n",
    "print(f\"w_hat_mle from exponential family: {w_hat_mle}\")\n",
    "\n",
    "# Show the difference\n",
    "diff = w_from_mle - w_hat_mle\n",
    "print(f\"Difference (w_from_mle - w_hat_mle): {diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb94d37",
   "metadata": {},
   "source": [
    "To numerically infer the natural parameters (MLE) for an exponential family, you solve the equation:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\log Z(w) = \\frac{1}{n} \\sum_{i=1}^n \\phi(x_i)\n",
    "$$\n",
    "\n",
    "This is a root-finding or optimization problem. The typical approach is:\n",
    "\n",
    "1. **Define a loss/objective function** that measures the squared difference between $\\nabla_w \\log Z(w)$ and the empirical mean of sufficient statistics.\n",
    "2. **Use a numerical optimizer** (e.g., gradient descent, Adam) to minimize this loss with respect to $w$.\n",
    "\n",
    "Example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6102a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"objective_fn\"])\n",
    "def step(w, opt_state, objective_fn):\n",
    "    loss, grads = jax.value_and_grad(objective_fn)(w)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, w)\n",
    "    w = optax.apply_updates(w, updates)\n",
    "    return w, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1fd9dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 9596/10000 [00:00<00:00, 47043.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: w_guess = [0.00999993 0.99000007], loss = 248.93556213378906\n",
      "Iteration 100: w_guess = [0.8779475 0.1792744], loss = 0.8200246691703796\n",
      "Iteration 200: w_guess = [0.8870315  0.17669371], loss = 0.041085150092840195\n",
      "Iteration 300: w_guess = [0.8895079  0.17716952], loss = 0.04048626869916916\n",
      "Iteration 400: w_guess = [0.8925052  0.17770821], loss = 0.03981725499033928\n",
      "Iteration 500: w_guess = [0.8959408 0.1783258], loss = 0.03906245157122612\n",
      "Iteration 600: w_guess = [0.89977425 0.17901497], loss = 0.038234904408454895\n",
      "Iteration 700: w_guess = [0.9039749  0.17977014], loss = 0.037345465272665024\n",
      "Iteration 800: w_guess = [0.90851647 0.18058664], loss = 0.03640415146946907\n",
      "Iteration 900: w_guess = [0.91337705 0.18146057], loss = 0.03541946038603783\n",
      "Iteration 1000: w_guess = [0.9185372  0.18238838], loss = 0.03439943119883537\n",
      "Iteration 1100: w_guess = [0.9239797 0.183367 ], loss = 0.03335116058588028\n",
      "Iteration 1200: w_guess = [0.9296882  0.18439354], loss = 0.03228117898106575\n",
      "Iteration 1300: w_guess = [0.9356478  0.18546528], loss = 0.0311955064535141\n",
      "Iteration 1400: w_guess = [0.94184434 0.18657964], loss = 0.030099788680672646\n",
      "Iteration 1500: w_guess = [0.94826496 0.18773438], loss = 0.02899906411767006\n",
      "Iteration 1600: w_guess = [0.9548968 0.1889272], loss = 0.027897909283638\n",
      "Iteration 1700: w_guess = [0.9617281  0.19015595], loss = 0.02680061012506485\n",
      "Iteration 1800: w_guess = [0.96874785 0.19141869], loss = 0.02571052312850952\n",
      "Iteration 1900: w_guess = [0.9759448  0.19271335], loss = 0.024631662294268608\n",
      "Iteration 2000: w_guess = [0.9833086  0.19403808], loss = 0.023566612973809242\n",
      "Iteration 2100: w_guess = [0.9908292  0.19539115], loss = 0.022518137469887733\n",
      "Iteration 2200: w_guess = [0.99849707 0.19677076], loss = 0.021488703787326813\n",
      "Iteration 2300: w_guess = [1.0063032  0.19817533], loss = 0.020480230450630188\n",
      "Iteration 2400: w_guess = [1.0142382  0.19960319], loss = 0.01949445530772209\n",
      "Iteration 2500: w_guess = [1.0222936  0.20105273], loss = 0.01853291317820549\n",
      "Iteration 2600: w_guess = [1.0304607  0.20252255], loss = 0.01759692095220089\n",
      "Iteration 2700: w_guess = [1.038732   0.20401113], loss = 0.01668747328221798\n",
      "Iteration 2800: w_guess = [1.0470991  0.20551704], loss = 0.015805479139089584\n",
      "Iteration 2900: w_guess = [1.0555547  0.20703898], loss = 0.014951454475522041\n",
      "Iteration 3000: w_guess = [1.0640908  0.20857549], loss = 0.014125923626124859\n",
      "Iteration 3100: w_guess = [1.072701   0.21012542], loss = 0.013329053297638893\n",
      "Iteration 3200: w_guess = [1.0813773  0.21168728], loss = 0.012561320327222347\n",
      "Iteration 3300: w_guess = [1.0901132  0.21326002], loss = 0.011822468601167202\n",
      "Iteration 3400: w_guess = [1.0989007  0.21484213], loss = 0.011112681590020657\n",
      "Iteration 3500: w_guess = [1.107734  0.2164325], loss = 0.010431730188429356\n",
      "Iteration 3600: w_guess = [1.1166061  0.21802995], loss = 0.009779399260878563\n",
      "Iteration 3700: w_guess = [1.1255103  0.21963327], loss = 0.009155236184597015\n",
      "Iteration 3800: w_guess = [1.1344395  0.22124113], loss = 0.008558903820812702\n",
      "Iteration 3900: w_guess = [1.143387  0.2228524], loss = 0.007989916950464249\n",
      "Iteration 4000: w_guess = [1.1523479  0.22446613], loss = 0.0074478439055383205\n",
      "Iteration 4100: w_guess = [1.1613125 0.2260806], loss = 0.006932209245860577\n",
      "Iteration 4200: w_guess = [1.1702766  0.22769503], loss = 0.006442222278565168\n",
      "Iteration 4300: w_guess = [1.1792325  0.22930811], loss = 0.005977359134703875\n",
      "Iteration 4400: w_guess = [1.1881739  0.23091862], loss = 0.005536995828151703\n",
      "Iteration 4500: w_guess = [1.1970931  0.23252515], loss = 0.005120447836816311\n",
      "Iteration 4600: w_guess = [1.2059841  0.23412667], loss = 0.00472685880959034\n",
      "Iteration 4700: w_guess = [1.2148392 0.2357218], loss = 0.004355749115347862\n",
      "Iteration 4800: w_guess = [1.2236528 0.2373095], loss = 0.004006209783256054\n",
      "Iteration 4900: w_guess = [1.2324164  0.23888823], loss = 0.0036776268389075994\n",
      "Iteration 5000: w_guess = [1.2411236 0.2404569], loss = 0.003369248239323497\n",
      "Iteration 5100: w_guess = [1.2497671  0.24201421], loss = 0.0030802953988313675\n",
      "Iteration 5200: w_guess = [1.2583404  0.24355875], loss = 0.002809993689879775\n",
      "Iteration 5300: w_guess = [1.2668352  0.24508928], loss = 0.002557578030973673\n",
      "Iteration 5400: w_guess = [1.2752444  0.24660437], loss = 0.0023224621545523405\n",
      "Iteration 5500: w_guess = [1.28356    0.24810286], loss = 0.002103822072967887\n",
      "Iteration 5600: w_guess = [1.2917755  0.24958314], loss = 0.0019008312374353409\n",
      "Iteration 5700: w_guess = [1.2998824  0.25104398], loss = 0.00171287776902318\n",
      "Iteration 5800: w_guess = [1.307873   0.25248387], loss = 0.0015391675988212228\n",
      "Iteration 5900: w_guess = [1.3157407  0.25390163], loss = 0.0013791014207527041\n",
      "Iteration 6000: w_guess = [1.3234766 0.2552957], loss = 0.0012319165980443358\n",
      "Iteration 6100: w_guess = [1.3310733  0.25666466], loss = 0.0010968882124871016\n",
      "Iteration 6200: w_guess = [1.3385236 0.2580075], loss = 0.0009734096238389611\n",
      "Iteration 6300: w_guess = [1.3458195  0.25932232], loss = 0.0008608074858784676\n",
      "Iteration 6400: w_guess = [1.3529532  0.26060787], loss = 0.0007584072882309556\n",
      "Iteration 6500: w_guess = [1.3599168  0.26186302], loss = 0.0006656239274889231\n",
      "Iteration 6600: w_guess = [1.366703   0.26308626], loss = 0.0005818125791847706\n",
      "Iteration 6700: w_guess = [1.3733051  0.26427606], loss = 0.0005063641001470387\n",
      "Iteration 6800: w_guess = [1.381241  0.2656562], loss = 0.0008947719470597804\n",
      "Iteration 6900: w_guess = [1.3870679 0.2667546], loss = 0.00036780867958441377\n",
      "Iteration 7000: w_guess = [1.3924166 0.2677244], loss = 0.00032054472831077874\n",
      "Iteration 7100: w_guess = [1.3990583  0.26896095], loss = 0.00029413262382149696\n",
      "Iteration 7200: w_guess = [1.4036037  0.26973763], loss = 0.00023297028383240104\n",
      "Iteration 7300: w_guess = [1.4100096  0.27065384], loss = 0.0003250734298489988\n",
      "Iteration 7400: w_guess = [1.4139766  0.27161035], loss = 0.00016532151494175196\n",
      "Iteration 7500: w_guess = [1.417526   0.27224705], loss = 0.00014488787564914674\n",
      "Iteration 7600: w_guess = [1.4228327 0.2728664], loss = 0.0017705053323879838\n",
      "Iteration 7700: w_guess = [1.4257617  0.27372906], loss = 0.00010320414730813354\n",
      "Iteration 7800: w_guess = [1.428575   0.27433154], loss = 0.00015202962094917893\n",
      "Iteration 7900: w_guess = [1.4334234 0.2750957], loss = 7.227704190881923e-05\n",
      "Iteration 8000: w_guess = [1.4357326 0.2755287], loss = 6.256023334572092e-05\n",
      "Iteration 8100: w_guess = [1.4399015  0.27550048], loss = 0.004238601308315992\n",
      "Iteration 8200: w_guess = [1.4419956 0.2766539], loss = 4.247082688380033e-05\n",
      "Iteration 8300: w_guess = [1.4438258  0.27698797], loss = 3.7292651541065425e-05\n",
      "Iteration 8400: w_guess = [1.4478573 0.2777268], loss = 5.987044642097317e-05\n",
      "Iteration 8500: w_guess = [1.4493138 0.2779761], loss = 2.3969560061232187e-05\n",
      "Iteration 8600: w_guess = [1.4506935  0.27823412], loss = 2.1655851014656946e-05\n",
      "Iteration 8700: w_guess = [1.4538697 0.2787959], loss = 1.9047489331569523e-05\n",
      "Iteration 8800: w_guess = [1.4549601  0.27899498], loss = 1.3384596059040632e-05\n",
      "Iteration 8900: w_guess = [1.4579312 0.2795662], loss = 0.0002394558396190405\n",
      "Iteration 9000: w_guess = [1.4588821  0.27970222], loss = 7.875773917476181e-06\n",
      "Iteration 9100: w_guess = [1.4588721 0.2807129], loss = 0.0072120544500648975\n",
      "Iteration 9200: w_guess = [1.4622412  0.28031236], loss = 5.180715561436955e-06\n",
      "Iteration 9300: w_guess = [1.4628435  0.28040498], loss = 4.793740117747802e-06\n",
      "Iteration 9400: w_guess = [1.4651095 0.2808543], loss = 6.8536264734575525e-06\n",
      "Iteration 9500: w_guess = [1.4655492  0.28090262], loss = 1.8742650809144834e-06\n",
      "Converged at iteration 9596 with loss 7.517576250393176e-07\n",
      "\n",
      "Numerical MLE (w_guess after optimization): [1.4678253  0.28136122]\n",
      "Analytic MLE (w_hat_mle): [1.4719148  0.28205132]\n",
      "Difference (numerical - analytic): [-0.00408947 -0.0006901 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# empirical_mean_phi is already computed\n",
    "def mle_objective(w):\n",
    "    grad = grad_log_Z(w)\n",
    "    return jnp.sum((grad - empirical_mean_phi) ** 2)\n",
    "\n",
    "\n",
    "# Use an optimizer (e.g., Adam) to minimize mle_objective\n",
    "w_guess = jnp.array([0.0, 1.0])\n",
    "opt_state = optimizer.init(w_guess)\n",
    "\n",
    "\n",
    "for i in tqdm(range(10000)):\n",
    "    w_guess, opt_state, loss = step(w_guess, opt_state, mle_objective)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}: w_guess = {w_guess}, loss = {loss}\")\n",
    "\n",
    "    if loss < 1e-6:\n",
    "        print(f\"Converged at iteration {i} with loss {loss}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nNumerical MLE (w_guess after optimization):\", w_guess)\n",
    "print(\"Analytic MLE (w_hat_mle):\", w_hat_mle)\n",
    "print(\"Difference (numerical - analytic):\", w_guess - w_hat_mle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec9b16",
   "metadata": {},
   "source": [
    "### Direct Maximization of the Log Likelihood\n",
    "\n",
    "Instead of solving for the MLE using the moment-matching property of exponential families, you can maximize the log likelihood directly with respect to the natural parameters $w$:\n",
    "\n",
    "$$\n",
    "\\log p_w(x_1, \\ldots, x_n) = \\left(\\sum_{i=1}^n \\phi(x_i)\\right)^T w - n \\log Z(w)\n",
    "$$\n",
    "\n",
    "#### Numerical Optimization Approach\n",
    "\n",
    "1. **Define the negative log likelihood (NLL) function** to minimize:\n",
    "    $$\n",
    "    \\text{NLL}(w) = -\\left(\\sum_{i=1}^n \\phi(x_i)\\right)^T w + n \\log Z(w)\n",
    "    $$\n",
    "2. **Use a numerical optimizer** (e.g., Adam, gradient descent) to minimize NLL with respect to $w$.\n",
    "3. **JAX's autograd** can compute gradients automatically.\n",
    "\n",
    "#### Optimization Loop\n",
    "\n",
    "You can use the same optimizer setup as before, but now minimizing `neg_log_likelihood(w)`. The optimizer will iteratively update $w$ to maximize the log likelihood.\n",
    "\n",
    "**Summary:**  \n",
    "Direct maximization of the log likelihood is equivalent to the MLE approach, but uses the explicit likelihood function as the objective. For exponential families, both approaches yield the same result, but direct maximization is more general and works for any model where you can write down the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cef58d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 28902.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: w_guess = [0.00999993 0.99000007], loss = 1630.85888671875\n",
      "Iteration 100: w_guess = [0.8192815  0.14025278], loss = 217.92494201660156\n",
      "Iteration 200: w_guess = [0.8784846  0.17459245], loss = 210.4298553466797\n",
      "Iteration 300: w_guess = [0.9520907 0.188031 ], loss = 209.02294921875\n",
      "Iteration 400: w_guess = [1.0243574  0.20108947], loss = 207.907470703125\n",
      "Iteration 500: w_guess = [1.0917753  0.21327691], loss = 207.07298278808594\n",
      "Iteration 600: w_guess = [1.1528786  0.22432601], loss = 206.46908569335938\n",
      "Iteration 700: w_guess = [1.2071865  0.23414855], loss = 206.0419158935547\n",
      "Iteration 800: w_guess = [1.2547268  0.24274847], loss = 205.74549865722656\n",
      "Iteration 900: w_guess = [1.2957942  0.25017852], loss = 205.5435028076172\n",
      "Iteration 1000: w_guess = [1.3308265  0.25651732], loss = 205.40838623046875\n",
      "Iteration 1100: w_guess = [1.3603371  0.26185745], loss = 205.31979370117188\n",
      "Iteration 1200: w_guess = [1.3848749  0.26629803], loss = 205.26300048828125\n",
      "Iteration 1300: w_guess = [1.4049995  0.26994008], loss = 205.22744750976562\n",
      "Iteration 1400: w_guess = [1.4212646  0.27288383], loss = 205.20582580566406\n",
      "Iteration 1500: w_guess = [1.4342071  0.27522627], loss = 205.1929473876953\n",
      "Iteration 1600: w_guess = [1.444334  0.2770592], loss = 205.18557739257812\n",
      "Iteration 1700: w_guess = [1.4521163  0.27846774], loss = 205.18148803710938\n",
      "Iteration 1800: w_guess = [1.4579841  0.27952984], loss = 205.17929077148438\n",
      "Iteration 1900: w_guess = [1.4623178 0.2803143], loss = 205.17816162109375\n",
      "\n",
      "Numerical MLE (w_guess after optimization): [1.465424  0.2808765]\n",
      "Analytic MLE (w_hat_mle): [1.4719148  0.28205132]\n",
      "Difference (numerical - analytic): [-0.00649083 -0.00117484]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def neg_log_likelihood(w):\n",
    "    return -jnp.dot(sum_phi_x, w) + n_samples * gaussian_log_partition_function(w)\n",
    "\n",
    "\n",
    "# Use an optimizer (e.g., Adam) to minimize mle_objective\n",
    "w_guess = jnp.array([0.0, 1.0])\n",
    "opt_state = optimizer.init(w_guess)\n",
    "\n",
    "\n",
    "for i in tqdm(range(2000)):\n",
    "    w_guess, opt_state, loss = step(w_guess, opt_state, neg_log_likelihood)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}: w_guess = {w_guess}, loss = {loss}\")\n",
    "\n",
    "print(\"\\nNumerical MLE (w_guess after optimization):\", w_guess)\n",
    "print(\"Analytic MLE (w_hat_mle):\", w_hat_mle)\n",
    "print(\"Difference (numerical - analytic):\", w_guess - w_hat_mle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc49f80",
   "metadata": {},
   "source": [
    "### Comparing MLE and Bayesian Inference in the Exponential Family\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE):**\n",
    "- MLE finds the parameter $w$ that maximizes the likelihood of the observed data.\n",
    "- For exponential families, the MLE solution is where the gradient of the log-partition function matches the empirical mean of the sufficient statistics:\n",
    "    $$\n",
    "    \\nabla_w \\log Z(\\hat{w}_{\\text{MLE}}) = \\frac{1}{n} \\sum_{i=1}^n \\phi(x_i)\n",
    "    $$\n",
    "- In this notebook, `w_hat_mle` is computed directly from the data, ignoring any prior information.\n",
    "\n",
    "**Bayesian Inference:**\n",
    "- Bayesian inference combines the likelihood with a prior to produce a posterior distribution over $w$.\n",
    "- For exponential families with conjugate priors, the posterior is in the same family, with updated parameters:\n",
    "    $$\n",
    "    \\alpha' = \\alpha + \\sum_{i=1}^n \\phi(x_i), \\quad \\nu' = \\nu + n\n",
    "    $$\n",
    "- The mode of the posterior (MAP estimate) is found by solving:\n",
    "    $$\n",
    "    \\nabla_w \\log Z(w^*) = \\frac{\\alpha + \\sum_{i=1}^n \\phi(x_i)}{\\nu + n}\n",
    "    $$\n",
    "- In this notebook, `w_optimized` is the posterior mode, which incorporates both the prior (`alpha_0`, `nu_0`) and the data.\n",
    "\n",
    "**Key Differences:**\n",
    "- **MLE** uses only the data; it can overfit if $n$ is small.\n",
    "- **Bayesian inference** incorporates prior beliefs, leading to more regularized estimates, especially with limited data.\n",
    "- As $n$ increases, the influence of the prior diminishes, and the Bayesian posterior mode approaches the MLE.\n",
    "\n",
    "**In summary:**  \n",
    "- `w_hat_mle` is the MLE estimate (data only).\n",
    "- `w_optimized` is the Bayesian posterior mode (prior + data).\n",
    "- For large datasets, both estimates become similar; for small datasets, the Bayesian approach is more robust due to the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c08c65",
   "metadata": {},
   "source": [
    "Consider the exponential family $ p_w(x \\mid w) = h(x) \\exp \\left[ \\phi(x)^\\top w - \\log Z(w) \\right] $\n",
    "- its conjugate prior is the exponential family \n",
    "\n",
    "$$\n",
    "p_\\alpha(w \\mid \\alpha, \\nu) = \\exp \\left[ \\left( \\begin{pmatrix} w \\\\ -\\log Z(w) \\end{pmatrix}^\\top \\begin{pmatrix} \\alpha \\\\ \\nu \\end{pmatrix} \\right) - \\log F(\\alpha, \\nu) \\right]\n",
    "$$\n",
    "\n",
    "with partition function\n",
    "$$\n",
    "F(\\alpha, \\nu) := \\int \\exp(\\alpha^\\top w - \\nu \\log Z(w)) \\, dw \n",
    "$$\n",
    "\n",
    "\n",
    "and the predictive posterior is\n",
    "\n",
    "$$\n",
    "p(x) = \\int p_w(x \\mid w) p_\\alpha(w \\mid \\alpha, \\nu) \\, dw = h(x) \\int e^{(\\phi(x) + \\alpha)^\\top w + (\\nu + 1) \\log Z(w) - \\log F(\\alpha, \\nu)} \\, dw\n",
    "$$\n",
    "\n",
    "$$\n",
    "= h(x) \\frac{F(\\phi(x) + \\alpha, \\nu + 1)}{F(\\alpha, \\nu)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a20c3",
   "metadata": {},
   "source": [
    "## Laplace Approximation (when F is intractable)\n",
    "The slides mention that computing the normalization constant $F(\\alpha, \\nu)$ for the conjugate prior can be difficult or impossible in closed form for some exponential families. When the full analytic posterior is intractable, we can use approximation methods.\n",
    "\n",
    "**Laplace Approximation** is one such method. It approximates the posterior distribution $p(w|x)$ with a Gaussian distribution centered at the mode of the true posterior.\n",
    "\n",
    "To perform Laplace approximation:\n",
    "\n",
    "1. **Find the mode $\\hat{w}$ of the posterior $p(w|x)$.**  \n",
    "  This is done by finding the value of $w$ where the gradient of the log posterior is zero:  \n",
    "  $$\\nabla_w \\log p(w|x) = 0.$$\n",
    "  As derived in the slides and the previous section, setting the gradient of the log posterior to zero is equivalent to solving the root-finding problem:\n",
    "  $$\n",
    "  \\nabla_w \\log Z(w^*) = \\frac{\\alpha + \\sum_i \\phi(x_i)}{\\nu + n}\n",
    "  $$\n",
    "  where $\\alpha$ and $\\nu$ are the prior parameters, and $\\sum \\phi(x_i)$ and $n$ are from the data.\n",
    "\n",
    "2. **Evaluate the Hessian (the matrix of second partial derivatives) of the negative log posterior at the mode $\\hat{w}$.**  \n",
    "  Let this Hessian be\n",
    "  $$\n",
    "  \\Psi = -\\nabla_w \\nabla_w^T \\log p(w|x) \\Big|_{w = \\hat{w}}\n",
    "  $$\n",
    "\n",
    "3. **Approximate the posterior as a Gaussian distribution**  \n",
    "  $$\n",
    "  N(w; \\hat{w}, \\Psi^{-1})\n",
    "  $$\n",
    "  The mode $\\hat{w}$ is the mean, and the inverse of the negative Hessian $\\Psi^{-1}$ is the covariance matrix of the approximating Gaussian.\n",
    "\n",
    "The root-finding problem in step 1 can be solved using numerical optimization. We can define an objective function that measures how far $\\nabla_w \\log Z(w)$ is from the target value $\\frac{\\alpha + \\sum \\phi(x_i)}{\\nu + n}$ and minimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca6ad4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target gradient value to find the posterior mode: [  5.1679263 -15.236288 ]\n",
      "Optimizing to find posterior mode...\n",
      "Step 0, Loss: 243.865646, Current w: [0.09999933 0.90000063]\n",
      "Step 200, Loss: 0.004169, Current w: [1.6672059  0.31928024]\n",
      "Step 400, Loss: 0.003533, Current w: [1.6666081  0.31888303]\n",
      "Step 600, Loss: 0.003507, Current w: [1.6652656  0.31863964]\n",
      "Step 800, Loss: 0.003474, Current w: [1.6635906  0.31833604]\n",
      "Step 1000, Loss: 0.003434, Current w: [1.661587  0.3179728]\n",
      "Step 1200, Loss: 0.003388, Current w: [1.6592479  0.31754875]\n",
      "Step 1400, Loss: 0.003335, Current w: [1.6565611  0.31706166]\n",
      "Step 1600, Loss: 0.003276, Current w: [1.6535076  0.31650817]\n",
      "Step 1800, Loss: 0.003209, Current w: [1.6500671  0.31588453]\n",
      "Optimization finished. Found w_hat (posterior mode): [1.6462348  0.31518978]\n",
      "Final Loss: 0.003135\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate finding the mode of the log posterior using optimization\n",
    "# This is equivalent to solving the root-finding problem: grad_log_Z(w_hat) = target\n",
    "\n",
    "# The target value for the gradient of log Z is (alpha + sum_phi_x) / (nu + n)\n",
    "# Let's use the posterior parameters we calculated earlier.\n",
    "target_grad_log_Z_for_mode = (alpha_posterior) / (nu_posterior)\n",
    "# Note: In the slide's equation for the mode, alpha and nu are prior parameters.\n",
    "# The target is (alpha_prior + sum_phi_x) / (nu_prior + n).\n",
    "# Let's use alpha_0 and nu_0 from our conceptual example for clarity.\n",
    "target_grad_log_Z_for_mode = (alpha_0 + sum_phi_x) / (nu_0 + n_samples)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\\nTarget gradient value to find the posterior mode: {target_grad_log_Z_for_mode}\"\n",
    ")\n",
    "\n",
    "# We want to find w_hat such that grad_log_Z(w_hat) is close to target_grad_log_Z_for_mode.\n",
    "# We can minimize the squared difference: ||grad_log_Z(w) - target_grad_log_Z_for_mode||^2\n",
    "\n",
    "\n",
    "def objective_fn_for_mode(w, target):\n",
    "    \"\"\"\n",
    "    Objective function to minimize to find w_hat (posterior mode).\n",
    "    We want grad_log_Z(w) to be equal to target.\n",
    "    \"\"\"\n",
    "    grad_at_w = grad_log_Z(w)\n",
    "    return jnp.sum((grad_at_w - target) ** 2)\n",
    "\n",
    "\n",
    "\n",
    "# We can use an optimizer from JAX's optax library to find the w that minimizes this objective.\n",
    "# Set up the optimizer (e.g., Adam is a good general-purpose optimizer)\n",
    "optimizer = optax.adam(learning_rate=0.1)\n",
    "\n",
    "# Initialize optimizer state with an initial guess for w\n",
    "# Ensure the initial guess for w2 is positive.\n",
    "initial_w_guess = jnp.array([0.0, 1.0])\n",
    "opt_state = optimizer.init(initial_w_guess)\n",
    "\n",
    "\n",
    "# Define the training step using JAX's jit for performance\n",
    "@jax.jit\n",
    "def train_step_mode_finder(w, opt_state, target):\n",
    "    # Calculate the loss and its gradient with respect to w\n",
    "    loss, grads = jax.value_and_grad(objective_fn_for_mode)(w, target)\n",
    "    # Update the parameters using the optimizer\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, w)\n",
    "    w = optax.apply_updates(w, updates)\n",
    "    return w, opt_state, loss\n",
    "\n",
    "\n",
    "# Run the optimization loop\n",
    "num_iterations = 2000  # More iterations might be needed for convergence\n",
    "w_optimized = initial_w_guess\n",
    "losses = []\n",
    "\n",
    "print(\"Optimizing to find posterior mode...\")\n",
    "for step in range(num_iterations):\n",
    "    w_optimized, opt_state, loss = train_step_mode_finder(\n",
    "        w_optimized, opt_state, target_grad_log_Z_for_mode\n",
    "    )\n",
    "    losses.append(loss)\n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss:.6f}, Current w: {w_optimized}\")\n",
    "\n",
    "print(f\"Optimization finished. Found w_hat (posterior mode): {w_optimized}\")\n",
    "print(f\"Final Loss: {losses[-1]:.6f}\")\n",
    "\n",
    "# In a full Laplace approximation, you would then compute the Hessian of the\n",
    "# negative log posterior at this found mode w_optimized.\n",
    "# The negative log posterior is proportional to -log p(w|x) = - (alpha + sum_phi_x)^T w + (nu + n) log Z(w) + const\n",
    "# The Hessian of -log p(w|x) w.r.t w is (nu + n) * Hessian of log Z(w) w.r.t w.\n",
    "# You would use jax.hessian to compute this at w_optimized.\n",
    "# The covariance of the Laplace approximation would be the inverse of this Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46f7b1",
   "metadata": {},
   "source": [
    "## TODO: Review Laplace approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7e8b31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior mode (w_optimized): [1.6462348  0.31518978]\n",
      "Laplace covariance matrix at mode:\n",
      " [[0.05678565 0.01027475]\n",
      " [0.01027475 0.00196721]]\n"
     ]
    }
   ],
   "source": [
    "# Laplace Approximation: Compute the covariance at the posterior mode\n",
    "\n",
    "# The negative log posterior (up to a constant) is:\n",
    "# -log p(w|x) = - (alpha_0 + sum_phi_x)^T w + (nu_0 + n_samples) * log Z(w)\n",
    "# The Hessian of -log p(w|x) w.r.t. w is (nu_0 + n_samples) * Hessian of log Z(w) at w_optimized\n",
    "\n",
    "# Compute the Hessian of log Z(w) using JAX\n",
    "hessian_log_Z = jax.hessian(gaussian_log_partition_function)\n",
    "\n",
    "# Evaluate the Hessian at the posterior mode\n",
    "hess_at_mode = hessian_log_Z(w_optimized)\n",
    "\n",
    "# Compute the precision matrix (negative Hessian of log posterior)\n",
    "precision = (nu_0 + n_samples) * hess_at_mode\n",
    "\n",
    "# Covariance is the inverse of the precision matrix\n",
    "cov_laplace = jnp.linalg.inv(precision)\n",
    "\n",
    "print(\"Posterior mode (w_optimized):\", w_optimized)\n",
    "print(\"Laplace covariance matrix at mode:\\n\", cov_laplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1ef8726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter w[0]: 95% credible interval = [1.1792, 2.1133], mean = 1.6462, std = 0.2383\n",
      "Parameter w[1]: 95% credible interval = [0.2283, 0.4021], mean = 0.3152, std = 0.0444\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# The Laplace approximation gives a Gaussian posterior: N(w_optimized, cov_laplace)\n",
    "# 95% credible interval for each parameter: mean ± 1.96 * std\n",
    "\n",
    "w_mean = w_optimized\n",
    "w_std = jnp.sqrt(jnp.diag(cov_laplace))\n",
    "z = 1.96  # for 95% interval\n",
    "\n",
    "lower = w_mean - z * w_std\n",
    "upper = w_mean + z * w_std\n",
    "\n",
    "for i, (l, m, u) in enumerate(zip(lower, w_mean, upper)):\n",
    "    print(\n",
    "        f\"Parameter w[{i}]: 95% credible interval = [{l:.4f}, {u:.4f}], mean = {m:.4f}, std = {w_std[i]:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c833a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook explored the fundamental concepts of Exponential Families as presented in the lecture slides:\n",
    "- The connection between probabilistic inference, sufficient statistics, and conjugate priors.\n",
    "- The definition of an Exponential Family and its components ($h(x)$, $\\phi(x)$, $w$, $Z(w)$).\n",
    "- How the univariate Gaussian fits into the Exponential Family framework.\n",
    "- The data reduction property provided by sufficient statistics.\n",
    "- The simple parameter update rule for conjugate priors.\n",
    "- The crucial property relating the gradient of the log-partition function to the empirical mean of sufficient statistics at the Maximum Likelihood Estimate.\n",
    "- How numerical optimization can be used to find the mode of the posterior distribution, a key step in Laplace approximation when the conjugate prior normalization constant is intractable.\n",
    "\n",
    "By working through these examples, you should gain a deeper understanding of why Exponential Families are so important in probabilistic modeling and how their"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
